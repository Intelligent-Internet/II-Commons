{"id": "8645", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=8645", "title": "Declension", "text": "In linguistics, declension (verb: \"to decline\") is the changing of the form of a word, generally to express its syntactic function in the sentence, by way of some inflection. Declensions may apply to nouns, pronouns, adjectives, adverbs, and determiners. It serves to indicate number (e.g. singular, dual, plural), case (e.g. nominative, accusative, genitive, dative), gender (e.g. masculine, neuter, feminine), and a number of other grammatical categories. Meanwhile, the inflectional change of verbs is called \"conjugation\".\nDeclension occurs in many of the world's languages. It is an important aspect of language families like Quechuan (i.e., languages native to the Andes), Indo-European (e.g. German, Icelandic, Irish, Lithuanian and Latvian, Slavic, Sanskrit, Latin, Ancient and Modern Greek, Albanian, Romanian, Kurdish, Classical and Modern Armenian), Bantu (e.g. Swahili, Zulu, Kikuyu), Semitic (e.g. Modern Standard Arabic), Finno-Ugric (e.g. Hungarian, Finnish, Estonian), and Turkic (e.g. Turkish).\nOld English was an inflectional language, but largely abandoned inflectional changes as it evolved into Modern English. Though traditionally classified as synthetic, Modern English has moved towards a mostly analytic language.\nEnglish-speaking perspective.\nUnlike English, many languages use suffixes to specify subjects and objects and word cases in general. Inflected languages have a freer word order than modern English, an analytic language in which word order identifies the subject and object. As an example, even though both of the following sentences consist of the same words, the meaning is different:\nHypothetically speaking, suppose English were a language with a more complex declension system in which cases were formed by adding the suffixes:\nThe first sentence above could be formed with any of the following word orders and would have the same meaning:\nAs a more complex example, the sentence: \nbecomes nonsensical in English if the words are rearranged (because there are no cases):\nBut if English were a highly inflected language, like Latin or some Slavic languages such as Croatian, both sentences could mean the same thing. They would both contain five nouns in five different cases: \"mum\" \u2013 vocative (hey!), \"dog\" \u2013 nominative (who?), \"boy\" \u2013 genitive (of whom?), \"cat\" \u2013 accusative (whom?), \"street\" \u2013 locative (where?); the adjective \"little\" would be in the same case as the noun it modifies (\"boy\"), and the case of the determiner \"our\" would agree with the case of the noun it determines (\"street\").\nUsing the case suffixes invented for this example, the original sentence would read:\nAnd like other inflected languages, the sentence rearranged in the following ways would mean virtually the same thing, but with different expressiveness:\nInstead of the \"locative\", the \"instrumental form\" of \"down our street\" could also be used:\nDifferent word orders preserving the original meaning are possible in an inflected language, while modern English relies on word order for meaning, with a little flexibility. This is one of the advantages of an inflected language. The English sentences above, when read without the made-up case suffixes, are confusing.\nThese contrived examples are relatively simple, whereas actual inflected languages have a far more complicated set of declensions, where the suffixes (or prefixes, or infixes) change depending on the gender of the noun, the quantity of the noun, and other possible factors. This complexity and the possible lengthening of words is one of the disadvantages of inflected languages. Notably, many of these languages lack articles. There may also be \"irregular nouns\" where the declensions are unique for each word (like irregular verbs with conjugation). In inflected languages, other parts of speech such as numerals, demonstratives, adjectives, and articles are also declined.\nHistory.\nIt is agreed that Ancient Greeks had a \"vague\" idea of the forms of a noun in their language. A fragment of Anacreon seems to confirm this idea. Nevertheless, it cannot be concluded that the Ancient Greeks actually knew what the cases were. The Stoics developed many basic notions that today are the rudiments of linguistics. The idea of grammatical cases is also traced back to the Stoics, but it is still not completely clear what the Stoics exactly meant with their notion of cases.\nModern English.\nIn Modern English, the system of declensions is so simple compared to some other languages that the term \"declension\" is rarely used.\nNouns.\nMost nouns in English have distinct \"singular\" and \"plural\" forms. Nouns and most noun phrases can form a \"possessive\" construction. Plurality is most commonly shown by the ending \"-s\" (or \"-es\"), whereas possession is always shown by the enclitic \"-'s\" or, for plural forms ending in \"s\", by just an apostrophe.\nConsider, for example, the forms of the noun \"girl\". Most speakers pronounce all forms other than the singular plain form (\"girl\") exactly the same.\nBy contrast, a few irregular nouns (like man/men) are slightly more complex in their forms. In this example, all four forms are pronounced distinctly.\nFor nouns, in general, gender is not declined in Modern English. There are isolated situations where certain nouns may be modified to reflect gender, though not in a systematic fashion. Loan words from other languages, particularly Latin and the Romance languages, often preserve their gender-specific forms in English, e.g. \"alumnus\" (masculine singular) and \"alumna\" (feminine singular). Similarly, names borrowed from other languages show comparable distinctions: \"Andrew\" and \"Andrea\", \"Paul\" and \"Paula\", etc. Additionally, suffixes such as \"-ess\", \"-ette\", and \"-er\" are sometimes applied to create overtly gendered versions of nouns, with marking for feminine being much more common than marking for masculine. Many nouns can actually function as members of two genders or even all three, and the gender classes of English nouns are usually determined by their agreement with pronouns, rather than marking on the nouns themselves.\nThere can be other derivations from nouns that are not considered declensions. For example, the proper noun \"Britain\" has the associated descriptive adjective \"British\" and the demonym \"Briton\". Though these words are clearly related, and are generally considered cognates, they are not specifically treated as forms of the \"same word\", and thus are not declensions.\nPronouns.\nPronouns in English have more complex declensions. For example, the first person \"I\":\nWhereas nouns do not distinguish between the subjective (nominative) and objective (oblique) cases, some pronouns do; that is, they decline to reflect their relationship to a verb or preposition, or case. Consider the difference between \"he\" (subjective) and \"him\" (objective), as in \"He saw it\" and \"It saw him\"; similarly, consider \"who\", which is subjective, and the objective \"whom\" (although it is increasingly common to use \"who\" for both).\nThe one situation where gender is still clearly part of the English language is in the pronouns for the third person singular. Consider the following:\nThe distinguishing of neuter for persons and non-persons is peculiar to English. This has existed since the 14th century. However, the use of \"singular they\" is often restricted to specific contexts, depending on the dialect or the speaker. It is most typically used to refer to a single person of unknown gender (e.g. \"someone left their jacket behind\") or a hypothetical person where gender is insignificant (e.g. \"If someone wants to, then they should\"). Its use has expanded in recent years due to increasing social recognition of persons who do not identify themselves as male or female (see gender-nonbinary). The \"singular they\" still uses plural verb forms, reflecting its origins.\nAdjectives and adverbs.\nSome English adjectives and adverbs are declined for degree of comparison. The unmarked form is the positive form, such as \"quick\". Comparative forms are formed with the ending \"-er\" (\"quicker\"), while superlative forms are formed with \"-est\" (\"quickest\"). Some are uncomparable; the remainder are usually periphrastic constructions with \"more\" (\"more beautiful\") and \"most\" (\"most modestly\"). See degree of comparison for more.\nAdjectives are not declined for case in Modern English (though they were in Old English), nor number nor gender.\nDeterminers.\nThe demonstrative determiners \"this\" and \"that\" are declined for number, as \"these\" and \"those\".\nThe article is never regarded as declined in Modern English, although formally, the words \"that\" and possibly \"she\" correspond to forms of the predecessor of \"the\" (\"s\u0113\" m., \"\u00fe\u00e6t\" n., \"s\u0113o\" f.) as it was declined in Old English.\nLatin.\nJust as verbs in Latin are conjugated to indicate grammatical information, Latin nouns and adjectives that modify them are declined to signal their roles in sentences. There are five important cases for Latin nouns: nominative, genitive, dative, accusative, and ablative. Since the vocative case usually takes the same form as the nominative, it is seldom spelt out in grammar books. Yet another case, the locative, is limited to a small number of words.\nThe usual basic functions of these cases are as follows:\nThe genitive, dative, accusative, and ablative also have important functions to indicate the object of a preposition.\nGiven below is the declension paradigm of Latin \"puer\" 'boy' and \"puella\" 'girl':\nFrom the provided examples we can see how cases work:\nSanskrit.\nSanskrit, another Indo-European language, has eight cases: nominative, vocative, accusative, genitive, dative, ablative, locative and instrumental. Some do not count vocative as a separate case, despite it having a distinctive ending in the singular, but consider it as a different use of the nominative.\nSanskrit grammatical cases have been analyzed extensively. The grammarian P\u0101\u1e47ini identified six semantic roles or \"karaka\", which correspond closely to the eight cases:\nFor example, consider the following sentence:\nHere \"leaf\" is the agent, \"tree\" is the source, and \"ground\" is the locus. The endings \"-a\u1e41\", \"-at\", \"-\u0101u\" mark the cases associated with these meanings.\nVerse 37 of the R\u0101marak\u1e63\u0101stotram gives an example of all 8 types of declensions in Sanskrit for the singular proper noun R\u0101ma."}
{"id": "8647", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=8647", "title": "Dozenal Society of Great Britain", "text": ""}
{"id": "8648", "revid": "48072816", "url": "https://en.wikipedia.org/wiki?curid=8648", "title": "Daffynition", "text": "A daffynition (a portmanteau blend of \"daffy\" and \"definition\") is a form of pun involving the reinterpretation of an existing word, on the basis that it sounds like another word (or group of words). Presented in the form of dictionary definitions, they are similar to transpositional puns, but often much less complex and easier to create.\nUnder the name Uxbridge English Dictionary, making up daffynitions is a popular game on the BBC Radio 4 comedy quiz show \"I'm Sorry I Haven't a Clue\".\nA lesser-known subclass of daffynition is the \"goofinition\", which relies strictly on literal associations and correct spellings, such as \"lobster = a weak tennis player\". This play on words is similar to Cockney rhyming slang."}
{"id": "8649", "revid": "46326017", "url": "https://en.wikipedia.org/wiki?curid=8649", "title": "List of football clubs in the Netherlands", "text": "The Dutch Football League is organized by the Royal Dutch Football Association (KNVB, Koninklijke Nederlandse Voetbalbond).The most successful teams are Ajax (36), PSV (24) and Feyenoord (16). Important teams of the past are HVV (10 titles), Sparta Rotterdam (6 titles) and Willem II (3 titles).\nThe annual match that marks the beginning of the season is called the Johan Cruijff Schaal (Johan Cruyff Shield). Contenders are the champions and the cup winners of the previous season."}
{"id": "8650", "revid": "39191556", "url": "https://en.wikipedia.org/wiki?curid=8650", "title": "Dragon 32/64", "text": "The Dragon 32 and Dragon 64 are home computers that were built in the 1980s. The Dragons are very similar to the TRS-80 Color Computer, and were produced for the European market by Dragon Data, Ltd., initially in Swansea, Wales, before moving to Port Talbot, Wales (until 1984), and by Eurohard S.A. in Casar de C\u00e1ceres, Spain (from 1984 to 1987), and for the US market by Tano Corporation of New Orleans, Louisiana. The model numbers reflect the primary difference between the two machines, which have 32 and 64 kilobytes of RAM, respectively.\nDragon Data introduced the Dragon 32 microcomputer in August 1982, followed by the Dragon 64 a year later. Despite initial success, the Dragon faced technical limitations in graphics capabilities and hardware-supported text modes, which restricted its appeal in the gaming and educational markets. Dragon Data collapsed in 1984 and was acquired by Spanish company Eurohard S.A. However, Eurohard filed for bankruptcy in 1987.\nThe Dragon computers were built around the Motorola MC6809E processor and featured a composite monitor port, allowing connection to (at the time) modern TVs. They used analog joysticks and had a range of peripherals and add-ons available. The Dragon had several high-resolution display modes, but limited graphics capabilities compared to other home computers of the time.\nThe Dragon came with a Microsoft BASIC interpreter in ROM, which allowed instant system start-up. The Dragon 32/64 was capable of running multiple disk operating systems, and a range of popular games were ported to the system. \nOverall, the Dragon computers were initially well-received but faced limitations that hindered their long-term success.\nDragon 32 vs. Dragon 64.\nAside from the amount of RAM, the Dragon 64 also has a functional RS-232 serial port which was not included on the Dragon 32. A minor difference between the two Dragon models is the outer case colour; the Dragon 32 is beige and the Dragon 64 is light grey. Besides the case, branding and the Dragon 64's serial port, the two machines look the same. The Dragon 32 is upgradable to Dragon 64. In some cases, buyers of the Dragon 32 found that they actually received a Dragon 64 unit.\nProduct history.\nDragon Data entered the market in August 1982 with the Dragon 32. The Dragon 64 followed a year later. The computers sold well initially and attracted the interest of independent software developers including Microdeal. A companion magazine, \"Dragon User\", began publication shortly after the microcomputer's launch.\nDespite this initial success, there were two technical impediments to the Dragon's acceptance. The graphics capabilities trailed behind other computers such as the ZX Spectrum and BBC Micro, a significant shortcoming for the games market. Additionally, as a cost-cutting measure, the hardware-supported text modes only included upper case characters; this restricted the system's appeal to the educational market.\nDragon Data collapsed in June 1984. It was acquired by the Spanish company Eurohard S.A., which moved the factory from Wales to C\u00e1ceres and released the Dragon 200 (a Dragon 64 with a new case that allowed a monitor to be placed on top) and the Dragon 200-E (an enhanced Dragon 200 with both upper and lower case characters and a Spanish keyboard), but ultimately filed for bankruptcy in 1987. The remaining stock from Eurohard was purchased by a Spanish electronics hobbyist magazine and given away to those who paid for a three-year subscription, until 1992.\nIn the United States it was possible to purchase the Tano Dragon new in box until early 2017 from California Digital, a retailer that purchased the remaining stock.\nReception.\n\"BYTE\" wrote in January 1983 that the Dragon 32 \"offers more feature for the money than most of its competitors\", but \"there's nothing exceptional about it\". The review described it as a redesigned, less-expensive Color Computer with 32K RAM and better keyboard.\nTechnical notes.\nHardware and peripherals.\nThe Dragon is built around the Motorola MC6809E processor running at 0.89 MHz. It was an advanced 8-bit CPU design, with limited 16-bit capabilities.\nIt was possible to increase the speed of the computer by using codice_1 which accelerated the ROM-resident BASIC interpreter, but temporarily disabled proper functioning of the cassette/printer ports. Manufacturing variances mean that not all Dragons were able to function at this higher speed, and use of this POKE could cause some units to crash or be unstable, though with no permanent damage. codice_2 returned the speed to normal. codice_3 pushed the speed yet higher but the display was lost until a slower speed was restored.\nThe Dragon used the SN74LS783/MC6883 Synchronous Address Multiplexer (SAM) and the MC6847 Video Display Generator (VDG). I/O was provided by two MC6821 Peripheral Interface Adapters (PIAs). Many Dragon 32s were upgraded by their owners to 64\u00a0KB of memory. A few were further expanded to 128\u00a0KB, 256\u00a0KB, or 512\u00a0KB with home-built memory controllers/memory management units (MMUs).\nA broad range of peripherals exist for the Dragon 32/64, and there are add-ons such as the Dragon's Claw which give the Dragons a port that is hardware-compatible with the BBC Micro's user port, though separate software drivers for connected devices must be developed. Although neither machine has a built-in disk operating system (Compact Cassettes being the standard storage mechanism commonly used for machines of the time), DragonDOS was supplied as part of the disk controller interface from Dragon Data Ltd. The versatile external ports, including the standard RS-232 on the 64, also allows hobbyists to attach a diverse range of equipment.\nThe computer featured a composite monitor port as an alternative to the TV RF output which can be used to connect the Dragon 32 to most modern TVs to deliver a much better picture.\nThe Dragon used analogue joysticks, unlike most systems of the time which used simpler and cheaper digital systems. Other uses for the joystick ports included light pens.\nTony Clarke and Richard Wadman established the specifications for the Dragon.\nThe units had a robust motherboard in a spacious case, reminiscent of the BBC Micro, and so were more tolerant of aftermarket modification than some of their contemporaries, which often had their components crammed into the smallest possible space.\nVideo modes.\nThe Dragon's main display mode is 'black on green' text (the black was, in actuality, a deeper, muddier green). The only graphics possible in this mode are quarter-tile block based.\nIt also has a selection of five high-resolution modes, named PMODEs 0\u20134, which alternate monochrome and four-colour in successively higher resolutions, culminating in the black-and-white 256\u00d7192 PMODE 4. Each mode has two possible colour palettes \u2013 these are rather garish and cause the system to fare poorly in visual comparisons with other home computers of the time. It is also impossible to use standard printing commands to print text in the graphical modes, causing software development difficulties.\nFull-colour, scanline-based 64\u00d7192 semi-graphics modes are also possible, though their imbalanced resolution and programming difficulty (not being accessible via BASIC) meant they were not often utilised.\nDisk systems.\nFirst to market was a complete disk operating system produced by Premier Microsystems, located near Croydon. The system was sold as the \"Delta\" disk operating system; there was a proposal for Dragon to market this as an addon. Dragon did not enter into such an agreement and instead produced the DragonDOS system. The two systems were incompatible.\nDelta's lead in availability ensured that software was released in the format, whilst Dragon's \"official\" status ensured that it, too, gained software published in its format. This led to confusion and frustration, with customers finding they had either purchased a version incompatible with their setup, or that the software was only available for the competing standard.\nSystem software.\nThe Dragon comes with a Microsoft BASIC interpreter in 16\u00a0KB of ROM. The BASIC appears to be nearly identical to Tandy Color Computer's Extended Basic with a few changes necessary to interact with the Dragon system.\nIn common with home computers of the time, the entire operating software was included on a ROM chip; therefore, the system starts instantly when powered up.\nSome software providers also produced compilers for BASIC and other languages to produce binary (or \"machine\") code which would run many times faster and make better use of the small system RAM. Towards the end of its life, Dragon Data produced an assembler/disassembler/editor suite called \"Dream\".\nIn addition to the DragonDOS disk operating system, the Dragon 32/64 is capable of running several others, including FLEX, and even OS-9 which brought UNIX-like multitasking to the platform. Memory-expanded and MMU-equipped Dragons are able to run OS-9 Level 2.\nGames.\nInitially, the Dragon was reasonably well supported by the major UK software companies, with versions of popular games from other systems being ported to the Dragon. Top-selling games available for the Dragon include \"Arcadia\" (Imagine), \"Chuckie Egg\" (A&amp;F), \"Manic Miner\" and sequel \"Jet Set Willy\" (Software Projects), \"Hunchback\" (Ocean) and \"Football Manager\" (Addictive). There were also companies that concentrated on the Dragon, such as Microdeal. Their character Cuthbert appeared in several games, with \"Cuthbert Goes Walkabout\" also being converted for Atari 8-bit and Commodore 64 systems.\nDue to the limited graphics modes of the Dragon, converted games had a distinctive appearance, with colour games being usually played on a green or white background (rather than the more common black on other systems) or games with high-definition graphics having to run in black and white.\nWhen the system was discontinued, support from software companies also effectively ended. However, Microdeal continued supporting the Dragon until January 1988. Some of their final games developed for the Dragon in 1987 such as \"Tanglewood\" and \"Airball\" were also converted for 16-bit machines such as the Atari ST and Amiga.\nDifferences from the TRS-80 Color Computer.\nBoth the Dragon and the TRS-80 Color Computer are based on a Motorola data sheet design for the MC6883 SAM (MMU) chip for memory management and peripheral control.\nThe systems are sufficiently similar that a significant fraction of the compiled software produced for one machine will run on the other. Software running via the built-in Basic interpreters also has a high level of compatibility, but only after they are re-tokenized, which can be achieved fairly easily by transferring via cassette tape with appropriate options.\nIt is possible to permanently convert a Color Computer into a Dragon by swapping the original Color Computer ROM and rewiring the keyboard cable.\nThe Dragon has additional circuitry to make the MC6847 VDG compatible with European 625-line PAL television standards, rather than the US 525-line NTSC standard, and a Centronics parallel printer port not present on the TRS-80. Some models were manufactured with NTSC video for the US and Canadian markets."}
{"id": "8651", "revid": "30571117", "url": "https://en.wikipedia.org/wiki?curid=8651", "title": "Dark matter", "text": "In astronomy, dark matter is an invisible and hypothetical form of matter that does not interact with light or other electromagnetic radiation. Dark matter is implied by gravitational effects which cannot be explained by general relativity unless more matter is present than can be observed. Such effects occur in the context of formation and evolution of galaxies, gravitational lensing, the observable universe's current structure, mass position in galactic collisions, the motion of galaxies within galaxy clusters, and cosmic microwave background anisotropies.\nIn the standard Lambda-CDM model of cosmology, the mass\u2013energy content of the universe is 5% ordinary matter, 26.8% dark matter, and 68.2% a form of energy known as dark energy. Thus, dark matter constitutes 85% of the total mass, while dark energy and dark matter constitute 95% of the total mass\u2013energy content.\nDark matter is not known to interact with ordinary baryonic matter and radiation except through gravity, making it difficult to detect in the laboratory. The most prevalent explanation is that dark matter is some as-yet-undiscovered subatomic particle, such as either weakly interacting massive particles (WIMPs) or axions. The other main possibility is that dark matter is composed of primordial black holes.\nDark matter is classified as \"cold\", \"warm\", or \"hot\" according to velocity (more precisely, its free streaming length). Recent models have favored a cold dark matter scenario, in which structures emerge by the gradual accumulation of particles.\nAlthough the astrophysics community generally accepts the existence of dark matter, a minority of astrophysicists, intrigued by specific observations that are not well explained by ordinary dark matter, argue for various modifications of the standard laws of general relativity. These include modified Newtonian dynamics, tensor\u2013vector\u2013scalar gravity, or entropic gravity. So far none of the proposed modified gravity theories can describe every piece of observational evidence at the same time, suggesting that even if gravity has to be modified, some form of dark matter will still be required.\nHistory.\nEarly history.\nThe hypothesis of dark matter has an elaborate history.\nWm. Thomson, Lord Kelvin, discussed the potential number of stars around the Sun in the appendices of a book based on a series of lectures given in 1884 in Baltimore. He inferred their density using the observed velocity dispersion of the stars near the Sun, assuming that the Sun was 20\u2013100\u00a0million years old. He posed what would happen if there were a thousand million stars within 1\u00a0kiloparsec of the Sun (at which distance their parallax would be 1\u00a0milli-arcsecond). Kelvin concluded\nMany of our supposed thousand million stars \u2013 perhaps a great majority of them \u2013 may be dark bodies.\nIn 1906, Poincar\u00e9 used the French term [] (\"dark matter\") in discussing Kelvin's work. He found that the amount of dark matter would need to be less than that of visible matter, incorrectly, it turns out.\nThe second to suggest the existence of dark matter using stellar velocities was Dutch astronomer Jacobus Kapteyn in 1922.\nA publication from 1930 by Swedish astronomer Knut Lundmark points to him being the first to realise that the universe must contain much more mass than can be observed. Dutch radio astronomy pioneer Jan Oort also hypothesized the existence of dark matter in 1932. Oort was studying stellar motions in the galactic neighborhood and found the mass in the galactic plane must be greater than what was observed, but this measurement was later determined to be incorrect.\nIn 1933, Swiss astrophysicist Fritz Zwicky studied galaxy clusters while working at Cal Tech and made a similar inference. Zwicky applied the virial theorem to the Coma Cluster and obtained evidence of unseen mass he called ('dark matter'). Zwicky estimated its mass based on the motions of galaxies near its edge and compared that to an estimate based on its brightness and number of galaxies. He estimated the cluster had about 400\u00a0times more mass than was visually observable. The gravity effect of the visible galaxies was far too small for such fast orbits, thus mass must be hidden from view. Based on these conclusions, Zwicky inferred some unseen matter provided the mass and associated gravitational attraction to hold the cluster together. Zwicky's estimates were off by more than an order of magnitude, mainly due to an obsolete value of the Hubble constant; the same calculation today shows a smaller fraction, using greater values for luminous mass. Nonetheless, Zwicky did correctly conclude from his calculation that most of the gravitational matter present was dark. However unlike modern theories, Zwicky considered \"dark matter\" to be non-luminous ordinary matter.\nFurther indications of mass-to-light ratio anomalies came from measurements of galaxy rotation curves. In 1939, H.W. Babcock reported the rotation curve for the Andromeda nebula (now called \"the Andromeda Galaxy\"), which suggested the mass-to-luminosity ratio increases radially. He attributed it to either light absorption within the galaxy or modified dynamics in the outer portions of the spiral, rather than to unseen matter. Following Babcock's 1939 report of unexpectedly rapid rotation in the outskirts of the Andromeda Galaxy and a mass-to-light ratio of 50; in 1940, Oort discovered and wrote about the large non-visible halo of NGC\u00a03115.\n1970s.\nThe hypothesis of dark matter largely took root in the 1970s. Several different observations were synthesized to argue that galaxies should be surrounded by halos of unseen matter. In two papers that appeared in 1974, this conclusion was drawn in tandem by independent groups: in Princeton, New Jersey, by Jeremiah Ostriker, Jim Peebles, and Amos Yahil, and in Tartu, Estonia, by Jaan Einasto, Enn Saar, and Ants Kaasik.\nOne of the observations that served as evidence for the existence of galactic halos of dark matter was the shape of galaxy rotation curves. These observations were done in optical and radio astronomy. In optical astronomy, Vera Rubin and Kent Ford worked with a new spectrograph to measure the velocity curve of edge-on spiral galaxies with greater accuracy.\nAt the same time, radio astronomers were making use of new radio telescopes to map the 21\u00a0cm line of atomic hydrogen in nearby galaxies. The radial distribution of interstellar atomic hydrogen (H) often extends to much greater galactic distances than can be observed as collective starlight, expanding the sampled distances for rotation curves \u2013 and thus of the total mass distribution \u2013 to a new dynamical regime. Early mapping of Andromeda with the telescope at Green Bank and the dish at Jodrell Bank already showed the H rotation curve did not trace the decline expected from Keplerian orbits.\nAs more sensitive receivers became available, Roberts &amp; Whitehurst (1975) were able to trace the rotational velocity of Andromeda to 30\u00a0kpc, much beyond the optical measurements. Illustrating the advantage of tracing the gas disk at large radii; that paper's \"Figure\u00a016\" combines the optical data (the cluster of points at radii of less than 15\u00a0kpc with a single point further out) with the H data between 20 and 30\u00a0kpc, exhibiting the flatness of the outer galaxy rotation curve; the solid curve peaking at the center is the optical surface density, while the other curve shows the cumulative mass, still rising linearly at the outermost measurement. In parallel, the use of interferometric arrays for extragalactic H spectroscopy was being developed. Rogstad &amp; Shostak (1972) published H rotation curves of five spirals mapped with the Owens Valley interferometer; the rotation curves of all five were very flat, suggesting very large values of mass-to-light ratio in the outer parts of their extended H\u00a0disks. In 1978, Albert Bosma showed further evidence of flat rotation curves using data from the Westerbork Synthesis Radio Telescope. \nBy the late 1970s the existence of dark matter halos around galaxies was widely recognized as real, and became a major unsolved problem in astronomy.\n1980\u20131990s.\nA stream of observations in the 1980\u20131990s supported the presence of dark matter. is notable for the investigation of 967\u00a0spirals. The evidence for dark matter also included gravitational lensing of background objects by galaxy clusters, the temperature distribution of hot gas in galaxies and clusters, and the pattern of anisotropies in the cosmic microwave background.\nAccording to the current consensus among cosmologists, dark matter is composed primarily of some type of not-yet-characterized subatomic particle.\nThe search for this particle, by a variety of means, is one of the major efforts in particle physics.\nTechnical definition.\nIn standard cosmological calculations, \"matter\" means any constituent of the universe whose energy density scales with the inverse cube of the scale factor, i.e., This is in contrast to \"radiation\", which scales as the inverse fourth power of the scale factor and a cosmological constant, which does not change with respect to (). The different scaling factors for matter and radiation are a consequence of radiation redshift. For example, after doubling the diameter of the observable Universe via cosmic expansion, the scale, , has doubled. The energy of the cosmic microwave background radiation has been halved (because the wavelength of each photon has doubled); the energy of ultra-relativistic particles, such as early-era standard-model neutrinos, is similarly halved. The cosmological constant, as an intrinsic property of space, has a constant energy density regardless of the volume under consideration.\nIn principle, \"dark matter\" means all components of the universe which are not visible but still obey In practice, the term \"dark matter\" is often used to mean only the non-baryonic component of dark matter, i.e., excluding \"missing baryons\". Context will usually indicate which meaning is intended.\nObservational evidence.\nGalaxy rotation curves.\nThe arms of spiral galaxies rotate around their galactic center. The luminous mass density of a spiral galaxy decreases as one goes from the center to the outskirts. If luminous mass were all the matter, then the galaxy can be modelled as a point mass in the centre and test masses orbiting around it, similar to the Solar System. From Kepler's Third Law, it is expected that the rotation velocities will decrease with distance from the center, similar to the Solar System. This is not observed. Instead, the galaxy rotation curve remains flat or even increases as distance from the center increases.\nIf Kepler's laws are correct, then the obvious way to resolve this discrepancy is to conclude the mass distribution in spiral galaxies is not similar to that of the Solar System. In particular, there may be a lot of non-luminous matter (dark matter) in the outskirts of the galaxy.\nVelocity dispersions.\nStars in bound systems must obey the virial theorem. The theorem, together with the measured velocity distribution, can be used to measure the mass distribution in a bound system, such as elliptical galaxies or globular clusters. With some exceptions, velocity dispersion estimates of elliptical galaxies do not match the predicted velocity dispersion from the observed mass distribution, even assuming complicated distributions of stellar orbits.\nAs with galaxy rotation curves, the obvious way to resolve the discrepancy is to postulate the existence of non-luminous matter.\nGalaxy clusters.\nGalaxy clusters are particularly important for dark matter studies since their masses can be estimated in three independent ways:\nGenerally, these three methods are in reasonable agreement that dark matter outweighs visible matter by approximately 5 to 1.\nGravitational lensing.\nOne of the consequences of general relativity is the gravitational lens. Gravitational lensing occurs when massive objects between a source of light and the observer act as a lens to bend light from this source. Lensing does not depend on the properties of the mass; it only requires there to be a mass. The more massive an object, the more lensing is observed. An example is a cluster of galaxies lying between a more distant source such as a quasar and an observer. In this case, the galaxy cluster will lens the quasar.\nStrong lensing is the observed distortion of background galaxies into arcs when their light passes through such a gravitational lens. It has been observed around many distant clusters including Abell 1689. By measuring the distortion geometry, the mass of the intervening cluster can be obtained. In the weak regime, lensing does not distort background galaxies into arcs, causing minute distortions instead. By examining the apparent shear deformation of the adjacent background galaxies, the mean distribution of dark matter can be characterized. The measured mass-to-light ratios correspond to dark matter densities predicted by other large-scale structure measurements.\nCosmic microwave background.\nAlthough both dark matter and ordinary matter are matter, they do not behave in the same way. In particular, in the early universe, ordinary matter was ionized and interacted strongly with radiation via Thomson scattering. Dark matter does not interact directly with radiation, but it does affect the cosmic microwave background (CMB) by its gravitational potential (mainly on large scales) and by its effects on the density and velocity of ordinary matter. Ordinary and dark matter perturbations, therefore, evolve differently with time and leave different imprints on the CMB.\nThe CMB is very close to a perfect blackbody but contains very small temperature anisotropies of a few parts in 100,000. A sky map of anisotropies can be decomposed into an angular power spectrum, which is observed to contain a series of acoustic peaks at near-equal spacing but different heights. The locations of these peaks depend on cosmological parameters. Matching theory to data, therefore, constrains cosmological parameters.\nThe CMB anisotropy was first discovered by COBE in 1992, though this had too coarse resolution to detect the acoustic peaks.\nAfter the discovery of the first acoustic peak by the balloon-borne BOOMERanG experiment in 2000, the power spectrum was precisely observed by WMAP in 2003\u20132012, and even more precisely by the \"Planck\" spacecraft in 2013\u20132015. The results support the Lambda-CDM model.\nThe observed CMB angular power spectrum provides powerful evidence in support of dark matter, as its precise structure is well fitted by the Lambda-CDM model, but difficult to reproduce with any competing model such as modified Newtonian dynamics (MOND).\nStructure formation.\nStructure formation refers to the period after the Big Bang when density perturbations collapsed to form stars, galaxies, and clusters. Prior to structure formation, the Friedmann solutions to general relativity describe a homogeneous universe. Later, small anisotropies gradually grew and condensed the homogeneous universe into stars, galaxies and larger structures. Ordinary matter is affected by radiation, which is the dominant element of the universe at very early times. As a result, its density perturbations are washed out and unable to condense into structure. If there were only ordinary matter in the universe, there would not have been enough time for density perturbations to grow into the galaxies and clusters currently seen.\nDark matter provides a solution to this problem because it is unaffected by radiation. Therefore, its density perturbations can grow first. The resulting gravitational potential acts as an attractive potential well for ordinary matter collapsing later, speeding up the structure formation process.\nBullet Cluster.\nThe Bullet Cluster is the result of a recent collision of two galaxy clusters. It is of particular note because the location of the center of mass as measured by gravitational lensing is different from the location of the center of mass of visible matter. This is difficult for modified gravity theories, which generally predict lensing around visible matter, to explain. Standard dark matter theory however has no issue: the hot, visible gas in each cluster would be cooled and slowed down by electromagnetic interactions, while dark matter (which does not interact electromagnetically) would not. This leads to the dark matter separating from the visible gas, producing the separate lensing peak as observed.\nType Ia supernova distance measurements.\nType Ia supernovae can be used as standard candles to measure extragalactic distances, which can in turn be used to measure how fast the universe has expanded in the past. Data indicates the universe is expanding at an accelerating rate, the cause of which is usually ascribed to dark energy. Since observations indicate the universe is almost flat, it is expected the total energy density of everything in the universe should sum to 1 (). The measured dark energy density is ; the observed ordinary (baryonic) matter energy density is and the energy density of radiation is negligible. This leaves a missing which nonetheless behaves like matter (see technical definition section above)dark matter.\nSky surveys and baryon acoustic oscillations.\nBaryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe on large scales. These are predicted to arise in the Lambda-CDM model due to acoustic oscillations in the photon\u2013baryon fluid of the early universe and can be observed in the cosmic microwave background angular power spectrum. BAOs set up a preferred length scale for baryons. As the dark matter and baryons clumped together after recombination, the effect is much weaker in the galaxy distribution in the nearby universe, but is detectable as a subtle (\u22481\u00a0percent) preference for pairs of galaxies to be separated by 147\u00a0Mpc, compared to those separated by 130\u2013160\u00a0Mpc. This feature was predicted theoretically in the 1990s and then discovered in 2005, in two large galaxy redshift surveys, the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. Combining the CMB observations with BAO measurements from galaxy redshift surveys provides a precise estimate of the Hubble constant and the average matter density in the Universe. The results support the Lambda-CDM model.\nRedshift-space distortions.\nLarge galaxy redshift surveys may be used to make a three-dimensional map of the galaxy distribution. These maps are slightly distorted because distances are estimated from observed redshifts; the redshift contains a contribution from the galaxy's so-called peculiar velocity in addition to the dominant Hubble expansion term. On average, superclusters are expanding more slowly than the cosmic mean due to their gravity, while voids are expanding faster than average. In a redshift map, galaxies in front of a supercluster have excess radial velocities towards it and have redshifts slightly higher than their distance would imply, while galaxies behind the supercluster have redshifts slightly low for their distance. This effect causes superclusters to appear squashed in the radial direction, and likewise voids are stretched. Their angular positions are unaffected. This effect is not detectable for any one structure since the true shape is not known, but can be measured by averaging over many structures. It was predicted quantitatively by Nick Kaiser in 1987, and first decisively measured in 2001 by the 2dF Galaxy Redshift Survey. Results are in agreement with the Lambda-CDM model.\nLyman-alpha forest.\nIn astronomical spectroscopy, the Lyman-alpha forest is the sum of the absorption lines arising from the Lyman-alpha transition of neutral hydrogen in the spectra of distant galaxies and quasars. Lyman-alpha forest observations can also constrain cosmological models. These constraints agree with those obtained from WMAP data.\nTheoretical classifications.\nDark matter can be divided into \"cold\", \"warm\", and \"hot\" categories. These categories refer to velocity rather than an actual temperature, and indicate how far corresponding objects moved due to random motions in the early universe, before they slowed due to cosmic expansion. This distance is called the \"free streaming length\" (FSL). The categories of dark matter are set with respect to the size of a protogalaxy (an object that later evolves into a dwarf galaxy): dark matter particles are classified as cold, warm, or hot if their FSL is much smaller (cold), similar to (warm), or much larger (hot) than a protogalaxy. Mixtures of the above are also possible: a theory of mixed dark matter was popular in the mid-1990s, but was rejected following the discovery of dark energy.\nThe significance of the free streaming length is that the universe began with some primordial density fluctuations from the Big Bang (in turn arising from quantum fluctuations at the microscale). Particles from overdense regions will naturally spread to underdense regions, but because the universe is expanding quickly, there is a time limit for them to do so. Faster particles (hot dark matter) can beat the time limit while slower particles cannot. The particles travel a free streaming length's worth of distance within the time limit; therefore this length sets a minimum scale for later structure formation. Because galaxy-size density fluctuations get washed out by free-streaming, hot dark matter implies the first objects that can form are huge supercluster-size pancakes, which then fragment into galaxies, while the reverse is true for cold dark matter. \nDeep-field observations show that galaxies formed first, followed by clusters and superclusters as galaxies clump together, and therefore that most dark matter is cold. This is also the reason why neutrinos, which move at nearly the speed of light and therefore would fall under hot dark matter, cannot make up the bulk of dark matter.\nComposition.\nThe identity of dark matter is unknown, but there are many hypotheses about what dark matter could consist of, as set out in the table below.\nBaryonic matter.\nDark matter can refer to any substance which interacts predominantly via gravity with visible matter (e.g., stars and planets). Hence in principle it need not be composed of a new type of fundamental particle but could, at least in part, be made up of standard baryonic matter, such as protons or neutrons. Most of the ordinary matter familiar to astronomers, including planets, brown dwarfs, red dwarfs, visible stars, white dwarfs, neutron stars, and black holes, fall into this category. A black hole would ingest both baryonic and non-baryonic matter that comes close enough to its event horizon; afterwards, the distinction between the two is lost. \nThese massive objects that are hard to detect are collectively known as MACHOs. Some scientists initially hoped that baryonic MACHOs could account for and explain all the dark matter.\nHowever, multiple lines of evidence suggest the majority of dark matter is not baryonic:\nNon-baryonic matter.\nIf baryonic matter cannot make up most of dark matter, then dark matter must be non-baryonic. There are two main candidates for non-baryonic dark matter: new hypothetical particles and primordial black holes. \nUnlike baryonic matter, nonbaryonic particles do not contribute to the formation of the elements in the early universe (Big Bang nucleosynthesis) and so its presence is felt only via its gravitational effects (such as weak lensing). In addition, some dark matter candidates can interact with themselves (self-interacting dark matter) or with ordinary particles (e.g. WIMPs or Weakly Interacting Massive Particles), possibly resulting in observable by-products such as gamma rays and neutrinos (indirect detection). Candidates abound (see the table above), each with their own strengths and weaknesses.\nUndiscovered massive particles.\nThere exists no formal definition of a Weakly Interacting Massive Particle, but broadly, it is an elementary particle which interacts via gravity and any other force (or forces) which is as weak as or weaker than the weak nuclear force, but also non-vanishing in strength. Many WIMP candidates are expected to have been produced thermally in the early Universe, similarly to the particles of the Standard Model according to Big Bang cosmology, and usually will constitute cold dark matter. Obtaining the correct abundance of dark matter today via thermal production requires a self-annihilation cross section of formula_1, which is roughly what is expected for a new particle in the 100\u00a0GeV mass range that interacts via the electroweak force.\nBecause supersymmetric extensions of the Standard Model of particle physics readily predict a new particle with these properties, this apparent coincidence is known as the \"WIMP miracle\", and a stable supersymmetric partner has long been a prime explanation for dark matter. Experimental efforts to detect WIMPs include the search for products of WIMP annihilation, including gamma rays, neutrinos and cosmic rays in nearby galaxies and galaxy clusters; direct detection experiments designed to measure the collision of WIMPs with nuclei in the laboratory, as well as attempts to directly produce WIMPs in colliders, such as the Large Hadron Collider at CERN. \nIn the early 2010s, results from direct-detection experiments along with the lack of evidence for supersymmetry at the Large Hadron Collider (LHC) experiment have cast doubt on the simplest WIMP hypothesis.\nUndiscovered ultralight particles.\nAxions are hypothetical elementary particles originally theorized in 1978 independently by Frank Wilczek and Steven Weinberg as the Goldstone boson of Peccei\u2013Quinn theory, which had been proposed in 1977 to solve the strong CP problem in quantum chromodynamics (QCD). QCD effects produce an effective periodic potential in which the axion field moves. Expanding the potential about one of its minima, one finds that the product of the axion mass with the axion decay constant is determined by the topological susceptibility of the QCD vacuum. An axion with mass much less than 60 keV is long-lived and weakly interacting: A perfect dark matter candidate.\nThe oscillations of the axion field about the minimum of the effective potential, the so-called misalignment mechanism, generate a cosmological population of cold axions with an abundance depending on the mass of the axion. With a mass above 5\u00a0\u03bceV/2 ( times the electron mass) axions could account for dark matter, and thus be both a dark-matter candidate and a solution to the strong CP problem. If inflation occurs at a low scale and lasts sufficiently long, the axion mass can be as low as 1\u00a0peV/2.\nBecause axions have extremely low mass, their de Broglie wavelength is very large, in turn meaning that quantum effects could help resolve the small-scale problems of the Lambda-CDM model. A single ultralight axion with a decay constant at the grand unified theory scale provides the correct relic density without fine-tuning.\nAxions as a dark matter candidate has gained in popularity in recent years, because of the non-detection of WIMPS.\nPrimordial black holes.\nPrimordial black holes are hypothetical black holes that formed soon after the Big Bang. In the inflationary era and early radiation-dominated universe, extremely dense pockets of subatomic matter may have been tightly packed to the point of gravitational collapse, creating primordial black holes without the supernova compression typically needed to make black holes today. Because the creation of primordial black holes would pre-date the first stars, they are not limited to the narrow mass range of stellar black holes and also not classified as baryonic dark matter.\nThe idea that black holes could form in the early universe was first suggested by Yakov Zeldovich and Igor Dmitriyevich Novikov in 1967, and independently by Stephen Hawking in 1971. It quickly became clear that such black holes might account for at least part of dark matter. Primordial black holes as a dark matter candidate has the major advantage that it is based on a well-understood theory (General Relativity) and objects (black holes) that are already known to exist. However, producing primordial black holes requires exotic cosmic inflation or physics beyond the standard model of particle physics, and might also require fine-tuning. Primordial black holes can also span nearly the entire possible mass range, from atom-sized to supermassive.\nThe idea that primordial black holes make up dark matter gained prominence in 2015 following results of gravitational wave measurements which detected the merger of intermediate-mass black holes. Black holes with about 30\u00a0solar masses are not predicted to form by either stellar collapse (typically less than 15\u00a0solar masses) or by the merger of black holes in galactic centers (millions or billions of solar masses), which suggests that the detected black holes might be primordial. A later survey of about a thousand supernovae detected no gravitational lensing events, when about eight would be expected if intermediate-mass primordial black holes above a certain mass range accounted for over 60% of dark matter. However, that study assumed that all black holes have the same or similar mass to the LIGO/Virgo mass range, which might not be the case (as suggested by subsequent James Webb Space Telescope observations).\nThe possibility that atom-sized primordial black holes account for a significant fraction of dark matter was ruled out by measurements of positron and electron fluxes outside the Sun's heliosphere by the \"Voyager\u00a01\" spacecraft. Tiny black holes are theorized to emit Hawking radiation. However the detected fluxes were too low and did not have the expected energy spectrum, suggesting that tiny primordial black holes are not widespread enough to account for dark matter. Nonetheless, research and theories proposing dense dark matter accounts for dark matter continue as of 2018, including approaches to dark matter cooling, and the question remains unsettled. In 2019, the lack of microlensing effects in the observation of Andromeda suggests that tiny black holes do not exist.\nNonetheless, there still exists a largely unconstrained mass range smaller than that which can be limited by optical microlensing observations, where primordial black holes may account for all dark matter.\nDark matter aggregation and dense dark matter objects.\nIf dark matter is composed of weakly interacting particles, then an obvious question is whether it can form objects equivalent to planets, stars, or black holes. Historically, the answer has been it cannot, because of two factors:\nDetection of dark matter particles.\nIf dark matter is made up of subatomic particles, then millions, possibly billions, of such particles must pass through every square centimeter of the Earth each second. Many experiments aim to test this hypothesis. Although WIMPs have been the main search candidates, axions have drawn renewed attention, with the Axion Dark Matter Experiment (ADMX) searches for axions and many more planned in the future. Another candidate is heavy hidden sector particles which only interact with ordinary matter via gravity.\nThese experiments can be divided into two classes: direct detection experiments, which search for the scattering of dark matter particles off atomic nuclei within a detector; and indirect detection, which look for the products of dark matter particle annihilations or decays.\nDirect detection.\nDirect detection experiments aim to observe low-energy recoils (typically a few keVs) of nuclei induced by interactions with particles of dark matter, which (in theory) are passing through the Earth. After such a recoil, the nucleus will emit energy in the form of scintillation light or phonons as they pass through sensitive detection apparatus. To do so effectively, it is crucial to maintain an extremely low background, which is the reason why such experiments typically operate deep underground, where interference from cosmic rays is minimized. Examples of underground laboratories with direct detection experiments include the Stawell mine, the Soudan mine, the SNOLAB underground laboratory at Sudbury, the Gran Sasso National Laboratory, the Canfranc Underground Laboratory, the Boulby Underground Laboratory, the Deep Underground Science and Engineering Laboratory and the China Jinping Underground Laboratory.\nThese experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100\u00a0mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include such projects as CDMS, CRESST, EDELWEISS, and EURECA, while noble liquid experiments include LZ, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO, which use alternative methods in their attempts to detect dark matter.\nCurrently there has been no well-established claim of dark matter detection from a direct detection experiment, leading instead to strong upper limits on the mass and interaction cross section with nucleons of such dark matter particles. The DAMA/NaI and more recent DAMA/LIBRA experimental collaborations have detected an annual modulation in the rate of events in their detectors, which they claim is due to dark matter. This results from the expectation that as the Earth orbits the Sun, the velocity of the detector relative to the dark matter halo will vary by a small amount. This claim is so far unconfirmed and in contradiction with negative results from other experiments such as LUX, SuperCDMS and XENON100.\nA special case of direct detection experiments covers those with directional sensitivity. This is a search strategy based on the motion of the Solar System around the Galactic Center. A low-pressure time projection chamber makes it possible to access information on recoiling tracks and constrain WIMP-nucleus kinematics. WIMPs coming from the direction in which the Sun travels (approximately towards Cygnus) may then be separated from background, which should be isotropic. Directional dark matter experiments include DMTPC, DRIFT, Newage and MIMAC.\nIndirect detection.\nIndirect detection experiments search for the products of the self-annihilation or decay of dark matter particles in outer space. For example, in regions of high dark matter density (e.g., the centre of the Milky Way) two dark matter particles could annihilate to produce gamma rays or Standard Model particle\u2013antiparticle pairs. Alternatively, if a dark matter particle is unstable, it could decay into Standard Model (or other) particles. These processes could be detected indirectly through an excess of gamma rays, antiprotons or positrons emanating from high density regions in the Milky Way and other galaxies. A major difficulty inherent in such searches is that various astrophysical sources can mimic the signal expected from dark matter, and so multiple signals are likely required for a conclusive discovery.\nA few of the dark matter particles passing through the Sun or Earth may scatter off atoms and lose energy. Thus dark matter may accumulate at the center of these bodies, increasing the chance of collision/annihilation. This could produce a distinctive signal in the form of high-energy neutrinos. Such a signal would be strong indirect proof of WIMP dark matter. High-energy neutrino telescopes such as AMANDA, IceCube and ANTARES are searching for this signal. The detection by LIGO in September 2015 of gravitational waves opens the possibility of observing dark matter in a new way, particularly if it is in the form of primordial black holes.\nMany experimental searches have been undertaken to look for such emission from dark matter annihilation or decay, examples of which follow.\nThe Energetic Gamma Ray Experiment Telescope observed more gamma rays in 2008 than expected from the Milky Way, but scientists concluded this was most likely due to incorrect estimation of the telescope's sensitivity.\nThe Fermi Gamma-ray Space Telescope is searching for similar gamma rays. In 2009, an as yet unexplained surplus of gamma rays from the Milky Way's galactic center was found in Fermi data. This Galactic Center GeV excess might be due to dark matter annihilation or to a population of pulsars. In April 2012, an analysis of previously available data from Fermi's Large Area Telescope instrument produced statistical evidence of a 130\u00a0GeV signal in the gamma radiation coming from the center of the Milky Way. WIMP annihilation was seen as the most probable explanation.\nAt higher energies, ground-based gamma-ray telescopes have set limits on the annihilation of dark matter in dwarf spheroidal galaxies and in clusters of galaxies.\nThe PAMELA experiment (launched in 2006) detected excess positrons. They could be from dark matter annihilation or from pulsars. No excess antiprotons were observed.\nIn 2013, results from the Alpha Magnetic Spectrometer on the International Space Station indicated excess high-energy cosmic rays which could be due to dark matter annihilation.\nCollider searches for dark matter.\nAn alternative approach to the detection of dark matter particles in nature is to produce them in a laboratory. Experiments with the Large Hadron Collider (LHC) may be able to detect dark matter particles produced in collisions of the LHC proton beams. Because a dark matter particle should have negligible interactions with normal visible matter, it may be detected indirectly as (large amounts of) missing energy and momentum that escape the detectors, provided other (non-negligible) collision products are detected. Constraints on dark matter also exist from the LEP experiment using a similar principle, but probing the interaction of dark matter particles with electrons rather than quarks. Any discovery from collider searches must be corroborated by discoveries in the indirect or direct detection sectors to prove that the particle discovered is, in fact, dark matter.\nAlternative hypotheses.\nBecause dark matter has not yet been identified, many other hypotheses have emerged aiming to explain the same observational phenomena without introducing a new unknown type of matter. The theory underpinning most observational evidence for dark matter, general relativity, is well-tested on Solar System scales, but its validity on galactic or cosmological scales has not been well proven. A suitable modification to general relativity can in principle conceivably eliminate the need for dark matter. The best-known theories of this class are MOND and its relativistic generalization tensor\u2013vector\u2013scalar gravity (TeVeS), f(R) gravity, negative mass, dark fluid, and entropic gravity. Alternative theories abound.\nA problem with alternative hypotheses is that observational evidence for dark matter comes from so many independent approaches (see the \"observational evidence\" section above). Explaining any individual observation is possible but explaining all of them in the absence of dark matter is very difficult. Nonetheless, there have been some scattered successes for alternative hypotheses, such as a 2016 test of gravitational lensing in entropic gravity and a 2020 measurement of a unique MOND effect.\nThe prevailing opinion among most astrophysicists is that while modifications to general relativity can conceivably explain part of the observational evidence, there is probably enough data to conclude there must be some form of dark matter present in the universe.\nIn popular culture.\nDark matter regularly appears as a topic in hybrid periodicals that cover both factual scientific topics and science fiction, and dark matter itself has been referred to as \"the stuff of science fiction\".\nMention of dark matter is made in works of fiction. In such cases, it is usually attributed extraordinary physical or magical properties, thus becoming inconsistent with the hypothesized properties of dark matter in physics and cosmology. For example:\nMore broadly, the phrase \"dark matter\" is used metaphorically in fiction to evoke the unseen or invisible."}
{"id": "8653", "revid": "48588006", "url": "https://en.wikipedia.org/wiki?curid=8653", "title": "Ducati", "text": "Ducati Motor Holding S.p.A () is an Italian motorcycle manufacturing company headquartered in Bologna, Italy. The company is directly owned by Italian automotive manufacturer Lamborghini, whose German parent company is Audi, itself owned by the Volkswagen Group.\nHistory.\nIn 1926 Antonio Cavalieri Ducati and his three sons, Adriano, Marcello, and Bruno, founded \"Societ\u00e0 Scientifica Radiobrevetti Ducati\" (SSR Ducati) in Bologna to produce vacuum tubes, capacitors, and other radio components. In 1935 they had become successful enough to enable construction of a new factory in the Borgo Panigale area of the city. Production was maintained during World War II, despite the Ducati factory being a repeated target of Allied bombing. It was finally destroyed by around 40 Consolidated B-24 Liberators on 12 October 1944 as part of the United States Army Air Forces's Operation Pancake, which involved some 700 aircraft flying from airfields in the Province of Foggia.\n Nonetheless, it maintained production.\nMeanwhile, at the small Turinese firm SIATA (\"Societ\u00e0 Italiana per Applicazioni Tecniche Auto-Aviatorie\"), Aldo Farinelli began developing a small pushrod engine for mounting on bicycles. Barely a month after the official liberation of Italy in 1944, SIATA announced its intention to sell this engine, called the \"Cucciolo\" (Italian for \"puppy,\" in reference to the distinctive exhaust sound) to the public. The first Cucciolos were available alone, to be mounted on standard bicycles, by the buyer; however, businessmen soon bought the little engines in quantity, and offered complete motorized-bicycle units for sale.\nIn 1950, after more than 200,000 Cucciolos had been sold, in collaboration with SIATA, the Ducati firm finally offered its own Cucciolo-based motorcycle. This first Ducati motorcycle was a 48\u00a0cc bike weighing , with a top speed of , and had a giving just under . Ducati soon dropped the Cucciolo name in favor of \"55M\" and \"65TL\".\nWhen the market moved toward larger motorcycles, Ducati management decided to respond, making an impression at an early-1952 Milan show, introducing their 65TS cycle and Cruiser (a four-stroke motor scooter). Despite being described as the most interesting new machine at the 1952 show, the Cruiser was not a great success, and only a few thousand were made over a two-year period before the model ceased production.\nIn 1953, management split the company into two separate entities, Ducati Meccanica SpA and Ducati Elettronica, in acknowledgment of its diverging motorcycle and electronics product lines. Dr. Giuseppe Montano took over as head of Ducati Meccanica SpA and the Borgo Panigale factory was modernized with government assistance. By 1954, Ducati Meccanica SpA had increased production to 120 bikes a day.\nIn the 1960s, Ducati earned its place in motorcycling history by producing the fastest 250\u00a0cc road bike then available, the Mach 1. In the 1970s Ducati began producing motorcycles with large-displacement V-twin engines, which Ducati branded as \"L-twin\" for their 90\u00b0 angle, and in 1973, introduced their trademarked desmodromic valve design. In 1985, Cagiva bought Ducati and planned to rebadge Ducati motorcycles with the \"Cagiva\" name. By the time the purchase was completed, Cagiva kept the \"Ducati\" name on its motorcycles. Eleven years later, in 1996, Cagiva accepted the offer from Texas Pacific Group and sold a 51% stake in the company for US$325 million; then, in 1998, Texas Pacific Group bought most of the remaining 49% to become the sole owner of Ducati. In 1999, TPG issued an initial public offering of Ducati stock and renamed the company \"Ducati Motor Holding SpA\". TPG sold over 65% of its shares in Ducati, leaving TPG the majority shareholder. In December 2005, Ducati returned to Italian ownership with the sale of Texas Pacific's stake (minus one share) to Investindustrial Holdings, the investment fund of Carlo and Andrea Bonomi.\nIn April 2012, Volkswagen Group's Audi subsidiary announced its intention to buy Ducati for \u20ac (US$). Volkswagen chairman Ferdinand Pi\u00ebch, a motorcycle enthusiast, had long coveted Ducati, and had regretted that he passed up an opportunity to buy the company from the Italian government in 1984. Analysts doubted a tiny motorcycle maker would have a meaningful effect on a company the size of Volkswagen, commenting that the acquisition has \"a trophy feel to it,\" and, \"is driven by VW's passion for nameplates rather than industrial or financial logic\". Italian luxury car brand Lamborghini was strengthened under VW ownership. AUDI AG's Automobili Lamborghini S.p.A. subsidiary acquired 100 percent of the shares of Ducati Motor Holding S.p.A. on 19 July 2012 for \u20ac (US$).\nOwnership.\nSince 1926, Ducati has been owned by a number of groups and companies.\n From the 1960s to the 1990s, the Spanish company MotoTrans licensed Ducati engines and produced motorcycles that, although they incorporated subtle differences, were clearly Ducati-derived. MotoTrans's most notable machine was the 250\u00a0cc \"24 Horas\" (Spanish for \"24 hours\").\nMotorcycle designs.\nDucati is best known for high-performance motorcycles characterized by large-capacity four-stroke, 90\u00b0 V-twin engines, with a desmodromic valve design. Ducati branded his configuration as L-twin because one cylinder is vertical while the other is horizontal, making it look like a letter \"L\". Ducati's desmodromic valve design is nearing its 50th year of use. Desmodromic valves are closed with a separate, dedicated cam lobe and lifter instead of the conventional valve springs used in most internal combustion engines in consumer vehicles. This allows the cams to have a more radical profile, thus opening and closing the valves more quickly without the risk of valve-float, which causes a loss of power that is likely when using a \"passive\" closing mechanism under the same conditions.\nWhile most other manufacturers use wet clutches (with the spinning parts bathed in oil) Ducati previously used multiplate dry clutches in many of their motorcycles. The dry clutch eliminates the power loss from oil viscosity drag on the engine, even though the engagement may not be as smooth as the oil-bath versions, but the clutch plates can wear more rapidly. Ducati has converted to wet clutches across their current product lines.\nDucati also extensively uses a trellis frame, although Ducati's MotoGP project broke with this tradition by introducing a revolutionary carbon fibre frame for the Ducati Desmosedici GP9.\nProduct history.\nThe chief designer of most Ducati motorcycles in the 1950s was Fabio Taglioni (1920\u20132001). His designs ranged from the small single-cylinder machines that were successful in the Italian 'street races' to the large-capacity twins of the 1980s. Ducati introduced the Pantah in 1979; its engine was updated in the 1990s in the Ducati SuperSport (SS) series. All modern Ducati engines are derivatives of the Pantah, which uses a toothed belt to actuate the engine's valves. Taglioni used the Cavallino Rampante (identified with the Ferrari brand) on his Ducati motorbikes. Taglioni chose this emblem of courage and daring as a sign of respect and admiration for Francesco Baracca, a World War I fighter pilot who died during an air raid in 1918.\n1960s.\nIn addition to manufacturing two-wheelers, Ducati also assembled Triumph Heralds for sale in the Italian market in their Borgo Panigale plant beginning in early 1963.\n1970s.\nIn 1973, Ducati commemorated its 1972 win at the Imola 200 with the production model green frame Ducati 750 SuperSport.\nDucati also targeted the offroad market with the two-stroke Regolarit\u00e0 125, building 3,486 models from 1975 to 1979, but the bike was not successful.\nIn 1975, the company introduced the 860 GT, designed by noted car stylist Giorgetto Giugiaro. Its angular lines were unique, but raised handlebars made for an uncomfortable seating position at high speeds and also caused steering issues. The 860GT's angular styling was a sales disaster, and it was hurriedly re-designed for the 1976 season with a more rounded fuel tank.\nIn 1975 Ducati offered hand-built production racers, the 'square case' 750SS and later 900SS models, built in limited numbers. Sales of the 900SS proved so strong, and sales of the 860GT/GTE/GTS so weak, that production of the 900SS was ramped up, and it became Ducati's #1 selling model.\n1980s.\nDucati's liquid-cooled, multi-valve 90\u00b0 V-twins, made from 1985 on, are known as \"Desmoquattro\" (\"desmodromic valve four\"). These include the 851, 916 and 996, 999 and a few predecessors and derivatives.\nThe Ducati Paso was introduced in 1986 with the Paso 750, followed in 1989 with the Paso 906. The final version came in 1991 with the 907IE (Iniezione Elettronica), now without the name \"Paso\". The design was from the hand of Massimo Tamburini, who also designed the Ducati 916 and MV Agusta F4. The Paso was a typical \"you love it, you hate it\" bike. However, at that time it looked like that all-enclosed bodywork would be the future for all motorcycles. The Paso design was copied for the and . Together with Tamburini's Bimota DB1, they were enormously influential in terms of styling.\n1990s.\nIn 1993, Miguel Angel Galluzzi introduced the Ducati Monster, a naked bike with exposed trellis and engine. Today the Monster accounts for almost half of the company's worldwide sales. The Monster has undergone the most changes of any motorcycle that Ducati has ever produced.\nIn 1993, Pierre Terblanche, Massimo Bordi and Claudio Domenicali designed the Ducati Supermono. A 550\u00a0cc single-cylinder lightweight \"Catalog Racer\". Only 67 were built between 1993 and 1997.\nIn 1994, the company introduced the Ducati 916 model designed by Massimo Tamburini, a water-cooled version that allowed for higher output levels and a striking new bodywork that had aggressive lines, an underseat exhaust, and a single-sided swingarm. Ducati has since ceased production of the 916, supplanting it (and its progeny, the 748, 996 and 998) with the 749 and 999.\n2000s.\nIn 2006, the retro-styled Ducati PaulSmart 1000 LE was released, which shared styling cues with the 1973 750 SuperSport (itself a production replica of Paul Smart's 1972 race winning 750 Imola Desmo), as one of a SportClassic series representing the 750 GT, 750 Sport, and 750 SuperSport Ducati motorcycles.\nMotorcycle design history.\nDucati has produced several styles of motorcycle engines, including varying the number of cylinders, type of valve actuation and fuel delivery. Ducati is best known for its 90\u00b0 V-twin engine, used on nearly all Ducatis since the 1970s. Ducati brands its engine as \"L-twin\", emphasizing the 90\u00b0 V angle, to create product differentiation from competing V-twin motorcycles. Ducati has also made other engine types, mostly before the 1970s, with one, two, three, or four cylinders; operated by pull rod valves and push rod valves; single, double and triple overhead camshafts; two-stroke and even at one stage manufactured small diesel engines, many of which were used to power boats, generators, garden machinery and emergency pumps (for example, for fire fighting). The engines were the IS series from air-cooled and the larger twin DM series water- and air-cooled. The engines have been found in all parts of the globe. Wisconsin Diesel even assembled and \"badge engineered\" the engines in the USA. They have also produced outboard motors for marine use. Currently, Ducati makes no other engines except for its motorcycles.\nOn current Ducati motors, except for the Desmosedici and 1199 Panigale, the valves are actuated by a standard valve cam shaft which is rotated by a timing belt driven by the motor directly. The teeth on the belt keep the camshaft drive pulleys indexed. On older Ducati motors, prior to 1986, drive was by solid shaft that transferred to the camshaft through bevel-cut gears. This method of valve actuation was used on many of Ducati's older single-cylinder motorcycles \u2014 the shaft tube is visible on the outside of the cylinder.\nDucati is also famous for using the desmodromic valve system championed by engineer and designer Fabio Taglioni, though the firm has also used engines that use valve springs to close their valves. In the early days, Ducati reserved the desmodromic valve heads for its higher performance bikes and its race bikes. These valves do not suffer from valve float at high engine speeds, thus a desmodromic engine is capable of far higher revolutions than a similarly configured engine with traditional spring-valve heads.\nIn the 1960s and 1970s, Ducati produced a wide range of small two-stroke bikes, mainly sub-100\u00a0cc capacities. Large quantities of some models were exported to the United States.\nDucati has produced the following motorcycle engine types:\nEnthusiasts groups.\nA key part of Ducati's marketing strategy since the 1990s has been fostering a distinct community identity in connection with branding efforts including online communities and local, regional, and national Ducati enthusiast clubs. There are more than 400 Ducati clubs worldwide and 20,000 registered users of the Ducati Owners Club web site and 17,000 subscribers to the racing web site. Enthusiasts and riders are informally referred to in the motorcycling community as Ducatista (singular) or Ducatisti (plural).\nIn North America there are several Ducati enthusiasts organizations with varying degrees of factory sponsorship, such as the Bay Area Desmo Owners Club (BADOC) located in and around the city of San Francisco, CA. Ducati Riders of Illinois (DRILL) located in Chicago, IL. DESMO, the Ducati Enthusiast Sport Motorcycle Organization, is a North American group affiliated with the factory Desmo Owners Club. Some groups are focused on vintage Ducatis while several are based primarily or entirely on email discussion lists or web forums.\nMerchandising.\nDucati has a wide range of accessories, lifestyle products and co-branded merchandise bearing their logos and designs. The company has a licensing agreement with Tumi Inc., launching a collection of eight co-branded luggage pieces in 2006, sold through both of the brands' retail outlets.\nRacing history.\nDucati's history with motorsport began with speed records on Cucciolo motorized bicycle factory racers in 1951, followed in 1954 with bringing in Fabio Taglioni to found a road-racing program with the 100 Gran Sport. , Ducati was still pursuing the \"win on Sunday, sell on Monday\" business model and spending 10% of company revenues\u20ac, , on its racing business.\nMotoGP World Championship.\nDucati rejoined Grand Prix motorcycle racing in , after a 30-year absence. On 23 September 2007, Casey Stoner clinched his and Ducati's first Grand Prix World Championship.\nWhen Ducati re-joined MotoGP in , MotoGP had changed its rules to allow four-stroke 990\u00a0cc engines to race. At the time Ducati was the fastest bike. In , MotoGP reduced the engine size to \"\", and Ducati continued to be the fastest with a bike that was markedly quicker than its rivals as was displayed by Casey Stoner on tracks with long straights.\nFor , Ducati Marlboro Team campaigned their Desmosedici GP9 with former World Champions Casey Stoner and Nicky Hayden. Ducati also supplied customer bikes to Pramac Racing, with Mika Kallio and Niccol\u00f2 Canepa riding for the team in 2009.\nNine-time world champion Valentino Rossi rode for Ducati Corse for the and seasons.\nRossi returned to the Yamaha team for the 2013 season.\nFor , Ducati Team raced with Nicky Hayden and the Italian rider Andrea Dovizioso. In 2014 Cal Crutchlow teamed up with Dovizioso for the season, and he left at the end of the year.\nIn , Ducati Team, under the control of the new race team director Gigi Dall'Igna and the new Desmosedici GP15, raced with two Italian riders: Andrea Dovizioso and Andrea Iannone. Dovizioso and Iannone returned for another season in with Michele Pirro as official tester. As well as this, Casey Stoner also tested Ducati machinery during the season.\n In and , Ducati Team rider Andrea Dovizioso raced with his new teammate Jorge Lorenzo, who joined the Ducati team from Yamaha Factory Racing with a two seasons contract. In 2019, Danilo Petrucci joined Dovizioso at the factory team.\nIn , Despite suffering five DNF's, four of which were individual errors throughout the 2022 season, Bagnaia became the newest MotoGP world champion today in Valencia. The Ducati rider also became the Italian manufacturer's second-ever MotoGP champion after Casey Stoner, and first in 15 years.\nSuperbike World Championship (SBK).\nThe company has won 16 riders world championships and 19 manufacturers world championships, competing since the series' inception in 1988. At the end of 2015, Ducati has amassed 318 wins, more than any other manufacturer involved in the championship.\nFIM Superstock 1000 Cup.\nDucati has also won the manufacturers' championship for years 2008\u20132009, 2011 and 2016.\nBritish Superbike Championship.\nDucati has won the British Superbike Championship twelve times.\nAMA Superbike Championship.\nIn the AMA Superbike Championship, Ducati has had its share of success, with Doug Polen winning the title in 1993 and Troy Corser the following year in 1994. Ducati has entered a bike in every AMA Superbike season since 1986, but withdrew from the series after the 2006 season.\nDucati had an important place in early Superbike racing history in the United States and vice versa: In 1977, \"Cycle\" magazine editors Cook Neilson and Phil Schilling took a Ducati 750SS to first place at Daytona in the second-ever season of AMA Superbike racing. \"Neilson retired from racing at the end of the year, but the bike he and Schilling built \u2014 nicknamed Old Blue for its blue livery \u2014 became a legend,\" says Richard Backus from \"Motorcycle Classics\": \"How big a legend? Big enough for Ducati to team with Italian specialty builder NCR to craft a limited-edition update, New Blue, based on the 2007 Sport 1000S, and big enough to inspire the crew at the Barber Vintage Motorsports Museum (see Barber Motorsports Park), arguably one of the most important motorcycle museums in the world, to commission Ducati specialist Rich Lambrechts to craft a bolt-by-bolt replica for its collection. The finished bike's name? Deja Blue.\"\nFormula TT.\nDucati's first ever world title was the 1978 TT Formula 1 World Championship, achieved thanks to Mike Hailwood's victory at the Isle of Man TT. Between 1981 and 1984 Tony Rutter won four TT Formula 2 World Championships riding Ducati bikes."}
{"id": "8654", "revid": "1782949", "url": "https://en.wikipedia.org/wiki?curid=8654", "title": "Data General Nova", "text": "The Nova is a series of 16-bit minicomputers released by the American company Data General. The Nova family was very popular in the 1970s and ultimately sold tens of thousands of units.\nThe first model, known simply as \"Nova\", was released in 1969. The Nova was packaged into a single 3U rack-mount case and had enough computing power to handle most simple tasks. The Nova became popular in science laboratories around the world. It was followed the next year by the SuperNOVA, which ran roughly four times as fast, making it the fastest mini for several years.\nIntroduced during a period of rapid progress in integrated circuit (or \"microchip\") design, the line went through several upgrades over the next five years, introducing the 800 and 1200, the Nova 2, Nova 3, and ultimately the Nova 4. A single-chip implementation was also introduced as the microNOVA in 1977, but did not see widespread use as the market moved to new microprocessor designs. Fairchild Semiconductor also introduced a microprocessor version of the Nova in 1977, the Fairchild 9440, but it also saw limited use in the market.\nThe Nova line was succeeded by the Data General Eclipse, which was similar in most ways but added virtual memory support and other features required by modern operating systems. A 32-bit upgrade of the Eclipse resulted in the Eclipse MV series of the 1980s.\nHistory.\nEdson de Castro and the PDP-X.\nEdson de Castro was the Product Manager of the pioneering Digital Equipment Corporation (DEC) PDP-8, a 12-bit computer widely referred to as the first true minicomputer. He also led the design of the upgraded PDP-8/I, which used early integrated circuits in place of individual transistors.\nDuring the PDP-8/I process, de Castro had been visiting circuit board manufacturers who were making rapid advances in the complexity of the boards they could assemble. de Castro concluded that the 8/I could be produced using fully automated assembly on large boards, which would have been impossible only a year earlier. Others within DEC had become used to the smaller boards used in earlier machines and were concerned about tracking down problems when there were many components on a single board. For the 8/I, the decision was made to stay with small boards, using the new \"flip-chip\" packaging for a modest improvement in density.\nDuring the period when the PDP-8 was being developed, the introduction of ASCII and its major update in 1967 led to a new generation of designs with word lengths that were multiples of 8 bits rather than multiples of 6 bits as in most previous designs. This led to mid-range designs working at 16-bit word lengths instead of DEC's current 12- and 18-bit lineups. de Castro was convinced that it was possible to improve upon the PDP-8 by building a 16-bit minicomputer CPU on a single 15-inch square board.\nIn 1967, de Castro began a new design effort known as \"PDP-X\" which included several advanced features. Among these was a single underlying design that could be used to build 8-, 16-, and 32-bit platforms. This progressed to the point of producing several detailed architecture documents. Ken Olsen was not supportive of this project, feeling it did not offer sufficient advantages over the 12-bit PDP-8 and the 18-bit PDP-9. It was eventually canceled in the spring of 1968.\nDesign of the Nova.\nCancelation of the PDP-X prompted de Castro to consider leaving DEC to build a system on his own. He was not alone; in late 1967 a group of like-minded engineers formed to consider such a machine. The group included Pat Green, a divisional manager; Richard Sogge, another hardware engineer; and Henry Burkhardt III, a software engineer. In contrast to the PDP-X, the new effort focused on a single machine that could be brought to market quickly, as de Castro felt the PDP-X concept was far too ambitious for a small startup company.\nDiscussing it with the others at DEC, the initial concept led to an 8-bit machine which would be less costly to implement. The group began talking with Herbert Richman, a salesman for Fairchild Semiconductor who knew the others through his contacts with DEC. At the time, Fairchild was battling with Texas Instruments and Signetics in the rapidly growing TTL market and were introducing new fabs that allowed more complex designs. Fairchild's latest 9300 series allowed up to 96 gates per chip, and they had used this to implement a number of 4-bit chips like binary counters and shift registers.\nUsing these ICs reduced the total IC count needed to implement a complete arithmetic logic unit (ALU), the core mathematical component of a CPU, allowing the expansion from an 8-bit design to 16-bit. This did require the expansion of the CPU from a single printed circuit board to two, but such a design would still be significantly cheaper to produce than the PDP-8/I while still being more powerful and ASCII-based. A third board held the input/output circuitry and a complete system typically included another board with 4\u00a0kB of random-access memory. A complete four-card system fit in a single rackmount chassis.\nThe boards were designed so they could be connected together using a printed circuit backplane, with minimal manual wiring, allowing all the boards to be built in an automated fashion. This greatly reduced costs over the PDP-8/I, which consisted of many smaller boards that had to be wired together at the backplane, which was itself connected together using wire wrap. The larger-board construction also made the Nova more reliable, which made it especially attractive for industrial or lab settings.\nThe new design used a simple load\u2013store architecture which would reemerge in the RISC designs in the 1980s. Because the complexity of a flip-flop was being rapidly reduced as they were implemented in chips, the design offset the lack of addressing modes of the load\u2013store design by adding four general-purpose accumulators, instead of the single register that would be found in similar low-cost offerings like the PDP series.\nNova introduction.\nLate in 1967, Richman introduced the group to New York-based lawyer Fred Adler, who began canvassing various funding sources for seed capital. By 1968, Adler had arranged a major funding deal with a consortium of venture capital funds from the Boston area, who agreed to provide an initial investment with a second available for production ramp-up. de Castro, Burkhart and Sogge quit DEC and started Data General (DG) on 15 April 1968. Green did not join them, considering the venture too risky, and Richman did not join until the product was up and running later in the year.\nWork on the first system took about nine months, and the first sales efforts started that November. They had a bit of luck because the Fall Joint Computer Conference had been delayed until December that year, so they were able to bring a working unit to San Francisco where they ran a version of \"Spacewar!\". DG officially released the Nova in 1969 at a base price of , advertising it as \"the best small computer in the world.\" The basic model was not very useful out of the box, and adding () RAM in the form of core memory typically brought the price up to . In contrast, a PDP-8/I with () was priced at .\nThe first sale was to a university in Texas, with the team hand-building an example which shipped out in February. However, this was in the midst of a strike in the airline industry and the machine never arrived. They sent a second example, which arrived promptly as the strike had ended by that point, and in May the original one was finally delivered as well.\nThe system was successful from the start, with the 100th being sold after six months, and the 500th after 15 months. Sales accelerated as newer versions were introduced, and by 1975 the company had annual sales of .\nSuperNOVA.\nKen Olsen had publicly predicted that DG would fail, but with the release of the Nova it was clear that was not going to happen. By this time, a number of other companies were talking about introducing 16-bit designs as well. Olsen decided these presented a threat to their 18-bit line as well as 12-bit, and began a new 16-bit design effort. This emerged in 1970 as the PDP-11, a much more complex design that was as different from the PDP-X as the Nova was. The two designs competed heavily in the market.\nRumors of the new system from DEC reached DG shortly after the Nova began shipping. In spring 1970 they hired a new designer, Larry Seligman, to leapfrog any possible machine in the making. Two major changes had taken place since the Nova was designed; one was that Signetics had introduced the 8260, a 4-bit IC that combined an adder, XNOR and AND, meaning the number of chips needed to implement the basic logic was reduced by about three times. Another was that Intel was aggressively talking up semiconductor-based memories, promising 1024 bits on a single chip and running at much higher speeds than core memory.\nSeligman's new design took advantage of both of these improvements. To start, the new ICs allowed the ALU to be expanded to full 16-bit width on the same two cards, allowing it to carry out math and logic operations in a single cycle and thereby making the new design four times as fast as the original. In addition, new smaller core memory was used that improved the cycle time from the original's 1,200\u00a0ns to 800\u00a0ns, offering a further improvement. Performance could be further improved by replacing the core with read-only memory; lacking core's read\u2013write cycle, this could be accessed in 300\u00a0ns for a dramatic performance boost.\nThe resulting machine, known as the SuperNOVA, was released in 1970. Although the initial models still used core, the entire design was based on the premise that faster semiconductor memories would become available and the platform could make full use of them. This was introduced later the same year as the SuperNOVA SC, featuring semiconductor (SC) memory. The much higher performance memory allowed the CPU, which was synchronous with memory, to be further increased in speed to run at a 300\u00a0ns cycle time (3.3\u00a0MHz). This made it the fastest available minicomputer for many years. Initially the new memory was also very expensive and ran hot, so it was not widely used.\n1200 and 800.\nAs a demonstration of the power of their Micromatrix gate array technology, in 1968 Fairchild prototyped the 4711, a single-chip 4-bit ALU. The design was never intended for mass production and was quite expensive to produce. The introduction of the Signetics 8260 in 1969 forced their hand; both Texas Instruments and Fairchild introduced 4-bit ALUs of their own in 1970, the 74181 and 9341, respectively. In contrast to the 8260, the new designs offered all common logic functions and further reduced the chip count.\nThis led DG to consider the design of a new CPU using these more integrated ICs. At a minimum, this would reduce the CPU to a single card for either the basic Nova or the SuperNOVA. A new concept emerged where a single chassis would be able to host either machine simply by swapping out the CPU circuit board. This would allow customers to purchase the lower-cost system and then upgrade at any time.\nWhile Seligman was working on the SuperNOVA, the company received a letter from Ron Gruner stating \"I've read about your product, I've read your ads, and I'm going to work for you. And I'm going to be at your offices in a week to talk to you about that.\" He was hired on the spot. Gruner was put in charge of the low-cost machine while Seligman designed a matching high-performance version.\nGruner's low-cost model launched in 1970 as the Nova 1200, the 1200 referring to the use of the original Nova's 1,200\u00a0ns core memory. It featured a 4-bit ALU based on a single 74181 chip, and was thus essentially a repackaged Nova. Seligman's repackaged four-ALU SuperNOVA was released in 1971 as the Nova 800, resulting in the somewhat confusing naming where the lower-numbered model has higher performance. Both models were offered in a variety of cases, the 1200 with seven slots, the 1210 with four and the 1220 with fourteen.\nLater models.\nBy this time, the PDP-11 was finally shipping. It offered a much richer instruction set architecture than the deliberately simple one in the Nova. Continuing improvement in IC designs, and especially their price\u2013performance ratio, was eroding the value of the original simplified instructions. Seligman was put in charge of designing a new machine that would be compatible with the Nova while offering a much richer environment for those who wanted it. This concept shipped as the Data General Eclipse series, which offered the ability to add additional circuitry to tailor the instruction set for scientific or data processing workloads. The Eclipse was successful in competing with the PDP-11 at the higher end of the market.\nAround the same time, rumors of a new 32-bit machine from DEC began to surface. DG decided they had to have a similar product, and Gruner was put in charge of what became the Fountainhead Project. Given the scope of the project, they agreed that the entire effort should be handled off-site, and Gruner selected a location at Research Triangle Park in North Carolina. This design became very complex and was ultimately canceled years later.\nWhile these efforts were underway, work on the Nova line continued.\n840.\nThe 840, first offered in 1973, also included a new paged memory system allowing for addresses of up to 17-bits. An index offset the base address into the larger 128\u00a0kword memory. Actually installing this much memory required considerable space; the 840 shipped in a large 14-slot case.\nNova 2.\nThe next version was the Nova 2, with the first versions shipping in 1973. The Nova 2 was essentially a simplified version of the earlier machines as increasing chip densities allowed the CPU to be reduced in size. While the SuperNOVA used three 15\u00d715\" boards to implement the CPU and its memory, the Nova 2 fitted all of this onto a single board. ROM was used to store the boot code, which was then copied into core when the \"program load\" switch was flipped. Versions were available with four (\"2/4\"), seven and ten (\"2/10\") slots.\nNova 3.\nThe Nova 3 of 1975 added two more registers, used to control access to a built-in stack. The processor was also re-implemented using TTL components, further increasing the performance of the system. The Nova 3 was offered in four-slot (the Nova 3/4) and twelve-slot (the Nova 3/12) versions.\nNova 4.\nIt appears that Data General originally intended the Nova 3 to be the last of its line, planning to replace the Nova with the later Eclipse machines. However, continued demand led to a Nova 4 machine introduced in 1978, this time based on four AMD Am2901 bit-slice ALUs. This machine was designed from the start to be both the Nova 4 and the Eclipse S/140, with different microcode for each. A floating-point co-processor was also available, taking up a separate slot. An additional option allowed for memory mapping, allowing programs to access up to 128\u00a0kwords of memory using bank switching. Unlike the earlier machines, the Nova 4 did not include a front panel console and instead included a ROM containing machine code that allows a terminal to emulate a console when needed.\nThere were three different versions of the Nova 4, the Nova 4/C, the Nova 4/S and the Nova 4/X. The Nova 4/C was a single-board implementation that included all of the memory (16 or 32\u00a0kwords). The Nova 4/S and 4/X used separate memory boards. The Nova 4/X had the on-board memory management unit (MMU) enabled to allow up to 128\u00a0kwords of memory to be used. The MMU was also installed in the Nova 4/S, but was disabled by firmware. Both the 4/S and the 4/X included a \"prefetcher\" to increase performance by fetching up to 11 instructions from memory before they were needed.\nmicroNOVA.\nData General also produced a series of microNOVA single-chip implementations of the Nova processor. To allow it to fit into a 40-pin dual in-line package (DIP) chip, the address bus and data bus shared a set of 16 pins. This meant that reads and writes to memory required two cycles, and that the machine ran about half the speed of the original Nova as a result.\nThe first chip in the series was the mN601, of 1977. This was sold both as a CPU for other users, a complete chipset for those wanting to implement a computer, a complete computer on a single board with 4\u00a0kB of RAM, and as a complete low-end model of the Nova. An upgraded version of the design, 1979's mN602, reduced the entire chipset to a single VLSI. This was offered in two machines, the microNOVA MP/100 and larger microNOVA MP/200.\nThe microNOVA was later re-packaged with a monitor in a PC-style case with two floppy disks as the Enterprise. Enterprise shipped in 1981, running RDOS, but the introduction of the IBM PC the same year made most other machines disappear under the radar.\nNova's legacy.\nThe Nova influenced the design of both the Xerox Alto (1973) and Apple I (1976) computers, and its architecture was the basis for the Computervision CGP (Computervision Graphics Processor) series. Its external design has been reported to be the direct inspiration for the front panel of the MITS Altair (1975) microcomputer.\nData General followed up on the success of the original Nova with a series of faster designs. The Eclipse family of systems was later introduced with an extended upwardly compatible instruction set, and the MV-series further extended the Eclipse into a 32-bit architecture to compete with the DEC VAX. The development of the MV-series was documented in Tracy Kidder's popular 1981 book, \"The Soul of a New Machine\". Data General itself would later evolve into a vendor of Intel processor-based servers and storage arrays, eventually being purchased by EMC.\nThere is a diverse but ardent group of people worldwide who restore and preserve original 16-bit Data General systems.\nTechnical description.\nProcessor design.\nThe Nova, unlike the PDP-8, was a load\u2013store architecture. It had four 16-bit accumulator registers, two of which (2 and 3) could be used as index registers. There was a 15-bit program counter and a single-bit carry register. As with the PDP-8, current + zero page addressing was central. There was no stack register, but later Eclipse designs would utilize a dedicated hardware memory address for this function.\nThe earliest models of the Nova processed math serially in 4-bit packets, using a single 74181 bitslice ALU. A year after its introduction, this design was improved to include a full 16-bit parallel math unit using four 74181s, this design being referred to as the SuperNova. Future versions of the system added a stack unit and hardware multiply/divide.\nThe Nova 4 / Eclipse S/140 was based on four AMD 2901 bit-slice ALUs, with microcode in read-only memory, and was the first Nova designed for DRAM main memory only, without provision for magnetic-core memory.\nMemory and I/O.\nThe first models were available with 8 K words of magnetic-core memory as an option, one that practically everyone had to buy, bringing the system cost up to $7,995.\nThis core memory board was organized in planar fashion as four groups of four banks, each bank carrying two sets of core in a 64 by 64 matrix; thus there were 64 x 64 = 4096 bits per set, x 2 sets giving 8,192 bits, x 4 banks giving 32,768 bits, x 4 groups giving a total of 131,072 bits, and this divided by the machine word size of 16 bits gave 8,192 words of memory.\nThe core on this 8K word memory board occupied a centrally located \"board-on-a-board\", 5.25\" wide by 6.125\" high, and was covered by a protective plate. It was surrounded by the necessary support driver read-write-rewrite circuitry. All of the core and the corresponding support electronics fit onto a single standard 15 x board. Up to 32K of such core RAM could be supported in one external expansion box. Semiconductor ROM was already available at the time, and RAM-less systems (i.e. with ROM only) became popular in many industrial settings. The original Nova machines ran at approximately 200 kHz, but its SuperNova was designed to run at up to 3\u00a0MHz when used with special semiconductor main memory.\nThe standardized backplane and I/O signals created a simple, efficient I/O design that made interfacing programmed I/O and Data Channel devices to the Nova simple compared to competing machines. In addition to its dedicated I/O bus structure, the Nova backplane had wire wrap pins that could be used for non-standard connectors or other special purposes.\nProgramming model.\nThe instruction format could be broadly categorized into one of three functions: 1) register-to-register manipulation, 2) memory reference, and 3) input/output. Each instruction was contained in one word. The register-to-register manipulation was almost RISC-like in its bit-efficiency; and an instruction that manipulated register data could also perform tests, shifts and even elect to discard the result. Hardware options included an integer multiply and divide unit, a floating-point unit (single and double precision), and memory management.\nThe earliest Nova came with a BASIC interpreter on punched tape. As the product grew, Data General developed many languages for the Nova computers, running under a range of consistent operating systems. FORTRAN IV, ALGOL, Extended BASIC, Data General Business Basic, Interactive COBOL, and several assemblers were available from Data General. Third-party vendors and the user community expanded the offerings with Forth, Lisp, BCPL, C, ALGOL, and other proprietary versions of COBOL and BASIC.\nInstruction set.\nThe machine instructions implemented below are the common set implemented by all of the Nova series processors. Specific models often implemented additional instructions, and some instructions were provided by optional hardware.\nArithmetic instructions.\nAll arithmetic instructions operated between accumulators. For operations requiring two operands, one was taken from the source accumulator, and one from the destination accumulator, and the result was deposited in the destination accumulator. For single-operand operations, the operand was taken from the source register and the result replaced the destination register. For all single-operand opcodes, it was permissible for the source and destination accumulators to be the same, and the operation functioned as expected.\nAll arithmetic instructions included a \"no-load\" bit which, when set, suppressed the transfer of the result to the destination register; this was used in conjunction with the test options to perform a test without losing the existing contents of the destination register. In assembly language, adding a '#' to the opcode set the no-load bit.\nThe CPU contained a single-bit register called the carry bit, which after an arithmetic operation would contain the carry out of the most significant bit. The carry bit could be set to a desired value prior to performing the operation using a two-bit field in the instruction. The bit could be set, cleared, or complemented prior to performing the instruction. In assembly language, these options were specified by adding a letter to the opcode: 'O' \u2014 set the carry bit; 'Z' \u2014 clear the carry bit, 'C' \u2014 complement the carry bit, nothing \u2014 leave the carry bit alone. If the no-load bit was also specified, the specified carry value would be used for the computation, but the actual carry register would remain unaltered.\nAll arithmetic instructions included a two-bit field which could be used to specify a shift option, which would be applied to the result before it was loaded into the destination register. A single-bit left or right shift could be specified, or the two bytes of the result could be swapped. Shifts were 17-bit circular, with the carry bit \"to the left\" of the most significant bit. In other words, when a left shift was performed, the most significant bit of the result was shifted into the carry bit, and the previous contents of the carry bit were shifted into the least significant bit of the result. Byte swaps did not affect the carry bit. In assembly language, these options were specified by adding a letter to the opcode: 'L' \u2014 shift left; 'R' \u2014 shift right, 'S' \u2014 swap bytes; nothing \u2014 do not perform a shift or swap.\nAll arithmetic instructions included a three-bit field that could specify a test which was to be applied to the result of the operation. If the test evaluated to true, the next instruction in line was skipped. In assembly language, the test option was specified as a third operand to the instruction. The available tests were:\nThe actual arithmetic instructions were:\nAn example arithmetic instructions, with all options utilized, is:\nThis decoded as: clear the carry bit; add the contents of AC2 (accumulator 2) to AC0; circularly shift the result one bit to the right; test the result to see if the carry bit is set and skip the next instruction if so. Discard the result after performing the test. In effect, this adds two numbers and tests to see if the result is odd or even.\nMemory reference instructions.\nThe Nova instruction set contained a pair of instructions that transferred memory contents to accumulators and vice versa, two transfer-of-control instructions, and two instructions that tested the contents of a memory location. All memory reference instructions contained an eight-bit address field, and a two-bit field that specified the mode of memory addressing. The four modes were:\nObviously, mode 0 was only capable of addressing the first 256 memory words, given the eight-bit address field. This portion of memory was referred to as \"page zero\". Page zero memory words were considered precious to Nova assembly language programmers because of the small number available; only page zero locations could be addressed from anywhere in the program without resorting to indexed addressing, which required tying up accumulator 2 or 3 to use as an index register. In assembly language, a \".ZREL\" directive caused the assembler to place the instructions and data words that followed it in page zero; an \".NREL\" directive placed the following instructions and data words in \"normal\" memory. Later Nova models added instructions with extended addressing fields, which overcame this difficulty (at a performance penalty).\nThe assembler computed relative offsets for mode 1 automatically, although it was also possible to write it explicitly in the source. If a memory reference instruction referenced a memory address in .NREL space but no mode specifier, mode 1 was assumed and the assembler calculated the offset between the current instruction and the referenced location, and placed this in the instruction's address field (provided that the resulting value fit into the 8-bit field).\nThe two load and store instructions were:\nBoth of these instructions included an \"indirect\" bit. If this bit was set (done in assembly language by adding a '@' to the opcode), the contents of the target address were assumed to be a memory address itself, and that address would be referenced to do the load or store.\nThe two transfer-of-control instructions were:\nAs in the case of the load and store instructions, the jump instructions contained an indirect bit, which likewise was specified in assembly using the '@' character. In the case of an indirect jump, the processor retrieved the contents of the target location, and used the value as the memory address to jump to. However, unlike the load and store instructions, if the indirect address had the most significant bit set, it would perform a further cycle of indirection. On the Nova series processors prior to the Nova 3, there was no limit on the number of indirection cycles; an indirect address that referenced itself would result in an infinite indirect addressing loop, with the instruction never completing. (This could be alarming to users, since when in this condition, pressing the STOP switch on the front panel did nothing. It was necessary to reset the machine to break the loop.)\nThe two memory test instructions were:\nAs in the case of the load and store instructions, there was an indirect bit that would perform a single level of indirect addressing. These instructions were odd in that, on the Novas with magnetic core memory, the instruction was executed within the memory board itself. As was common at the time, the memory boards contained a \"write-back\" circuit to solve the destructive-read problem inherent to magnetic core memory. But the write-back mechanism also contained a mini arithmetic unit, which the processor used for several purposes. For the ISZ and DSZ instructions, the increment or decrement occurred between the memory location being read and the write-back; the CPU simply waited to be told if the result was zero or nonzero. These instructions were useful because they allowed a memory location to be used as a loop counter without tying up an accumulator, but they were slower than performing the equivalent arithmetic instructions.\nSome examples of memory reference instructions:\nTransfers the contents of the memory location labeled COUNT into accumulator 1. Assuming that COUNT is in .NREL space, this instruction is equivalent to: LDA 1,1,(COUNT-(.+1))\nwhere '.' represents the location of the LDA instruction.\nJump indirect to the memory address specified by the contents of location 17, in page zero space, and deposit the return address in accumulator 3. This was the standard method for making an RDOS system call on early Nova models; the assembly language mnemonic \".SYSTM\" translated to this.\nJump to the memory location whose address is contained in accumulator 3. This was a common means of returning from a function or subroutine call, since the JSR instruction left the return address in accumulator 3.\nStore the contents of accumulator 0 in the location that is one less than the address contained in accumulator 3.\nDecrement the value in the location labeled COUNT, and skip the next instruction if the result is zero. As in the case above, if COUNT is assumed to be in .NREL space, this is equivalent to: DSZ 1,(COUNT-(.+1))\nI/O Instructions.\nThe Novas implemented a channelized model for interfacing to I/O devices. In the model, each I/O device was expected to implement two flags, referred to as \"Busy\" and \"Done\", and three data and control registers, referred to as A, B, and C. I/O instructions were available to read and write the registers, and to send one of three signals to the device, referred to as \"start\", \"clear\", and \"pulse\". In general, sending a start signal initiated an I/O operation that had been set up by loading values into the A/B/C registers. The clear signal halted an I/O operation and cleared any resulting interrupt. The pulse signal was used to initiate ancillary operations on complex subsystems, such as seek operations on disk drives. Polled devices usually moved data directly between the device and the A register. DMA devices generally used the A register to specify the memory address, the B register to specify the number of words to be transferred, and the C register for control flags. Channel 63 referred to the CPU itself and was used for various special functions.\nEach I/O instruction contained a six-bit channel number field, a four-bit to specify which register to read or write, and a two-bit field to specify which signal was to be sent. In assembly language, the signal was specified by adding a letter to the opcode: 'S' for start, 'C' for clear, 'P' for pulse, and nothing for no signal. The opcodes were:\nIn addition, four instructions were available to test the status of a device:\nStarting a device caused it to set its busy flag. When the requested operation was completed, conventionally the device cleared its busy flag and set its done flag; most devices had their interrupt request mechanism wired to the done flag, so setting the done flag caused an interrupt (if interrupts were enabled and the device wasn't masked).\nSpecial Instructions.\nThese instructions performed various CPU control and status functions. All of them were actually shorthand mnemonics for I/O instructions on channel 63, the CPU's self-referential I/O channel.\nInterrupts and interrupt handling.\nFrom the hardware standpoint, the interrupt mechanism was relatively simple, but also less flexible, than current CPU architectures. The backplane supported a single interrupt request line, which all devices capable of interrupting connected to. When a device needed to request an interrupt, it raised this line. The CPU took the interrupt as soon as it completed the current instruction. As stated above, a device was expected to raise its \"done\" I/O flag when it requested an interrupt, and the convention was that the device would clear its interrupt request when the CPU executed a I/O clear instruction on the device's channel number.\nThe CPU expected the operating system to place the address of its interrupt service routine into memory address 1. When a device interrupted, the CPU did an indirect jump through address 1, placing the return address into memory address 0, and disabling further interrupts. The interrupt handler would then perform an INTA instruction to discover the channel number of the interrupting device. This worked by raising an \"acknowledge\" signal on the backplane. The acknowledge signal was wired in a daisy-chain format across the backplane, such that it looped through each board on the bus. Any device requesting an interrupt was expected to block the further propagation of the acknowledge signal down the bus, so that if two or more devices had pending interrupts simultaneously, only the first one would see the acknowledge signal. That device then responded by placing its channel number on the data lines on the bus. This meant that, in the case of simultaneous interrupt requests, the device that had priority was determined by which one was physically closest to the CPU in the card cage.\nThe operating system's interrupt service routine then typically performed an indexed jump using the received channel number, to jump to the specific interrupt handling routine for the device. There were a few devices, notably the CPU's power-failure detection circuit, which did not respond to the INTA instruction. If the INTA returned a result of zero, the interrupt service routine had to poll all of the non-INTA-responding devices using the SKPDZ/SKPDN instructions to see which one interrupted.\nAfter the interrupt had been processed and the service routine had sent the device an I/O clear, it resumed normal processing by enabling interrupts and then returning via an indirect jump through memory address 0. In order to prevent a pending interrupt from interrupting immediately before the return jump (which would cause the return address to be overwritten), the INTEN instruction had a one-instruction-cycle delay. When it was executed, interrupts would not be enabled until after the following instruction, which was expected to be the JMP@ 0 instruction, was executed.\nThe operating system could somewhat manage the ordering of interrupts by setting an interrupt mask using the MSKO instruction. This was intended to allow the operating system to determine which devices were permitted to interrupt at a given time. When this instruction was issued, a 16-bit interrupt mask was transmitted to all devices on the backplane. It was up to the device to decide what the mask actually meant to it; by convention, a device that was masked out was not supposed to raise the interrupt line, but the CPU had no means of enforcing this. Most devices that were maskable allowed the mask bit to be selected via a jumper on the board. There were devices that ignored the mask altogether.\nOn the systems having magnetic core memory (which retained its contents without power), recovery from a power failure was possible. A power failure detection circuit in the CPU issued an interrupt when loss of the main power coming into the computer was detected; from this point, the CPU had a short amount of time until a capacitor in the power supply lost its charge and the power to the CPU failed. This was enough time to stop I/O in progress, by issuing an IORST instruction, and then save the contents of the four accumulators and the carry bit to memory. When the power returned, if the CPU's front panel key switch was in the LOCK position, the CPU would start and perform an indirect jump through memory address 2. This was expected to be the address of an operating system service routine that would reload the accumulators and carry bit, and then resume normal processing. It was up to the service routine to figure out how to restart I/O operations that were aborted by the power failure.\nFront panel layout.\nAs was the convention of the day, most Nova models provided a front panel console to control and monitor CPU functions. Models prior to the Nova 3 all relied on a canonical front panel layout, as shown in the Nova 840 panel photo to the right. The layout contained a keyed power switch, two rows of address and data display lamps, a row of data entry switches, and a row of function switches that activated various CPU functions when pressed. The address lamps always displayed the current value of the program counter, in binary. The data lamps displayed various values depending on which CPU function was active at the moment. To the left of the leftmost data lamp, an additional lamp displayed the current value of the carry bit. On most models the lamps were incandescent lamps which were soldered to the panel board; replacing burned-out lamps was a bane of existence for Data General field service engineers.\nEach of the data switches controlled the value of one bit in a 16-bit value, and per Data General convention, they were numbered 0-15 from left to right. The data switches provided input to the CPU for various functions, and could also be read by a running program using the READS assembly language instruction. To reduce panel clutter and save money, the function switches were implemented as two-way momentary switches. When a function switch lever was lifted, it triggered the function whose name was printed above the switch on the panel; when the lever was pressed down, it activated the function whose name appeared below the switch. The switch lever returned to a neutral position when released.\nReferencing the Nova 840 photo, the first four switches from the left performed the EXAMINE and DEPOSIT functions for the four accumulators. Pressing EXAMINE on one of these caused the current value of the accumulator to be displayed in binary by the data lamps. Pressing DEPOSIT transferred the binary value represented by the current settings of the data switches to the accumulator.\nGoing to the right, the next switch was the RESET/STOP switch. Pressing STOP caused the CPU to halt after completing the current instruction. Pressing RESET caused the CPU to halt immediately, cleared a number of CPU internal registers, and sent an I/O reset signal to all connected devices. The switch to the right of that was the START/CONTINUE switch. Pressing CONTINUE caused the CPU to resume executing at the instruction currently pointed at by the program counter. Pressing START transferred the value currently set in data switches 1\u201315 to the program counter, and then began executing from there.\nThe next two switches provided read and write access to memory from the front panel. Pressing EXAMINE transferred the value set in data switches 1\u201315 to the program counter, fetched the value in the corresponding memory location, and displayed its value in the data lamps. Pressing EXAMINE NEXT incremented the program counter and then performed an examine operation on that memory location, allowing the user to step through a series of memory locations. Pressing DEPOSIT wrote the value contained in the data switches to the memory location pointed at by the program counter. Pressing DEPOSIT NEXT first incremented the program counter and then deposited to the pointed-to memory location.\nThe INST STEP function caused the CPU to execute one instruction, at the current program counter location, and then halt. Since the program counter would be incremented as part of the instruction execution, this allowed the user to single-step through a program. MEMORY STEP, a misnomer, caused the CPU to run through a single clock cycle and halt. This was of little use to users and was generally only used by field service personnel for diagnostics.\nPROGRAM LOAD was the mechanism usually used to boot a Nova. When this switch was triggered, it caused the 32-word boot ROM to be mapped over the first 32 words of memory, set the program counter to 0, and started the CPU. The boot ROM contained code that would read 256 words (512 bytes) of code from a selected I/O device into memory and then transfer control to the read-in code. The data switches 8-15 were used to tell the boot ROM which I/O channel to boot from. If switch 0 was off, the boot ROM would assume the device was a polled device (e.g., the paper tape reader) and run a polled input loop until 512 bytes had been read. If switch 0 was on, the boot ROM assumed the device was a DMA-capable device and it initiated a DMA data transfer. The boot ROM was not smart enough to position the device prior to initiating the transfer. This was a problem when rebooting after a crash; if the boot device was a disk drive, its heads had likely been left on a random cylinder. They had to be repositioned to cylinder 0, where RDOS wrote the first-level boot block, in order for the boot sequence to work. Conventionally this was done by cycling the drive through its load sequence, but users who got frustrated with the wait time (up to 5 minutes depending on the drive model) learned how to input from the front panel a drive \"recalibrate\" I/O code and single-step the CPU through it, an operation that took an experienced user only a few seconds.\nThe power switch was a 3-way keyed switch with positions marked OFF, ON, and LOCK. In the OFF position all power was removed from the CPU. Turning the key to ON applied power to the CPU. However, unlike current CPUs, the CPU did not start automatically when power was applied; the user had to use PROGRAM LOAD or some other method to start the CPU and initiate the boot sequence. Turning the switch to LOCK disabled the front panel function switches; by turning the switch to LOCK and removing the key, the user could render the CPU resistant to tampering. On systems with magnetic core memory, the LOCK position also enabled the auto power failure recovery function. The key could be removed in the OFF or LOCK positions.\nPerformance.\nThe Nova 1200 executed core memory access instructions (LDA and STA) in 2.55 microseconds (\u03bcs). Use of read-only memory saved 0.4 \u03bcs. Accumulator instructions (ADD, SUB, COM, NEG, etc.) took 1.55 \u03bcs, MUL 2.55 \u03bcs, DIV 3.75 \u03bcs, ISZ 3.15-4.5 \u03bcs. On the later Eclipse MV/6000, LDA and STA took 0.44 \u03bcs, ADD, etc. took 0.33 \u03bcs, MUL 2.2 \u03bcs, DIV 3.19 \u03bcs, ISZ 1.32 \u03bcs, FAD 5.17 \u03bcs, FMMD 11.66 \u03bcs.\nAssembly language examples.\nHello world program.\nThis is a minimal programming example in Nova assembly language. It is designed to run under RDOS and prints the string \u201cHello, world.\u201d on the console.\n ; a \"hello, world\" program for Nova running RDOS\n ; uses PCHAR system call\n .titl hello\n .nrel\n .ent start\n start:\n dochar:\n lda 0,@pmsg ; load ac0 with next character,\n mov# 0,0,snr ; test ac0; skip if nonzero (don't load result)\n jmp done\n .systm\n .pchar ; print first\n jmp er ; skipped if OK\n movs 0,0 ; swap bytes\n .systm\n .pchar ; print second\n jmp er ; skipped if OK\n isz pmsg ; point to next character\n jmp dochar ; go around again\n done:\n .systm ; normal exit\n .rtn\n er:\n .systm ; error exit\n .ertn\n halt\n pmsg:\n .+1 ; pointer to first character of string\n ; note bytes are packed right-to-left by default\n ; &lt;15&gt;&lt;12&gt; denotes a CR LF pair.\n .txt /Hello, world.&lt;15&gt;&lt;12&gt;/\n 0 ; flag word to end string\n .end start\n16-bit multiplication.\nBasic models of the Nova came without built-in hardware multiply and divide capability, to keep prices competitive. The following routine multiplies two 16-bit words to produce a 16-bit word result (overflow is ignored). It demonstrates combined use of ALU op, shift, and test (skip). Note that when this routine is called by codice_1, AC3 holds the return address. This is used by the return instruction codice_2. An idiomatic way to clear an accumulator is codice_3. Other single instructions can be arranged to load a specific set of useful constants (e.g. -2, -1, or +1).\n mpy: ; multiply AC0 &lt;- AC1 * AC2, by Toby Thain\n sub 0,0 ; clear result\n mbit: movzr 1,1,szc ; shift multiplier, test lsb\n add 2,0 ; 1: add multiplicand\n movzl 2,2,szr ; shift and test for zero\n jmp mbit ; not zero, do another bit\n jmp 0,3 ; return\nBinary print accumulator.\nThe following routine prints the value of AC1 as a 16-digit binary number, on the RDOS console. It reveals further quirks of the Nova instruction set. For instance, there is no instruction to load an arbitrary \"immediate\" value into an accumulator (although memory reference instructions do encode such a value to form an effective address). Accumulators must generally be loaded from initialized memory locations (e.g. codice_4). Other contemporary machines such as the PDP-11, and practically all modern architectures, allow for immediate loads, although many such as ARM restrict the range of values that can be loaded immediately.\nBecause the RDOS codice_5 call macro implements a codice_1, AC3 is overwritten by the return address for the codice_7 function. Therefore, a temporary location is needed to preserve the return address of the caller of this function. For a recursive or otherwise re-entrant routine, a stack, hardware if available, software if not, must be used instead. The return instruction becomes codice_8 which exploits the Nova's indirect addressing mode to load the return PC.\nThe constant definitions at the end show two assembler features: the assembler radix is octal by default (codice_9 = sixteen), and character constants could be encoded as e.g. codice_10.\n pbin: ; print AC1 on console as 16 binary digits, by Toby Thain\n sta 3,retrn ; save return addr\n lda 2,n16 ; set up bit counter\n loop: lda 0,chr0 ; load ASCII '0'\n movzl 1,1,szc ; get next bit in carry\n inc 0,0 ; bump to '1'\n .systm\n .pchar ; AC0-2 preserved\n jmp err ; if error\n inc 2,2,szr ; bump counter\n jmp loop ; loop again if not zero\n lda 0,spc ; output a space\n .systm\n .pchar\n jmp err ; if error\n jmp @retrn\n spc: \" ;that's a space\n chr0: \"0\n n16: -20\n retrn: 0\nApplications.\nThe Canadian Broadcasting Corporation in Montreal used the Nova 1200 for channel play-out automation up until the late 1980s. It was then replaced with refurbished Nova 4 units and these were in use until the mid-1990s."}
{"id": "8659", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=8659", "title": "Protestant Church in the Netherlands", "text": "The Protestant Church in the Netherlands (, abbreviated PKN) is the largest Protestant denomination in the Netherlands, being both Calvinist and Lutheran.\nIt was founded on 1 May 2004 as the merger of the vast majority of the Dutch Reformed Church, the vast majority of the Reformed Churches in the Netherlands, and the Evangelical Lutheran Church in the Kingdom of the Netherlands. The merger was the culmination of an organizational process started in 1961. Several orthodox Reformed and liberal churches did not merge into the new church.\nThe Protestant Church in the Netherlands (PKN) forms the country's second largest Christian denomination after the Catholic Church, with approximately 1.4 million members as per the church official statistics or some 7.9% of the population in 2023. It is the traditional faith of the Dutch Royal Family \u2013 a remnant of historical dominance of the Dutch Reformed Church, the main predecessor of the Protestant Church.\nDoctrine and practice.\nThe doctrine of the Protestant Church in the Netherlands is expressed in its creeds. In addition to holding the Apostles', the Nicene, and the Athanasian creeds of the universal Church, it also holds to the confessions of its predecessor bodies. From the Lutheran tradition are the unaltered Augsburg Confession and Luther's Catechism, and from the Calvinist tradition are the Heidelberg and Genevan Catechisms along with the Belgic Confession with the Canons of Dordt. The Church also acknowledges the Theological Declaration of Barmen and the Leuenberg Agreement. Ordination of women and blessings of same-sex marriages are allowed.\nThe PKN contains both liberal and conservative movements, although the liberal Remonstrants left talks when they could not agree with the unaltered adoption of the Canons of Dordt. Local congregations have far-reaching powers concerning \"controversial\" matters (such as admittance to holy communion or whether women are admitted as members of the congregation's consistory).\nOrganization.\nThe polity of the Protestant Church in the Netherlands is a hybrid of presbyterian and congregationalist church governance. Church governance is organised along local, regional, and national lines. At the local level is the congregation. An individual congregation is led by a church council made of the minister along with elders and deacons elected by the congregation. At the regional level were 75 classical assemblies whose members are chosen by the church councils. As of May 1, 2018, these 75 classical assemblies are reorganized into 11 larger ones. At the national level is the General Synod which directs areas of common interest, such as theological education, ministry training and ecumenical cooperation.\nThe PKN has four different types of congregations:\nLutherans are a minority (about 1 percent) of the PKN's membership. To ensure that Lutherans are represented in the Church, the Lutheran congregations have their own synod. The Lutheran Synod also has representatives in the General Synod.\nStatistical details.\nThe Protestant Church in the Netherlands issues yearly reports regarding its membership and finances.\nIts make-up by former affiliation of its congregations was as follows in 2017:\nTrend shows that since 2011 identification with former denominations has been falling in favor of simply identifying as \"Protestant\".\nSecularization.\nSecularization, or the decline in religiosity, first became noticeable after 1960 in the Protestant rural areas of Friesland and Groningen. Then, it spread to Amsterdam, Rotterdam, and the other large cities in the west. Finally, the southern Catholic areas showed religious declines. \nResearch in 2007 concluded that 42% of the members of the PKN were non-theists. Furthermore, in the PKN and several other smaller denominations of the Netherlands, one in six clergy were either agnostic or atheist. A Dutch minister of the PKN, Klaas Hendrikse once described God as \"a word for experience, or human experience\" and said that Jesus may have never existed.\nA countervailing trend is produced by a religious revival in the Dutch Bible Belt.\nSeparations.\nOnly those congregations belonging to the former Reformed Churches in the Netherlands have the legal right to secede from the PKN without losing its property and church during a transition period of 10 years. Seven congregations have so far decided to form the Continued Reformed Churches in the Netherlands. Two congregations have joined one of the other smaller Calvinist churches in the Netherlands. Some minorities within congregations that joined the PKN decided to leave the church and associated themselves individually with one of the other Reformed churches.\nSome congregations and members in the Dutch Reformed Church did not agree with the merger and have separated. They have organized themselves in the Restored Reformed Church. Estimations of their membership vary from 35,000 up to 70,000 people in about 120 local congregations. They disagree with the pluralism of the merged church which maintains, as they see it, contradicting Calvinist and Lutheran confessions. This group also considers same-sex marriages and female clergy unbiblical.\nChart of splits and mergers of the Dutch Reformed churches\nInvolvement in the Middle East.\nIn a meeting of eight Jewish and eight Protestant Dutch leaders in Israel in May 2011, a statement of cooperation was issued, indicating, for the most part, that the Protestant Church recognizes the issues involved with the Palestinian Christians and that this is sometimes at odds with support for the State of Israel, but standing up for the rights of the Palestinians does not detract from the emphasis on the safety of the State of Israel and vice versa."}
{"id": "8660", "revid": "23912905", "url": "https://en.wikipedia.org/wiki?curid=8660", "title": "Christian Church (Disciples of Christ)", "text": "The Christian Church (Disciples of Christ) is a mainline Protestant Christian denomination in the United States and Canada. The denomination started with the Restoration Movement during the Second Great Awakening, first existing during the 19th century as a loose association of churches working toward Christian unity. These slowly formed quasi-denominational structures through missionary societies, regional associations, and an international convention. In 1968, the Disciples of Christ officially adopted a denominational structure. At that time, a group of churches left in order to remain nondenominational.\nThe denomination is referred to by several versions of its full name, including \"Disciples of Christ\", \"Disciples\", \"Christian Church\", and \"DOC\". The Christian Church was a charter participant in the formation of the World Council of Churches (WCC) and of the Federal Council of Churches (now the National Council of Churches), and it continues to be engaged in ecumenical conversations.\nThe Disciples' local churches are congregationally governed. In 2008 there were 679,563 members in 3,714 congregations in the United States and Canada. By 2015, this number had declined to a baptized membership of 497,423 in 3,267 congregations, of whom about 306,905 were active members, while approximately 177,000 attended Sunday services each week. In 2018, the denomination reported 380,248 members with 124,437 people in average worship attendance. By 2022, membership had dropped to 277,864 members, 89,894 of whom attended worship on average.\nHistory.\nThe Christian Church (Disciples of Christ) traces its roots to the Stone-Campbell Movement on the American frontier. The Movement is so named because it started as two distinct but similar movements rising from the Presbyterian Church, each without knowledge of the other, during the Second Great Awakening in the early 19th century. The first of these two groups, led by Barton W. Stone, began at Cane Ridge, Bourbon County, Kentucky. The group called themselves simply \"Christians\". The second began in western Pennsylvania and Virginia (now West Virginia), led by Thomas Campbell and his son, Alexander Campbell. Because the founders wanted to abandon all denominational labels, they used the biblical names for the followers of Jesus that they found in the Bible.\nStone.\nIn 1801, the Cane Ridge Revival in Kentucky planted the seed for a movement in Kentucky and the Ohio River Valley to disassociate from denominationalism. In 1803 Stone and others withdrew from the Kentucky Presbytery and formed the Springfield Presbytery. The defining event of the Stone wing of the movement was the publication of the \"Last Will and Testament of the Springfield Presbytery\", at Cane Ridge, Kentucky, in 1804. \"The Last Will\" is a brief document in which Stone and five others announced their withdrawal from Presbyterianism and their intention to be solely part of the body of Christ. The writers appealed for the unity of all who follow Jesus, suggested the value of congregational self-governance, and lifted the Bible as the source for understanding the will of God. They denounced the use of the Westminster Confession of Faith as divisive.\nSoon, they adopted the name \"Christian\" to identify their group. Thus, the remnants of the Springfield Presbytery became the Christian Church. It is estimated that the Christian Church numbered about 12,000 by 1830.\nCampbells.\nIndependently of Stone, Thomas Campbell published the \"Declaration and Address of the Christian Association of Washington,\" (Pennsylvania) in 1809. In \"The Declaration and Address,\" he set forth some of his convictions about the church of Jesus Christ, emphasizing Christian unity and the restoration of the New Testament church. He organized the Christian Association of Washington, not as a church but as an association of persons seeking to grow in faith. On May 4, 1811, however, the Christian Association constituted itself as a congregationally governed church. With the building it then constructed at Brush Run, it became known as Brush Run Church.\nWhen their study of the New Testament led the reformers to begin to practice baptism by immersion, the nearby Redstone Baptist Association invited Brush Run Church to join with them for the purpose of fellowship. The reformers agreed provided that they would be \"allowed to preach and to teach whatever they learned from the Scriptures.\"\nThus began a sojourn for the reformers among the Baptists within the Redstone Baptist Association (1815\u20131824). While the reformers and the Baptists shared the same beliefs in baptism by immersion and congregational polity, it was soon clear that the reformers were not traditional Baptists. Within the Redstone Association, the differences became intolerable to some of the Baptist leaders, when Alexander Campbell began publishing a journal, \"The Christian Baptist,\" promoting reform. Campbell anticipated the conflict and moved his membership to a congregation of the Mahoning Baptist Association in 1824.\nIn 1827, the Mahoning Association appointed reformer Walter Scott as an Evangelist. Through Scott's efforts, the Mahoning Association grew rapidly. In 1828, Thomas Campbell visited several of the congregations formed by Scott and heard him preach. The elder Campbell realized that Scott was bringing an important new dimension to the movement with his approach to evangelism.\nSeveral Baptist associations began disassociating from congregations that refused to subscribe to the Philadelphia Confession. The Mahoning Association came under attack. In 1830, the Mahoning Baptist Association disbanded. Alexander ceased publication of \"The Christian Baptist\". In January 1831, he began publication of the \"Millennial Harbinger\".\n1832 Merger.\nThe two groups united at High Street Meeting House, Lexington, Kentucky, with a handshake between Barton W. Stone and \"Raccoon\" John Smith, on Saturday, December 31, 1831. Smith had been chosen by those present to speak on behalf of the followers of the Campbells. While contemporaneous accounts are clear that the handshake took place on Saturday, some historians have changed the date of the merger to Sunday, January 1, 1832. The 1832 date has become generally accepted. The actual difference is about 20 hours.\nTwo representatives of those assembled were appointed to carry the news of the union to all the churches: John Rogers for the Christians and \"Raccoon\" John Smith for the reformers. Despite some challenges, the merger succeeded.\nWith the merger, there was the challenge of what to call the new movement. Clearly, finding a Biblical, non-sectarian name was important. Stone wanted to continue to use the name \"Christians.\" Alexander Campbell insisted upon \"Disciples of Christ\". Walter Scott and Thomas Campbell sided with Stone, but the younger Campbell had strong reasons and would not yield. As a result, both names were used.\nNational Conventions.\nIn 1849, the first National Convention was held at Cincinnati, Ohio. Alexander Campbell had concerns that holding conventions would lead the movement into divisive denominationalism. He did not attend the gathering. Among its actions, the convention elected Alexander Campbell its President and created the American Christian Missionary Society (ACMS).\nThe formation of a missionary society set the stage for further \"co-operative\" efforts. By the end of the century, the Foreign Christian Missionary Society and the Christian Women's Board of Missions were also engaged in missionary activities. Forming the ACMS did not reflect a consensus of the entire movement. Sponsorship of missionary activities became a divisive issue. In the succeeding decades, for some congregations and their leaders, co-operative work through missionary societies and the adoption of instrumental music in church worship was straying too far from their conception of the early church. After the American Civil War, the schism grew. While there was no disagreement over the need for evangelism, many believed that missionary societies were not authorized by scripture and would compromise the autonomy of local congregations. This became one important factor leading to the separation of the Churches of Christ from the Christian Church (Disciples of Christ).\nJournals.\nFrom the beginning of the movement, the free exchange of ideas among the people was fostered by the journals published by its leaders. Alexander Campbell published the \"Christian Baptist\" and the \"Millennial Harbinger\". Barton W. Stone published the \"Christian Messenger\". In a respectful way, both men routinely published the contributions of others whose positions were radically different from their own.\nFollowing Campbell's death in 1866, journals continued to keep the discussion and conversation alive. Between 1870 and 1900, two journals emerged as the most prominent. The \"Christian Standard\" was edited and published by Isaac Errett of Cincinnati. The \"Christian Evangelist\" was edited and published by J. H. Garrison from St. Louis. The two men enjoyed a friendly rivalry, and kept the dialog going within the movement. A third journal became part of the conversation with the publication in 1884 of \"The Christian Oracle\", later to become \"The Christian Century\", with an interdenominational appeal. In 1914, Garrison's Christian Publishing company was purchased by R. A. Long, who then established a non-profit corporation, \"The Christian Board of Publication\" as the Brotherhood publishing house.\nFirst Division.\nIn 1906, the U.S. Religious Census listed Churches of Christ for the first time as a group that was separate and distinct from the Disciples of Christ. However, the division had been growing for years, with published reports as early as 1883. \nThe most obvious distinction between the two groups was the Churches of Christ rejecting the use of musical instruments in worship. The controversy over musical instruments began in 1860, when some congregations introduced organs, traditionally associated with wealthier, denominational churches. More basic were the underlying approaches to Biblical interpretation. The Churches of Christ permitted only those practices found in accounts of New Testament worship. They could find no New Testament documentation of the use of instrumental music in worship. The Disciples, by contrast, considered permissible any practices that the New Testament did not expressly forbid. While music and the approach to missionary work were the most visible issues, there were also some deeper ones. The process that led to the separation had begun prior to the American Civil War.\nThe Brotherhood.\nIn the early 20th century, a central point of conflict for the remaining Christian Churches was cooperative missionary efforts, both nationally and internationally. Several missionary societies had already been established, and the congregations that contributed to these societies and attended the national convention became known as \"cooperative\" and began referring to the larger grouping of these congregations as \"the Brotherhood.\" In 1917 the National Convention became the International Convention of Christian Churches (Disciples of Christ) with the incorporation of Canadian Disciples. In 1920, three separate missionary societies merged into the United Christian Missionary Society in 1920, which undertook missions work both in the \"homeland\" and abroad. Over the next fifty years, the UCMS was the largest agency of the Brotherhood. The National Benevolent Association was also established during the early 20th century as a social services ministry providing assistance to orphans, the elderly and the disabled.\nThe congregations that did not participate were known as \"independents.\" Until the cooperative churches underwent the process of restructure in the 1960s, the cooperatives and independents coexisted together under the same identity, but were following different paths by the 1940s, with the independents forming the North American Christian Convention in 1947.\nWhile issues of ecclesiology were at the forefront of the growing division, theological issues also divided the two groups, with the cooperative churches largely adopting the new methods of Biblical analysis developed in the late 19th century.\nRestructure.\nFollowing World War II, it became obvious that the organizations that had been developed in previous decades no longer effectively met the needs of the postwar era. After a number of discussions throughout the 1950s, the 1960 International Convention of Christian Churches adopted a process to \"restructure\" the entire organization. The Commission on Restructure, chaired by Granville T. Walker, held its first meeting on October 30 &amp; November 1, 1962. In 1968, the International Convention of Christian Churches (Disciples of Christ) adopted the commission's proposed \"Provisional Design of the Christian Church (Disciples of Christ)\". Soon \"The Provisional Design\" became \"The Design.\"\nThe Brotherhood's adoption of \"The Design\" made the earlier split between the cooperative and independent churches official. Under \"The Design\", all churches in the 1968 yearbook of Christian Churches (Disciples of Christ) were automatically recognized as part of the Christian Church (Disciples of Christ). In the years that followed, many of the Independent Christian Church Congregations requested formal withdrawal from the yearbook. Many of those congregations were already part of the North American Christian Convention; this group would become known as the Christian churches and churches of Christ and became the third primary group of the Stone-Campbell Movement.\nLogo.\nIn 1971, the General Assembly adopted a logo for the denomination. The logo depicts a red chalice with a white St. Andrew's Cross. Symbolically the chalice is said to represent the Lord's Supper, which is central to Disciples practice, and the cross of St. Andrew is said to represent the denomination's roots in Scottish Presbyterian and the ministry of all people. The logo was designed by Ronald E. Osborn who drew the logo with a red pen, leading to the red color of the logo, and refined by Bruce Tilsley. The logo can be used by all Disciples congregations, ministries, and other affiliated institutions and provides clarity among confusion from the \"Christian Church\" moniker many Disciple and non-Disciple congregations use.\nBeliefs and practices.\nAs a congregational denomination, each Disciples congregation determines the nature of its worship, study, Christian service, and witness to the world. Through belief in the priesthood of all believers, Disciples also practice freedom of interpretation among its members, with only baptism and confession of Christ as Lord required.\nDoctrine and interpretation.\nEarly members of the Stone-Campbell Movement adopted the slogan \"In essentials, Unity; In non-essentials, Liberty; and in all things, Charity.\" For modern disciples the one essential is the acceptance of Jesus Christ as Lord and Savior, and obedience to him in baptism. There is no requirement to give assent to any other statement of belief or creed, or any official interpretation of the Bible. Hierarchical doctrine was traditionally rejected by Disciples as human-made and divisive, and subsequently, freedom of belief and scriptural interpretation allows many Disciples to question or even deny beliefs common in doctrinal churches such as the Incarnation, the Trinity, and the Atonement. Beyond the essential commitment to follow Jesus, there is a tremendous freedom of belief and interpretation. As the basic teachings of Jesus are studied and applied to life, there is the freedom to interpret Jesus' teachings in different ways, and there is a wide diversity among Disciples in what individuals and congregations believe. It is not uncommon to find individuals who seemingly hold diametrically opposed beliefs within the same congregation affirming one another's journeys of faith as sisters and brothers in Christ.\nModern Disciples reject the use of creeds as \"tests of faith\" \u2013 as required beliefs, necessary to be accepted as a follower of Jesus. Although Disciples respect the great creeds of the church as informative affirmations of faith, they are never seen as binding. Since the adoption of The Design of the Christian Church (Disciples of Christ), in 1968, Disciples have celebrated a sense of unity in reading the preamble to the Design publicly.\nWorship and Communion.\nMost congregations sing hymns, read from the Old and New Testaments, hear the word of God proclaimed through sermon or other medium and extend an invitation to become Christ's Disciples.\nMost Disciple congregations practice weekly celebrations of the Lord's Supper, often referred to by Disciples as Communion, as an integral part of worship. Through the observance of Communion, individuals are invited to acknowledge their faults and sins, to remember the death and resurrection of Jesus Christ, to remember their baptism, and to give thanks for God's redeeming love. Because Disciples believe that the invitation to the table comes from Jesus Christ, Communion is open to all who confess that Jesus Christ is Lord, regardless of their denominational affiliation.\nBaptism.\nMost Disciple congregations practice believer's baptism in the form of immersion, believing it to be the form used in the New Testament. The experiences of yielding to Christ in being buried with him in the waters of baptism and rising to a new life have profound meaning for the church. While most congregations exclusively practice baptism by immersion, Disciples also accept other forms of baptism including infant baptism.\nEcumenical efforts.\nThe Disciples celebrate their oneness with all who seek God through Jesus Christ, throughout time and regardless of location. In local communities, congregations share with churches of other denominations in joint worship and in community Christian service. Ecumenical cooperation and collaboration with other Christian Communions has long been practiced by the Regions.\nAt the General Church level, the Christian Unity and Interfaith Ministries Unity (CUIM) coordinates the ecumenical and interfaith activities of the church. The Disciples continues to relate to the National Council of Churches and Canadian Council of Churches, both of which it was a founding member. It shares in the dialog and in the theological endeavors of the World Council of Churches. The Disciples has been a full participant in the Consultation on Church Union since it began in the 1960s. It continues to support those ongoing conversations which have taken on the title Churches Uniting in Christ.\nThe Disciples have two full communion partners: the United Church of Christ, since 1989, and the United Church of Canada, since 2019. These three denominations all share mutual full communion with each other. CUIM describes these partnerships as the proclamation of \"mutual recognition of their sacraments and ordained ministry.\" Ordained Disciple ministers are able to directly serve in the United Church of Christ without having to seek additional qualifications.\nAdditionally, the Disciples combined their overseas ministries with the United Church of Christ in 1996. Known as Global Ministries, it is a common agency of both denominations with a joint staff and is a continuance of decades of cooperative work in global missions.\nWhile the Disciples of Christ and United Church of Canada have entered full communion, the recentness of the agreement means that the provisions for mutual recognition of clergy are not yet finalized and adopted.\nOrdained ministry.\nThe Disciples believe in the priesthood of all believers, in that all people baptized are called to minister to others with diverse spiritual gifts. The Disciples view their Order of Ministry as a specific subset of all believers who are called with spiritual gifts specifically suited for pastoral ministry. Congregations use different terms to refer to persons in the Order of Ministry including Pastor and Reverend but most call them Ministers, including the denomination's governing documents.\nCongregations sponsor members seeking ordination or commissioning as a Minister, and Regional Ministries organize committees to oversee the process. Ordination can be achieved by obtaining a Master of Divinity from a theological institution, which does not have to be an institution associated with the Disciples. Ordination can also be achieved through an \"Apprentice\" track which has candidates shadow ordained ministers. Finally, Ministers can be Commissioned, a shorter process for seminary students and those seeking short-term ministry in a Region. Regional requirements for ministry vary. Ordination is made official through a service which includes members of the church, clergy, and Regional Minister laying their hands on the candidate as the ordaining act. Ecumenical representatives are often included to emphasize the Disciples' desire for Christian unity.\nDisciples recognize the ordinations of the United Church of Christ as do they for Disciples.\nA General Commission on the Order of Ministry exists to interpret and review definitions of ministry, give oversight to Regions and congregations, provide other support, and maintain the standing of Regional Ministers and Ministers of General (National) Ministries.\nAbortion.\nMembers of the Disciples of Christ have many different opinions on abortion. This would be a prime example of \"Resolve to Love, Agree to Differ, Unite to Serve, Break Bread Together\".\nLGBTQ inclusion.\nIn 1977, the General Assembly of the denomination debated resolutions about homosexuality for the first time; a resolution condemning the \"homosexual lifestyle\" was defeated by the Assembly and a resolution to ban gay people from the ordained ministry was referred to the General Minister and President for further study. At the next General Assembly two years later, the Assembly approved a resolution that declared \"The ordination of persons who engage in homosexual practices is not in accord with God's will,\" but concurrently declared that \"The Christian Church (Disciples of Christ) intends to continue the current pattern of assigning responsibility to the regions with respect to the nurture, certification, and ordination of ministers.\" Since then, some regions have ordained LGBTQ ministers before the denomination officially supported it. Concerns about LGBTQ people continued to be an issue at the General Assembly, but resolutions that called on more civil rights protections for LGBTQ people were passed with overwhelming majorities and resolutions to ban the \"homosexual lifestyle\" continued to be rejected.\nIn 2011, the Christian Church (Disciples of Christ) stated that \"Disciples do not have a formal policy on same-sex marriage. Different congregations have the autonomy to discern on issues such as this one.\" In 2013, the Disciples of Christ voted in favor of a resolution affirming all members regardless of sexual orientation. After same-sex marriage was legalized in the US, the denomination reiterated that it leaves \"all decisions of policy on same-sex marriage to local congregations\".\nIn 2019, the General Assembly passed a resolution specifically affirming that transgender and gender non-conforming people are welcome in the Christian Church (Disciples of Christ).\nDisciples LGBTQ+ Alliance provides resources to congregations that want to be certified as \"Open and Affirming\" to show that they are accepting of all gender identities and sexual orientations. The Alliance was founded as the Gay, Lesbian, and Affirming Disciples Alliance (GLAD) during the 1979 General Assembly.\nStructure.\nThe structure of the Disciples is unique among Mainline Protestant churches. \"The Design\", the governing document of the denomination, describes three \"expressions\" of the church: congregational, regional, and general. Each of these expressions are \"characterized by its integrity, self-governance, authority, rights, and responsibilities.\" In relating to each other, they work in covenant and not authority to support the ministry and work of the church.\nCongregations.\nCongregations of the Disciples are self-governing in the tradition of congregational polity. They call their own Ministers, select their own leadership, own their own property, and manage their own affairs.\nIn Disciples congregations, the priesthood of all believers finds its expression in worship and Christian service. Congregations elect and ordain lay persons as Elders to share in duties of congregational ministry with the staff ministers, including visiting the sick and administering communion to them, providing spiritual guidance for the congregation, and presiding over Communion during worship, either with or without the staff ministers.\nRegional Ministries.\nRegional churches consist of all Disciples in a given area, usually a state or group of states. As of 2023, the denomination has 31 regions, including the Christian Church (Disciples of Christ) in Canada which has a dual identity as a region in the binational church and a national denomination in its own right.\nRegions meet in a Regional Assembly every two to three years to conduct business. Each Region calls a Regional Minister to serve as its primary pastor and chief executive; most regions also have Associate Regional Ministers and other staff to serve specific aspects of its ministries. Canada calls its regional minister a \"national pastor\". Regions are analogous to the middle judicatories of other denominations, and Regional Ministers are analogous to Bishops.\nOne of the primary responsibilities of the Regions is the care for and oversight of clergy. The Design places primary responsibility for ordination and licensing of ministers with the region. Candidates seeking ordination are sponsored by a congregation but must be approved by their region, which usually entails a process of interviews and other evaluations by a committee made up of clergy and lay people. The Regional Minister usually officiates the ordination service in the sponsoring congregation. After ordination, regions continue to oversee clergy through a process known as standing, which requires ministers to undergo certain trainings periodically and maintain membership in a Disciples congregation. Ministers can lose their standing for violating the ministerial code of ethics the denomination maintains. Finally, Regional Ministers often provide pastoral care to ministers in their region.\nRegions also nurture congregations in their region, including planting new churches, providing guidance, supporting struggling congregations, and helping congregations hire their ministers. This latter process consists of a system known as Search and Call, in which ministers seeking a church declare which regions they would like to serve in and the region then suggests those candidates to congregations seeking a minister. Regional Ministers usually provides congregations with a set of candidates that they feel will meet the congregation's particular needs.\nRegions also provide fellowship and education opportunities for its members. Many regions have summer camping experiences for children and youth.\nAs with all parts of the Disciples, Regions do not have authority to control congregations and congregations are not required to use regional programming, including the search and call system.\nGeneral Ministries.\nThe Christian Church (Disciples of Christ) at the \"General Church\" level consists of a number of self-governing agencies, which focus upon specific Christian witnesses to the world. The church agencies report to the General Assembly, which meets biennially in odd-numbered years and is an assembly of representatives selected by congregations and ordained ministers with standing in the denomination. The General Minister and President (GMP) is the lead pastor for the denomination and the chief executive officer of the legal corporation. Following the covenantal understanding of the denomination, the GMP does not have direct executive power over the General Ministries, regions, or congregations. The GMP is elected to a six-year term by the General Assembly, with the option for a second term.\nThe current General Minister and President is Teresa Hord Owens. When she was elected in 2017, Owens was the first black woman to lead a mainline denomination as their chief executive. Her presidency followed the presidency of Sharon E. Watkins, the first woman to lead a mainline denomination as their chief executive.\nThe General Ministries are:\nOne highly popular and respected General Agency program is the \"Week of Compassion,\" named for the special offering to fund the program when it began in the 1950s. The Week of Compassion is the disaster relief and Third World development agency. It works closely with Church World Service and church-related organizations in countries around the world where disasters strike, providing emergency aid.\nThe General Church has challenged the entire denomination to work for a 2020 Vision for the first two decades of the 21st century. Together the denomination is well on the way to achieving its four foci:\nMembership trends.\nThe Christian Church (Disciples of Christ) has experienced a very significant loss of membership since the middle of the 20th century. Membership peaked in 1958 at just under 2 million. In 1993, membership dropped below 1 million. In 2009, the denomination reported 658,869 members in 3,691 congregations. In 2010, the five states with the highest adherence rates were Kansas, Missouri, Iowa, Kentucky and Oklahoma. The states with the largest absolute number of adherents were Missouri, Texas, Indiana, Kentucky and Ohio. In 2017, membership had declined to 450,425 members.\nAffiliated academic institutions.\nFrom the very beginnings of the movement, Disciples have founded institutions of higher learning. Alexander Campbell taught young leaders and founded Bethany College. The movement established similar schools, especially in the years following the American Civil War.\nBecause intellectual and religious freedom are important values for the Disciples of Christ, the colleges, universities, and seminaries founded by its congregations do not seek to indoctrinate students or faculty with a sectarian point of view.\nIn the 21st century, the relationship between the Christian Church (Disciples of Christ) and its affiliated universities is the purview of Higher Education and Leadership Ministries (HELM), an agency of the General Church.\nSeminaries and theological institutions.\nThe Disciples have four seminaries and divinity schools directly affiliated with the denomination. These institutions have an ecumenical student body, a reflection of the Disciples' focus on church unity. They are:\nThe Disciples have three additional institutions that provide supplementary education and community living for ecumenical theological institutions. They are:\nEcumenical relations.\nThe Disciples of Christ maintains ecumenical relations with the Pontifical Council for Promoting Christian Unity. It is also affiliated with other ecumenical organizations such as Churches Uniting in Christ, Christian Churches Together, the National Council of Churches and the World Council of Churches. It maintains Ordained Ministerial Partner Standing with the United Church of Christ, which means that clergy ordained in the Disciples of Christ may also serve in the United Church of Christ. Since 2019, it has been a full Communion partner and had an agreement for mutual recognition of ministerial credentials with the United Church of Canada. It is affiliated with the Disciples Ecumenical Consultative Council and the World Communion of Reformed Churches."}
{"id": "8662", "revid": "39911211", "url": "https://en.wikipedia.org/wiki?curid=8662", "title": "David Rice Atchison", "text": "David Rice Atchison (August 11, 1807January 26, 1886) was a mid-19th-century Democratic United States Senator from Missouri. He served as president pro tempore of the United States Senate for six years. Atchison served as a major general in the Missouri State Militia in 1838 during Missouri's Mormon War and as a Confederate brigadier general during the American Civil War under Major General Sterling Price in the Missouri Home Guard. Some of Atchison's associates claimed that for 24 hours\u2014Sunday, March 4, 1849, through noon on Monday\u2014he may have been acting president of the United States. This belief, however, is dismissed by most scholars.\nAtchison, owner of many slaves and a plantation, was a prominent pro-slavery activist and Border Ruffian leader, deeply involved with violence against abolitionists and other free-staters during the \"Bleeding Kansas\" events that preceded admission of the state to the Union.\nEarly life.\nAtchison was born to William Atchison and his wife in Frogtown (later Kirklevington), which is now part of Lexington, Kentucky. He was educated at Transylvania University in Lexington. Classmates included five future Democratic senators (Solomon Downs of Louisiana, Jesse Bright of Indiana, George Wallace Jones of Iowa, Edward Hannegan of Indiana, and Jefferson Davis of Mississippi). Atchison completed law studies and was admitted to the Kentucky bar in 1829.\nMissouri lawyer and politician.\nIn 1830, he moved to Liberty in Clay County in western Missouri, and set up practice there. He also acquired a farm or plantation, with labor provided by enslaved African Americans. Atchison's law practice flourished, and his best-known client was Joseph Smith, founder of the Latter Day Saint Movement. Atchison represented Smith in land disputes with non-Mormon settlers in Caldwell County and Daviess County.\nAlexander William Doniphan joined Atchison's law practice in Liberty in May 1833. The two became fast friends and spent many leisure time hours playing cards, going to horse races, hunting, fishing, and attending social functions and political events. Atchison, already a member of the Liberty Blues, a volunteer militia in Missouri, got Doniphan to join.\nAtchison was elected to the Missouri House of Representatives in 1834. He worked hard for the Platte Purchase, which required Native American tribes to cede land to the United States and extended the northwestern boundary of Missouri to the Missouri River in 1837.\nWhen early disputes broke out into the Mormon War of 1838, Atchison was appointed a major general in the state militia. He took part in suppressing violence by both sides.\nActive in the Democratic Party, Atchison was re-elected to the Missouri State House of Representatives in 1838. In 1841, he was appointed a circuit court judge for the six-county area of the Platte Purchase. In 1843, he was named a county commissioner in Platte County, where he then lived.\nSenate career.\nIn October 1843, Atchison was appointed to the U.S. Senate to fill the vacancy left by the death of Lewis F. Linn. He was the first senator from western Missouri to serve in this position. At age 36, he was the youngest senator from Missouri up to that time. Atchison was re-elected to a full term on his own account in 1849.\nAtchison was very popular with his fellow Senate Democrats. When the Democrats took control of the US Senate in December 1845, they chose Atchison as president pro tempore, placing him second in succession for the presidency. He also was responsible for presiding over the Senate when the vice president was absent. At 38, he was a young man with low seniority in the Senate after two years to gain such a position.\nIn 1849, Atchison stepped down as president pro tempore in favor of William R. King. King, in turn, yielded the office back to Atchison in December 1852, after being elected Vice President of the United States. Atchison continued as president pro tempore until December 1854.\nAs a senator, Atchison was a fervent advocate of slavery and territorial expansion. He supported the annexation of Texas and the U.S.-Mexican War. Atchison and Thomas Hart Benton, Missouri's other senator, became rivals and finally enemies, although both were Democrats. Benton declared himself to be against slavery in 1849. In 1851 Atchison allied with the Whigs to defeat incumbent Benton for re-election.\nBenton, intending to challenge Atchison in 1854, began to agitate for territorial organization of the area west of Missouri (now the states of Kansas and Nebraska) so that it could be opened to settlement. To counter this, Atchison proposed that the area be organized \"and\" that the section of the Missouri Compromise banning slavery there be repealed in favor of popular sovereignty. Under this plan, settlers in each territory would vote to decide whether they would allow slavery.\nAt Atchison's request, Senator Stephen Douglas of Illinois introduced the Kansas\u2013Nebraska Act, which embodied this idea, in November 1853. The act was passed and became law in May 1854, establishing the Territories of Kansas and Nebraska.\nBorder Ruffians.\nBoth Douglas and Atchison had believed that Nebraska would be settled by Free-State men from Iowa and Illinois, and Kansas by pro-slavery Missourians and other Southerners, thus preserving the numerical balance between free states and slave states in the nation. In 1854 Atchison helped found the town of Atchison, Kansas, as a pro-slavery settlement. The town (and county) were named for him.\nWhile Southerners supported the idea of settling in Kansas, few migrated there. Most free-soilers preferred Kansas to Nebraska. Furthermore, anti-slavery activists throughout the North came to view Kansas as a battleground and formed societies to encourage free-soil settlers to go to Kansas, to ensure there would be enough voters in both Kansas and Nebraska to approve their entry as free states.\nIt appeared as if the Kansas Territorial legislature to be elected in March 1855 would be controlled by free-soilers and ban slavery. Atchison and his supporters viewed this as a breach of faith. An angry Atchison called on pro-slavery Missourians to uphold slavery by force and \"to kill every God-damned abolitionist in the district\" if necessary. He recruited an immense mob of heavily armed Missourians, the infamous \"Border Ruffians\". On election day, March 30, 1855, Atchison led 5,000 Border Ruffians into Kansas. They seized control of all polling places at gunpoint, cast tens of thousands of fraudulent votes for pro-slavery candidates, and elected a pro-slavery legislature.\nThe outrage was nonetheless accepted by the Federal government. When Territorial Governor Andrew Reeder objected, President Franklin Pierce fired him.\nDespite this show of force, far more free-soilers than pro-slavery settlers migrated to Kansas. There were continual raids and ambushes by both sides in \"Bleeding Kansas\". In spite of the best efforts of Atchison and the Ruffians, Kansas rejected slavery and finally became a free state in 1861.\nCharles Sumner, in the epic \"Crimes Against Kansas\" speech on May 19, 1856, exposed Atchison's role in the invasion, tortures, and killings in Kansas. Speaking in the flamboyant style he and others used, lacing his prose with references to Roman history, Sumner compared Atchison to Roman Senator Catiline, who betrayed his country in a plot to overthrow the existing order. For two days, Sumner listed crime, after crime, in detail, complete with documentation by newspapers and letters of the time, showing the tortures and violence by Atchison and his men.\nTwo days later, Atchison gave his own speech, totally unaware that he had been exposed on the Senate floor in such a fashion. Atchison's speech was to the Texas men he had just met, hired, and paid for, Atchison reveals in his speech, by \"authorities in Washington\". They are about to invade Lawrence, Kansas. Atchison makes the men promise to kill and \"draw blood,\" and boasts of his flag, which was red in color for \"Southern Rights\" and the color of blood. They would press \"to blood\" the spread of slavery into Kansas. He revealed in this speech that the immediate goal of the invasion was to stop the newspaper in Lawrence from publishing anti-slavery material. Atchison's men had made it a crime to publish anti-slavery newspapers in Kansas.\nAtchison made it clear the men were to kill and draw blood, told the men they would be \"well paid,\" and encouraged them to plunder from the homes that they invaded. That was after the hundreds of dozens of tortures and killings that Sumner had detailed in his Crimes Against Kansas speech. In other words, things were about to get much worse since Atchison had his hired men from Texas.\nDefeated for re-election.\nAtchison's Senate term expired on March 3, 1855. He sought election to another term, but the Democrats in the Missouri legislature were split between him and Benton, while the Whig minority put forward their own man. No senator was elected until January 1857, when James S. Green was chosen.\nRailroad proposal.\nWhen the first transcontinental railroad was proposed in the 1850s, Atchison called for it to be built along the central route (from St. Louis through Missouri, Kansas, and Utah), rather than the southern route (from New Orleans through Texas and New Mexico). Naturally, his suggested route went through Atchison.\nAmerican Civil War.\nAtchison and his law partner Doniphan fell out over politics in 1859\u20131861, disagreeing on how Missouri should proceed. Atchison favored secession, while Doniphan was torn and would remain, for the most part, non-committal. Privately, Doniphan favored the Union, but found it difficult to oppose his friends and associates.\nDuring the secession crisis in Missouri at the beginning of the American Civil War, Atchison sided with Missouri's pro-Confederate governor, Claiborne Jackson. He was appointed a major general in the Missouri State Guard. Atchison actively recruited State Guardsmen in northern Missouri and served with Guard commander General Sterling Price in the summer campaign of 1861. In September 1861, Atchison led 3,500 State Guard recruits across the Missouri River to reinforce Price and defeated Union troops that tried to block his force in the Battle of Liberty.\nAtchison served in the State Guard through the end of 1861. In March 1862, Union forces in the Trans-Mississippi theater won a decisive victory at Pea Ridge in Arkansas and secured Union control of Missouri. Atchison then resigned from the army over reported strategy arguments with Price and moved to Texas for the duration of the war. After the war, he retired to his farm near Gower. He denied many of his pro-slavery public statements made prior to the Civil War. Then, his retirement cottage outside of Plattsburg, Missouri burned to the ground before he died in 1886. This entailed the complete loss of his library containing books, documents, and letters documenting his role in the Mormon War, Indian affairs, pro-slavery activities, Civil War activities, and other legislation covering his career as a lawyer, senator, and soldier.\nPurported one-day presidency.\nInauguration Day\u2014March 4\u2014fell on a Sunday in 1849, and so president-elect Zachary Taylor did not take the presidential oath of office until the next day out of religious concerns. Even so, the term of the outgoing president, James K. Polk, ended at noon on March 4. On March 2, outgoing vice president George M. Dallas relinquished his position as president of the Senate. Congress had previously chosen Atchison as president pro tempore. In 1849, according to the Presidential Succession Act of 1792, the Senate president pro tempore immediately followed the vice president in the presidential line of succession. As Dallas's term also ended at noon on the 4th, and as neither Taylor nor vice president-elect Millard Fillmore had been sworn into office on that day, it was claimed by some of Atchison's friends and colleagues that from March 4\u20135, 1849, Atchison was acting president of the United States.\nHistorians, constitutional scholars, and biographers dismiss the claim. They point out that Atchison's Senate term had also ended on March 4. When the Senate of the new Congress convened on March 5 to allow new senators and the new vice president to take the oath of office, the secretary of the Senate called members to order, as the Senate had no president pro tempore. Although an incoming president must take the oath of office before any official acts, the prevailing view is that presidential succession does not depend on the oath. Even supposing that an oath was necessary, Atchison never took it, so he was no more the president than Taylor.\nIn September 1872, Atchison, who never himself claimed that he was technically president, told a reporter for the \"Plattsburg Lever\":\nDeath.\nAtchison died on January 26, 1886, at his home near Gower, Missouri at the age of 78. He was buried at Greenlawn Cemetery in Plattsburg, Missouri. His grave marker reads \"President of the United States for One Day.\""}
{"id": "8663", "revid": "1270680599", "url": "https://en.wikipedia.org/wiki?curid=8663", "title": "Daniel Gabriel Fahrenheit", "text": "Daniel Gabriel Fahrenheit FRS (; ; 24 May 1686 \u2013 16 September 1736) was a physicist, inventor, and scientific instrument maker, born in Poland to a family of German extraction. Fahrenheit invented thermometers accurate and consistent enough to allow the comparison of temperature measurements between different observers using different instruments. Fahrenheit is also credited with inventing mercury-in-glass thermometers more accurate and superior to spirit-filled thermometers at the time. The popularity of his thermometers led to the widespread adoption of his Fahrenheit scale attached to his instruments.\nBiography.\nEarly life.\nFahrenheit was born in Danzig (Gda\u0144sk), then in the Polish\u2013Lithuanian Commonwealth. The Fahrenheits were a German Hanse merchant family who had lived in several Hanseatic cities. Fahrenheit's great-grandfather had lived in Rostock, and research suggests that the Fahrenheit family originated in Hildesheim. Daniel's grandfather moved from Kneiphof in K\u00f6nigsberg (then in the Duchy of Prussia) to Danzig and settled there as a merchant in 1650. His son, Daniel Fahrenheit (the father of Daniel Gabriel), married Concordia Schumann, the daughter of a well-known Danzig business family. Daniel was the eldest of the five Fahrenheit children (two sons, three daughters) who survived childhood. His sister, Virginia Elisabeth Fahrenheit, married Benjamin Kr\u00fcger and was the mother of Benjamin Ephraim Kr\u00fcger, a clergyman and playwright.\nAs a young adult, Fahrenheit \"showed a particular desire for studying,\" and was scheduled to enroll in the Danzig Gymnasium. But on 14 August 1701, his parents died after eating poisonous mushrooms. Fahrenheit, along with two brothers and sisters, was placed under guardianship. In 1702, Fahrenheit's guardians enrolled him in a bookkeeping course and sent him to a four-year merchant trade apprenticeship in Amsterdam.\nUpon completing his apprenticeship, Fahrenheit ran off and began a period of travel through the Holy Roman Empire, Sweden, and Denmark in 1707. At the request of his guardians, a warrant was issued for his arrest with the intention of placing him into the service of the Dutch East India company.\nWork with thermometers, Fahrenheit scale.\nBy around 1706, Fahrenheit was manufacturing and shipping barometers and spirit-filled thermometers using the . In 1708, Fahrenheit met with the mayor of Copenhagen and astronomer, Ole R\u00f8mer, and was introduced to R\u00f8mer's temperature scale and his methods for making thermometers. R\u00f8mer told Fahrenheit that demand for accurate thermometers was high. The visit inspired Fahrenheit to try to improve his own offerings. Perhaps not coincidentally, Fahrenheit's arrest warrant was dropped around the time of his meeting with R\u00f8mer.\nIn 1709, Fahrenheit returned to Danzig and took observations using his barometers and thermometers, traveled more in 1710 and returned to Danzig in 1711 to settle his parents' estate. After additional travel to K\u00f6nigsberg and Mitau in 1711, he returned to Danzig in 1712 and stayed there for two years. During this period he worked on solving technical problems with his thermometers.\nFahrenheit began experimenting with mercury thermometers in 1713. Also by this time, Fahrenheit was using a modified version of R\u00f8mer's scale for his thermometers which would later evolve into his own Fahrenheit scale. In 1714, Fahrenheit left Danzig for Berlin and Dresden to work closely with the glass-blowers there. In that year Christian Wolff wrote about Fahrenheit's thermometers in a journal after receiving a pair of his alcohol-based devices, helping to boost Fahrenheit's reputation in the scientific community.\nIn addition to his interest in meteorological instruments, Fahrenheit also worked on his ideas for a mercury clock, a perpetual motion machine, and a heliostat around 1715. He struck up a correspondence with Leibniz about some of these projects. From the exchange of letters, we learn that Fahrenheit was running out of money while working on his projects and asked Leibniz for help obtaining a paid post so he could continue his work.\nIn 1717 or 1718, Fahrenheit returned to Amsterdam and began selling barometers, areometers, and his mercury and alcohol-based thermometers commercially. By 1721, Fahrenheit had perfected the process of crafting and standardizing his thermometers. The superiority of his mercury thermometers over alcohol-based thermometers made them very popular, leading to the widespread adoption of his Fahrenheit scale, the measurement system he developed and used for his thermometers.\nLater life and controversy.\nFahrenheit spent the remainder of his life in Amsterdam. From 1718 onward, he lectured in chemistry in Amsterdam. He visited England in 1724 and was elected into the Fellow of the Royal Society on May 5. In that year, he published five papers in Latin for the Royal Society's scientific journal, \"Philosophical Transactions\", on various topics. In his second paper, \"Experimenta et observationes de congelatione aqu\u00e6 in vacuo fact\u00e6\", he provides a description of his thermometers and the reference points he used for calibrating them. For two centuries, this document was the only description of Fahrenheit's process for making thermometers. In the 20th century, Ernst Cohen uncovered correspondences between Fahrenheit and Herman Boerhaave which cast considerable doubt on the veracity of Fahrenheit's article explaining the reference points for his scale and that, in fact, Fahrenheit's scale was largely derived from R\u00f8mer's scale. In his book, \"The History of the Thermometer and Its Use in Meteorology\", W. E. Knowles Middleton writes,\nFrom August 1736 to his death, Fahrenheit stayed in the house of Johannes Frisleven at Plein Square in The Hague in connection with an application for a patent at the States of Holland and West Friesland. At the beginning of September, he became ill and on the 7th his health had deteriorated to such an extent that he had notary Willem Ruijsbroek come to draw up his will. On the 11th, the notary came by again to make some changes. Five days after that, Fahrenheit died at the age of fifty. Four days later, he received the fourth-class funeral of one who is classified as destitute, in the Kloosterkerk in The Hague (the Cloister or Monastery Church).\nFahrenheit scale.\nAccording to Fahrenheit's 1724 article, he determined his scale by reference to three fixed points of temperature. The lowest temperature was achieved by preparing a frigorific mixture of ice, water, and a salt (\"ammonium chloride or even sea salt\"), and waiting for the eutectic system to reach equilibrium temperature. The thermometer then was placed into the mixture and the liquid in the thermometer allowed to descend to its lowest point. The thermometer's reading there was taken as . The second reference point was selected as the reading of the thermometer when it was placed in still water when ice was just forming on the surface. This was assigned as . The third calibration point, taken as , was selected as the thermometer's reading when the instrument was placed under the arm or in the mouth.\nFahrenheit came up with the idea that mercury boils around 300 degrees on this temperature scale. Work by others showed that water boils about 180 degrees above its freezing point. The Fahrenheit scale later was redefined to make the freezing-to-boiling interval exactly 180 degrees, a convenient value as 180 is a highly composite number, meaning that it is evenly divisible into many fractions. It is because of the scale's redefinition that normal mean body temperature today is taken as 98.6 degrees, whereas it was 96 degrees on Fahrenheit's original scale.\nThe Fahrenheit scale was the primary temperature standard for climatic, industrial and medical purposes in English-speaking countries until the 1970s, presently mostly replaced by the Celsius scale long used in the rest of the world, apart from the United States, where temperatures and weather reports are still broadcast in Fahrenheit."}
{"id": "8664", "revid": "25082147", "url": "https://en.wikipedia.org/wiki?curid=8664", "title": "Freescale DragonBall", "text": "Motorola/Freescale Semiconductor's DragonBall, or MC68328, is a microcontroller design based on the famous 68000 core, but implemented as an all-in-one low-power system for handheld computer use. It is supported by \u03bcClinux. It was designed by Motorola in Hong Kong and released in 1995.\nThe DragonBall's major design win was in numerous devices running the Palm OS platform. However, from Palm OS 5 onwards their use was superseded by ARM-based processors from Texas Instruments and Intel. \nThe processor is capable of speeds of up to 16.58\u00a0MHz and can run up to 2.7 MIPS (million instructions per second), for the base 68328 and DragonBall EZ (MC68EZ328) model. It was extended to 33\u00a0MHz, 5.4 MIPS for the DragonBall VZ (MC68VZ328) model, and 66\u00a0MHz, 10.8 MIPS for the DragonBall Super VZ (MC68SZ328).\nIt is a 32-bit processor with 32-bit internal and external address bus (24-bit external address bus for EZ and VZ variants) and 32-bit data bus (8/16-bit external data bus). It has many built-in functions, like a color and grayscale display controller, PC speaker sound, serial port with UART and IRDA support, UART bootstrap, real time clock, is able to directly access DRAM, Flash ROM, mask ROM, and has built-in support for touch screens.\nThe more recent DragonBall MX series microcontrollers, later renamed the Freescale i.MX (MC9328MX/MCIMX) series, are intended for similar application to the earlier DragonBall devices but are based on an ARM processor core instead of a 68000 core."}
{"id": "8665", "revid": "48734", "url": "https://en.wikipedia.org/wiki?curid=8665", "title": "D.W. Griffith", "text": ""}
{"id": "8666", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=8666", "title": "Decnet", "text": ""}
{"id": "8667", "revid": "42335939", "url": "https://en.wikipedia.org/wiki?curid=8667", "title": "Double-slit experiment", "text": "In modern physics, the double-slit experiment demonstrates that light and matter can exhibit behavior of both classical particles and classical waves. This type of experiment was first performed by Thomas Young in 1801, as a demonstration of the wave behavior of visible light. In 1927, Davisson and Germer and, independently, George Paget Thomson and his research student Alexander Reid demonstrated that electrons show the same behavior, which was later extended to atoms and molecules. Thomas Young's experiment with light was part of classical physics long before the development of quantum mechanics and the concept of wave\u2013particle duality. He believed it demonstrated that the Christiaan Huygens' wave theory of light was correct, and his experiment is sometimes referred to as Young's experiment or Young's slits.\nThe experiment belongs to a general class of \"double path\" experiments, in which a wave is split into two separate waves (the wave is typically made of many photons and better referred to as a wave front, not to be confused with the wave properties of the individual photon) that later combine into a single wave. Changes in the path-lengths of both waves result in a phase shift, creating an interference pattern. Another version is the Mach\u2013Zehnder interferometer, which splits the beam with a beam splitter.\nIn the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate. The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen \u2013 a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles (not waves); the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave). However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through. These results demonstrate the principle of wave\u2013particle duality.\nOther atomic-scale entities, such as electrons, are found to exhibit the same behavior when fired towards a double slit. Additionally, the detection of individual discrete impacts is observed to be inherently probabilistic, which is inexplicable using classical mechanics.\nThe experiment can be done with entities much larger than electrons and photons, although it becomes more difficult as size increases. The largest entities for which the double-slit experiment has been performed were molecules that each comprised 2000 atoms (whose total mass was 25,000 atomic mass units).\nThe double-slit experiment (and its variations) has become a classic for its clarity in expressing the central puzzles of quantum mechanics. Richard Feynman called it \"a phenomenon which is impossible [\u2026] to explain in any classical way, and which has in it the heart of quantum mechanics. In reality, it contains the only mystery [of quantum mechanics].\"\nOverview.\nIf light consisted strictly of ordinary or classical particles, and these particles were fired in a straight line through a slit and allowed to strike a screen on the other side, we would expect to see a pattern corresponding to the size and shape of the slit. However, when this \"single-slit experiment\" is actually performed, the pattern on the screen is a diffraction pattern in which the light is spread out. The smaller the slit, the greater the angle of spread. The top portion of the image shows the central portion of the pattern formed when a red laser illuminates a slit and, if one looks carefully, two faint side bands. More bands can be seen with a more highly refined apparatus. Diffraction explains the pattern as being the result of the interference of light waves from the slit.\nIf one illuminates two parallel slits, the light from the two slits again interferes. Here the interference is a more pronounced pattern with a series of alternating light and dark bands. The width of the bands is a property of the frequency of the illuminating light. (See the bottom photograph to the right.)\nWhen Thomas Young (1773\u20131829) first demonstrated this phenomenon, it indicated that light consists of waves, as the distribution of brightness can be explained by the alternately additive and subtractive interference of wavefronts. Young's experiment, performed in the early 1800s, played a crucial role in the understanding of the wave theory of light, vanquishing the corpuscular theory of light proposed by Isaac Newton, which had been the accepted model of light propagation in the 17th and 18th centuries.\nHowever, the later discovery of the photoelectric effect demonstrated that under different circumstances, light can behave as if it is composed of discrete particles. These seemingly contradictory discoveries made it necessary to go beyond classical physics and take into account the quantum nature of light.\nFeynman was fond of saying that all of quantum mechanics can be gleaned from carefully thinking through the implications of this single experiment. He also proposed (as a thought experiment) that if detectors were placed before each slit, the interference pattern would disappear.\nThe Englert\u2013Greenberger duality relation provides a detailed treatment of the mathematics of double-slit interference in the context of quantum mechanics.\nA low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, by reducing the level of incident light until photon emission/absorption events were mostly non-overlapping.\nA slit interference experiment was not performed with anything other than light until 1961, when Claus J\u00f6nsson of the University of T\u00fcbingen performed it with coherent electron beams and multiple slits. In 1974, the Italian physicists Pier Giorgio Merli, Gian Franco Missiroli, and Giulio Pozzi performed a related experiment using single electrons from a coherent source and a biprism beam splitter, showing the statistical nature of the buildup of the interference pattern, as predicted by quantum theory. In 2002, the single-electron version of the experiment was voted \"the most beautiful experiment\" by readers of \"Physics World.\" Since that time a number of related experiments have been published, with a little controversy.\nIn 2012, Stefano Frabboni and co-workers sent single electrons onto nanofabricated slits (about 100\u00a0nm wide) and, by detecting the transmitted electrons with a single-electron detector, they could show the build-up of a double-slit interference pattern. Many related experiments involving the coherent interference have been performed; they are the basis of modern electron diffraction, microscopy and high resolution imaging.\nIn 2018, single particle interference was demonstrated for antimatter in the Positron Laboratory (L-NESS, Politecnico di Milano) of Rafael Ferragut in Como (Italy), by a group led by Marco Giammarchi.\nVariations of the experiment.\nInterference from individual particles.\nAn important version of this experiment involves single particle detection. Illuminating the double-slit with a low intensity results in single particles being detected as white dots on the screen. Remarkably, however, an interference pattern emerges when these particles are allowed to build up one by one (see the image below).\nThis demonstrates the wave\u2013particle duality, which states that all matter exhibits both wave and particle properties: The particle is measured as a single pulse at a single position, while the modulus squared of the wave describes the probability of detecting the particle at a specific place on the screen giving a statistical interference pattern. This phenomenon has been shown to occur with photons, electrons, atoms, and even some molecules: with buckminsterfullerene () in 2001, with 2 molecules of 430 atoms ( and ) in 2011, and with molecules of up to 2000 atoms in 2019.\nIn addition to interference patterns built up from single particles, up to 4 entangled photons can also show interference patterns.\nMach-Zehnder interferometer.\nThe Mach\u2013Zehnder interferometer can be seen as a simplified version of the double-slit experiment. Instead of propagating through free space after the two slits, and hitting any position in an extended screen, in the interferometer the photons can only propagate via two paths, and hit two discrete photodetectors. This makes it possible to describe it via simple linear algebra in dimension 2, rather than differential equations.\nA photon emitted by the laser hits the first beam splitter and is then in a superposition between the two possible paths. In the second beam splitter these paths interfere, causing the photon to hit the photodetector on the right with probability one, and the photodetector on the bottom with probability zero. Blocking one of the paths, or equivalently detecting the presence of a photon on a path eliminates interference between the paths: both photodetectors will be hit with probability 1/2. This indicates that after the first beam splitter the photon does not take one path or another, but rather exists in a quantum superposition of the two paths.\n\"Which-way\" experiments and the principle of complementarity.\nA well-known thought experiment predicts that if particle detectors are positioned at the slits, showing through which slit a photon goes, the interference pattern will disappear. This which-way experiment illustrates the complementarity principle that photons can behave as either particles or waves, but cannot be observed as both at the same time.\nDespite the importance of this thought experiment in the history of quantum mechanics (for example, see the discussion on ), technically feasible realizations of this experiment were not proposed until the 1970s. (Naive implementations of the textbook thought experiment are not possible because photons cannot be detected without absorbing the photon.) Currently, multiple experiments have been performed illustrating various aspects of complementarity.\nAn experiment performed in 1987 produced results that demonstrated that partial information could be obtained regarding which path a particle had taken without destroying the interference altogether. This \"wave-particle trade-off\" takes the form of an inequality relating the visibility of the interference pattern and the distinguishability of the which-way paths.\nDelayed choice and quantum eraser variations.\nWheeler's delayed-choice experiments demonstrate that extracting \"which path\" information after a particle passes through the slits can seem to retroactively alter its previous behavior at the slits.\nQuantum eraser experiments demonstrate that wave behavior can be restored by erasing or otherwise making permanently unavailable the \"which path\" information.\nA simple do-it-at-home illustration of the quantum eraser phenomenon was given in an article in \"Scientific American\". If one sets polarizers before each slit with their axes orthogonal to each other, the interference pattern will be eliminated. The polarizers can be considered as introducing which-path information to each beam. Introducing a third polarizer in front of the detector with an axis of 45\u00b0 relative to the other polarizers \"erases\" this information, allowing the interference pattern to reappear. This can also be accounted for by considering the light to be a classical wave, and also when using circular polarizers and single photons. Implementations of the polarizers using entangled photon pairs have no classical explanation.\nWeak measurement.\nIn a highly publicized experiment in 2012, researchers claimed to have identified the path each particle had taken without any adverse effects at all on the interference pattern generated by the particles. In order to do this, they used a setup such that particles coming to the screen were not from a point-like source, but from a source with two intensity maxima. However, commentators such as Svensson have pointed out that there is in fact no conflict between the weak measurements performed in this variant of the double-slit experiment and the Heisenberg uncertainty principle. Weak measurement followed by post-selection did not allow simultaneous position and momentum measurements for each individual particle, but rather allowed measurement of the average trajectory of the particles that arrived at different positions. In other words, the experimenters were creating a statistical map of the full trajectory landscape.\nOther variations.\nIn 1967, Pfleegor and Mandel demonstrated two-source interference using two separate lasers as light sources.\nIt was shown experimentally in 1972 that in a double-slit system where only one slit was open at any time, interference was nonetheless observed provided the path difference was such that the detected photon could have come from either slit. The experimental conditions were such that the photon density in the system was much less than 1.\nIn 1991, Carnal and Mlynek performed the classic Young's double slit experiment with metastable helium atoms passing through micrometer-scale slits in gold foil.\nIn 1999, a quantum interference experiment (using a diffraction grating, rather than two slits) was successfully performed with buckyball molecules (each of which comprises 60 carbon atoms). A buckyball is large enough (diameter about 0.7\u00a0nm, nearly half a million times larger than a proton) to be seen in an electron microscope.\nIn 2002, an electron field emission source was used to demonstrate the double-slit experiment. In this experiment, a coherent electron wave was emitted from two closely located emission sites on the needle apex, which acted as double slits, splitting the wave into two coherent electron waves in a vacuum. The interference pattern between the two electron waves could then be observed. In 2017, researchers performed the double-slit experiment using light-induced field electron emitters. With this technique, emission sites can be optically selected on a scale of ten nanometers. By selectively deactivating (closing) one of the two emissions (slits), researchers were able to show that the interference pattern disappeared.\nIn 2005, E. R. Eliel presented an experimental and theoretical study of the optical transmission of a thin metal screen perforated by two subwavelength slits, separated by many optical wavelengths. The total intensity of the far-field double-slit pattern is shown to be reduced or enhanced as a function of the wavelength of the incident light beam.\nIn 2012, researchers at the University of Nebraska\u2013Lincoln performed the double-slit experiment with electrons as described by Richard Feynman, using new instruments that allowed control of the transmission of the two slits and the monitoring of single-electron detection events. Electrons were fired by an electron gun and passed through one or two slits of 62\u00a0nm wide \u00d7 4\u00a0\u03bcm tall.\nIn 2013, a quantum interference experiment (using diffraction gratings, rather than two slits) was successfully performed with molecules that each comprised 810 atoms (whose total mass was over 10,000 atomic mass units). The record was raised to 2000 atoms (25,000 amu) in 2019.\nHydrodynamic pilot wave analogs.\nHydrodynamic analogs have been developed that can recreate various aspects of quantum mechanical systems, including single-particle interference through a double-slit. A silicone oil droplet, bouncing along the surface of a liquid, self-propels via resonant interactions with its own wave field. The droplet gently sloshes the liquid with every bounce. At the same time, ripples from past bounces affect its course. The droplet's interaction with its own ripples, which form what is known as a pilot wave, causes it to exhibit behaviors previously thought to be peculiar to elementary particles \u2013 including behaviors customarily taken as evidence that elementary particles are spread through space like waves, without any specific location, until they are measured.\nBehaviors mimicked via this hydrodynamic pilot-wave system include quantum single particle diffraction, tunneling, quantized orbits, orbital level splitting, spin, and multimodal statistics. It is also possible to infer uncertainty relations and exclusion principles. Videos are available illustrating various features of this system. (See the External links.)\nHowever, more complicated systems that involve two or more particles in superposition are not amenable to such a simple, classically intuitive explanation. Accordingly, no hydrodynamic analog of entanglement has been developed. Nevertheless, optical analogs are possible.\nDouble-slit experiment on time.\nIn 2023, an experiment was reported recreating an interference pattern in time by shining a pump laser pulse at a screen coated in indium tin oxide (ITO) which would alter the properties of the electrons within the material due to the Kerr effect, changing it from transparent to reflective for around 200 femtoseconds long where a subsequent probe laser beam hitting the ITO screen would then see this temporary change in optical properties as a slit in time and two of them as a double slit with a phase difference adding up destructively or constructively on each frequency component resulting in an interference pattern. Similar results have been obtained classically on water waves.\nClassical wave-optics formulation.\nMuch of the behaviour of light can be modelled using classical wave theory. The Huygens\u2013Fresnel principle is one such model; it states that each point on a wavefront generates a secondary wavelet, and that the disturbance at any subsequent point can be found by summing the contributions of the individual wavelets at that point. This summation needs to take into account the phase as well as the amplitude of the individual wavelets. Only the intensity of a light field can be measured\u2014this is proportional to the square of the amplitude.\nIn the double-slit experiment, the two slits are illuminated by the quasi-monochromatic light of a single laser. If the width of the slits is small enough (much less than the wavelength of the laser light), the slits diffract the light into cylindrical waves. These two cylindrical wavefronts are superimposed, and the amplitude, and therefore the intensity, at any point in the combined wavefronts depends on both the magnitude and the phase of the two wavefronts. The difference in phase between the two waves is determined by the difference in the distance travelled by the two waves.\nIf the viewing distance is large compared with the separation of the slits (the far field), the phase difference can be found using the geometry shown in the figure below right. The path difference between two waves travelling at an angle is given by:\nWhere d is the distance between the two slits. When the two waves are in phase, i.e. the path difference is equal to an integral number of wavelengths, the summed amplitude, and therefore the summed intensity is maximum, and when they are in anti-phase, i.e. the path difference is equal to half a wavelength, one and a half wavelengths, etc., then the two waves cancel and the summed intensity is zero. This effect is known as interference. The interference fringe maxima occur at angles\nwhere \u03bb is the wavelength of the light. The angular spacing of the fringes, , is given by\nThe spacing of the fringes at a distance from the slits is given by\nFor example, if two slits are separated by 0.5\u00a0mm (), and are illuminated with a 0.6 \u03bcm wavelength laser (), then at a distance of 1 m (), the spacing of the fringes will be 1.2\u00a0mm.\nIf the width of the slits is appreciable compared to the wavelength, the Fraunhofer diffraction equation is needed to determine the intensity of the diffracted light as follows:\nwhere the sinc function is defined as sinc(\"x\") = sin(\"x\")/\"x\" for \"x\" \u2260 0, and sinc(0) = 1.\nThis is illustrated in the figure above, where the first pattern is the diffraction pattern of a single slit, given by the function in this equation, and the second figure shows the combined intensity of the light diffracted from the two slits, where the function represents the fine structure, and the coarser structure represents diffraction by the individual slits as described by the function.\nSimilar calculations for the near field can be made by applying the Fresnel diffraction equation, which implies that as the plane of observation gets closer to the plane in which the slits are located, the diffraction patterns associated with each slit decrease in size, so that the area in which interference occurs is reduced, and may vanish altogether when there is no overlap in the two diffracted patterns.\nPath-integral formulation.\nThe double-slit experiment can illustrate the path integral formulation of quantum mechanics provided by Feynman. The path integral formulation replaces the classical notion of a single, unique trajectory for a system, with a sum over all possible trajectories. The trajectories are added together by using functional integration.\nEach path is considered equally likely, and thus contributes the same amount. However, the phase of this contribution at any given point along the path is determined by the action along the path:\nformula_6\nAll these contributions are then added together, and the magnitude of the final result is squared, to get the probability distribution for the position of a particle:\nformula_7\nAs is always the case when calculating probability, the results must then be normalized by imposing:\nformula_8\nThe probability distribution of the outcome is the normalized square of the norm of the superposition, over all paths from the point of origin to the final point, of waves propagating proportionally to the action along each path. The differences in the cumulative action along the different paths (and thus the relative phases of the contributions) produces the interference pattern observed by the double-slit experiment. Feynman stressed that his formulation is merely a mathematical description, not an attempt to describe a real process that we can measure.\nInterpretations of the experiment.\nLike the Schr\u00f6dinger's cat thought experiment, the double-slit experiment is often used to highlight the differences and similarities between the various interpretations of quantum mechanics.\nStandard quantum physics.\nThe standard interpretation of the double slit experiment is that the pattern is a wave phenomenon, representing interference between two probability amplitudes, one for each slit. Low intensity experiments demonstrate that the pattern is filled in one particle detection at a time. Any change to the apparatus designed to detect a particle at a particular slit alters the probability amplitudes and the interference disappears. This interpretation is independent of any conscious observer.\nComplementarity.\nNiels Bohr interpreted quantum experiments like the double-slit experiment using the concept of complementarity. In Bohr's view quantum systems are not classical, but measurements can only give classical results. Certain pairs of classical properties will never be observed in a quantum system simultaneously: the interference pattern of waves in the double slit experiment will disappear if particles are detected at the slits. Modern quantitative versions of the concept allow for a continuous tradeoff between the visibility of the interference fringes and the probability of particle detection at a slit.\nCopenhagen interpretation.\nThe Copenhagen interpretation is a collection of views about the meaning of quantum mechanics, stemming from the work of Niels Bohr, Werner Heisenberg, Max Born, and others. The term \"Copenhagen interpretation\" was apparently coined by Heisenberg during the 1950s to refer to ideas developed in the 1925\u20131927 period, glossing over his disagreements with Bohr. Consequently, there is no definitive historical statement of what the interpretation entails. Features common across versions of the Copenhagen interpretation include the idea that quantum mechanics is intrinsically indeterministic, with probabilities calculated using the Born rule, and some form of complementarity principle. Moreover, the act of \"observing\" or \"measuring\" an object is irreversible, and no truth can be attributed to an object, except according to the results of its measurement. In the Copenhagen interpretation, complementarity means a particular experiment can demonstrate particle behavior (passing through a definite slit) or wave behavior (interference), but not both at the same time. In a Copenhagen-type view, the question of which slit a particle travels through has no meaning when there is no detector.\nRelational interpretation.\nAccording to the relational interpretation of quantum mechanics, first proposed by Carlo Rovelli, observations such as those in the double-slit experiment result specifically from the interaction between the observer (measuring device) and the object being observed (physically interacted with), not any absolute property possessed by the object. In the case of an electron, if it is initially \"observed\" at a particular slit, then the observer\u2013particle (photon\u2013electron) interaction includes information about the electron's position. This partially constrains the particle's eventual location at the screen. If it is \"observed\" (measured with a photon) not at a particular slit but rather at the screen, then there is no \"which path\" information as part of the interaction, so the electron's \"observed\" position on the screen is determined strictly by its probability function. This makes the resulting pattern on the screen the same as if each individual electron had passed through both slits.\nMany-worlds interpretation.\nAs with Copenhagen, there are multiple variants of the many-worlds interpretation. The unifying theme is that physical reality is identified with a wavefunction, and this wavefunction always evolves unitarily, i.e., following the Schr\u00f6dinger equation with no collapses. Consequently, there are many parallel universes, which only interact with each other through interference. David Deutsch argues that the way to understand the double-slit experiment is that in each universe the particle travels through a specific slit, but its motion is affected by interference with particles in other universes, and this interference creates the observable fringes. David Wallace, another advocate of the many-worlds interpretation, writes that in the familiar setup of the double-slit experiment the two paths are not sufficiently separated for a description in terms of parallel universes to make sense.\nDe Broglie\u2013Bohm theory.\nAn alternative to the standard understanding of quantum mechanics, the De Broglie\u2013Bohm theory states that particles also have precise locations at all times, and that their velocities are defined by the wave-function. So while a single particle will travel through one particular slit in the double-slit experiment, the so-called \"pilot wave\" that influences it will travel through both. The two slit de Broglie-Bohm trajectories were first calculated by Chris Dewdney while working with Chris Philippidis and Basil Hiley at Birkbeck College (London). The de Broglie-Bohm theory produces the same statistical results as standard quantum mechanics, but dispenses with many of its conceptual difficulties by adding complexity through an \"ad hoc\" quantum potential to guide the particles.\nWhile the model is in many ways similar to Schr\u00f6dinger equation, it is known to fail for relativistic cases and does not account for features such as particle creation or annihilation in quantum field theory. Many authors such as nobel laureates Werner Heisenberg, Sir Anthony James Leggett and Sir Roger Penrose have criticized it for not adding anything new.\nMore complex variants of this type of approach have appeared, for instance the \"three wave hypothesis\" of Ryszard Horodecki as well as other complicated combinations of de Broglie and Compton waves. To date there is no evidence that these are useful."}
{"id": "8668", "revid": "6969503", "url": "https://en.wikipedia.org/wiki?curid=8668", "title": "Dan Bricklin", "text": "Daniel Singer Bricklin (born July 16, 1951) is an American businessman and engineer who is the co-creator, with Bob Frankston, of VisiCalc, the first spreadsheet program. He also founded Software Garden, Inc., of which he is currently president, and Trellix, which he left in 2004. He currently serves as the chief technology officer of Alpha Software.\nHis book, \"Bricklin on Technology\", was published by Wiley in May 2009. For his work with VisiCalc, Bricklin is often referred to as \"the father of the Spreadsheet\". He was one of six people spotlighted when the Computer was denoted \"Machine of the Year\" by \"Time\" magazine in 1982.\nEarly life and education.\nBricklin was born in Philadelphia, where he attended Akiba Hebrew Academy. He began his college as a mathematics major, but soon switched to computer science. He earned a Bachelor of Science in electrical engineering and computer science from the Massachusetts Institute of Technology in 1973, where he was a resident of Bexley Hall.\nUpon graduating from MIT, Bricklin worked for Digital Equipment Corporation (DEC) where he was part of the team that worked on WPS-8 until 1976, when he began working for FasFax, a cash register manufacturer. In 1977, he returned to education, and was awarded a Master of Business Administration from Harvard University in 1979.\nWhile a student at Harvard Business School, Bricklin co-developed VisiCalc in 1979, making it the first electronic spreadsheet readily available for home and office use. It ran on an Apple II computer, and was considered a fourth generation software program. VisiCalc is widely credited for fueling the rapid growth of the personal computer industry. Instead of doing financial projections with manually calculated spreadsheets, and having to recalculate with every single cell in the sheet, VisiCalc allowed the user to change any cell, and have the entire sheet automatically recalculated. This could turn 20 hours of work into 15 minutes and allowed for more creativity.\nCareer.\nSoftware Arts.\nIn 1979, Bricklin and Frankston founded Software Arts, Inc., and began selling VisiCalc, via a separate company named VisiCorp. Along with Frankston, Bricklin started writing versions of the program for the Tandy TRS-80, Commodore PET and the Atari 800. Soon after its launch, VisiCalc became a fast seller at $100.\nSoftware Arts also published TK/Solver and Spotlight, a desktop organizer for the IBM Personal Computer.\"\nBricklin was awarded the Grace Murray Hopper Award in 1981 for VisiCalc. Bricklin could not patent VisiCalc, since software inventions were not eligible for patent protection at the time.\nBricklin was chairman of Software Arts until 1985, the year that Software Arts was acquired by Lotus. He left and founded Software Garden.\nSoftware Garden.\nDan Bricklin founded Software Garden, a small consulting firm and developer of software applications, in 1985. The company's focus was to produce and market \u201cDan Bricklin's Demo Program\u201d. The program allowed users to create demonstrations of their programs before they were even written, and was also used to create tutorials for Windows-based programs. Other versions released soon after included demo-it! He remained the president of the company until he co-founded Slate Corporation in 1990. In 1992, he became the vice president of Phoenix-based Slate corporation, and developed \"At Hand\", a pen-based spreadsheet. When Slate closed in 1994, Bricklin returned to Software Garden.\nHis \"Dan Bricklin's Overall Viewer\" (described by \"The New York Times\" as \"a visual way to display information in Windows-based software\") was released in November 1994.\nCorporation.\nIn 1995 Bricklin founded Trellix Corporation, named for \"Trellix Site Builder\".\nTrellix was bought by Interland (now Web.com) in 2003, and Bricklin became Interland's chief technology officer until early 2004.\nCurrent work.\nBricklin continues to serve as president of Software Garden, a small company that develops and markets software tools he creates, as well as providing speaking and consulting services.\nHe has released Note Taker HD, an application that integrates handwritten notes on the Apple iPad tablet.\nHe is also developing wikiCalc, a collaborative, basic spreadsheet running on the Web.\nHe is currently the chief technology officer of Alpha Software in Burlington, Massachusetts, a company that creates tools to easily develop cross-platform mobile business applications.\nAffiliations.\nIn 1994, Bricklin was inducted as a Fellow of the Association for Computing Machinery. He is a founding trustee of the Massachusetts Technology Leadership Council and has served on the boards of the Software Publishers Association and the Boston Computer Society.\nHe was elected a member of the National Academy of Engineering in 2003 for the invention and creation of the electronic spreadsheet.\nAwards.\nIn 1981, Bricklin was given a Grace Murray Hopper Award for VisiCalc.\nIn 1996, Bricklin was awarded by the IEEE Computer Society with the Computer Entrepreneur Award for pioneering the development and commercialization of the spreadsheet and the profound changes it fostered in business and industry.\nIn 2003, Bricklin was given the Wharton Infosys Business Transformation Award for being a technology change leader. He was recognized for having used information technology in an industry-transforming way. He has received an Honorary Doctor of Humane Letters from Newbury College. He also became a member of the National Academy of Engineering.\nIn 2004, he was made a Fellow of the Computer History Museum \"for advancing the utility of personal computers by developing the VisiCalc electronic spreadsheet.\"\nBricklin:"}
{"id": "8669", "revid": "19431", "url": "https://en.wikipedia.org/wiki?curid=8669", "title": "Dragon 32", "text": ""}
{"id": "8670", "revid": "740137", "url": "https://en.wikipedia.org/wiki?curid=8670", "title": "Document Editor", "text": ""}
{"id": "8674", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=8674", "title": "DECT", "text": "Digital Enhanced Cordless Telecommunications (DECT) is a cordless telephony standard maintained by ETSI. It originated in Europe, where it is the common standard, replacing earlier standards, such as CT1 and CT2. Since the DECT-2020 standard onwards, it also includes IoT communication.\nBeyond Europe, it has been adopted by Australia and most countries in Asia and South America. North American adoption was delayed by United States radio-frequency regulations. This forced development of a variation of DECT called DECT\u00a06.0, using a slightly different frequency range, which makes these units incompatible with systems intended for use in other areas, even from the same manufacturer. DECT has almost completely replaced other standards in most countries where it is used, with the exception of North America.\nDECT was originally intended for fast roaming between networked base stations, and the first DECT product was Net3 wireless LAN. However, its most popular application is single-cell cordless phones connected to traditional analog telephone, primarily in home and small-office systems, though gateways with multi-cell DECT and/or DECT repeaters are also available in many private branch exchange (PBX) systems for medium and large businesses, produced by Panasonic, Mitel, Gigaset, Ascom, Cisco, Grandstream, Snom, Spectralink, and RTX. DECT can also be used for purposes other than cordless phones, such as baby monitors, wireless microphones and industrial sensors. The ULE Alliance's DECT ULE and its \"HAN FUN\" protocol are variants tailored for home security, automation, and the internet of things (IoT).\nThe DECT standard includes the generic access profile (GAP), a common interoperability profile for simple telephone capabilities, which most manufacturers implement. GAP-conformance enables DECT handsets and bases from different manufacturers to interoperate at the most basic level of functionality, that of making and receiving calls. Japan uses its own DECT variant, J-DECT, which is supported by the DECT forum.\nThe New Generation DECT (NG-DECT) standard, marketed as CAT-iq by the DECT Forum, provides a common set of advanced capabilities for handsets and base stations. CAT-iq allows interchangeability across IP-DECT base stations and handsets from different manufacturers, while maintaining backward compatibility with GAP equipment. It also requires mandatory support for wideband audio.\nDECT-2020 New Radio, marketed as NR+ (New Radio plus), is a 5G data transmission protocol which meets ITU-R IMT-2020 requirements for ultra-reliable low-latency and massive machine-type communications, and can co-exist with earlier DECT devices.\nStandards history.\nThe DECT standard was developed by ETSI in several phases, the first of which took place between 1988 and 1992 when the first round of standards were published. These were the ETS 300-175 series in nine parts defining the air interface, and ETS 300-176 defining how the units should be type approved. A technical report, ETR-178, was also published to explain the standard. Subsequent standards were developed and published by ETSI to cover interoperability profiles and standards for testing.\nNamed Digital European Cordless Telephone at its launch by CEPT in November 1987; its name was soon changed to Digital European Cordless Telecommunications, following a suggestion by Enrico Tosato of Italy, to reflect its broader range of application including data services. In 1995, due to its more global usage, the name was changed from European to Enhanced. DECT is recognized by the ITU as fulfilling the IMT-2000 requirements and thus qualifies as a 3G system. Within the IMT-2000 group of technologies, DECT is referred to as IMT-2000 Frequency Time (IMT-FT).\nDECT was developed by ETSI but has since been adopted by many countries all over the World. The original DECT frequency band (1880\u20131900\u00a0MHz) is used in all countries in Europe. Outside Europe, it is used in most of Asia, Australia and South America. In the United States, the Federal Communications Commission in 2005 changed channelization and licensing costs in a nearby band (1920\u20131930\u00a0MHz, or 1.9\u00a0GHz), known as Unlicensed Personal Communications Services (UPCS), allowing DECT devices to be sold in the U.S. with only minimal changes. These channels are reserved exclusively for voice communication applications and therefore are less likely to experience interference from other wireless devices such as baby monitors and wireless networks.\nThe New Generation DECT (NG-DECT) standard was first published in 2007; it was developed by ETSI with guidance from the Home Gateway Initiative through the DECT Forum to support IP-DECT functions in home gateway/IP-PBX equipment. The ETSI TS 102 527 series comes in five parts and covers wideband audio and mandatory interoperability features between handsets and base stations. They were preceded by an explanatory technical report, ETSI TR 102 570. The DECT Forum maintains the CAT-iq trademark and certification program; CAT-iq wideband voice profile 1.0 and interoperability profiles 2.0/2.1 are based on the relevant parts of ETSI TS 102 527.\nThe DECT Ultra Low Energy (DECT ULE) standard was announced in January 2011 and the first commercial products were launched later that year by Dialog Semiconductor. The standard was created to enable home automation, security, healthcare and energy monitoring applications that are battery powered. Like DECT, DECT ULE standard uses the 1.9\u00a0GHz band, and so suffers less interference than Zigbee, Bluetooth, or Wi-Fi from microwave ovens, which all operate in the unlicensed 2.4\u00a0GHz ISM band. DECT ULE uses a simple star network topology, so many devices in the home are connected to a single control unit.\nA new low-complexity audio codec, LC3plus, has been added as an option to the 2019 revision of the DECT standard. This codec is designed for high-quality voice and music applications such as wireless speakers, headphones, headsets, and microphones. LC3plus supports scalable 16-bit narrowband, wideband, super wideband, fullband, and 24-bit high-resolution fullband and ultra-band coding, with sample rates of 8, 16, 24, 32, 48 and 96\u00a0kHz and audio bandwidth of up to 48\u00a0kHz.\nDECT-2020 New Radio protocol was published in July 2020; it defines a new physical interface based on cyclic prefix orthogonal frequency-division multiplexing (CP-OFDM) capable of up to 1.2Gbit/s transfer rate with QAM-1024 modulation. The updated standard supports multi-antenna MIMO and beamforming, FEC channel coding, and hybrid automatic repeat request. There are 17 radio channel frequencies in the range from 450MHz up to 5,875MHz, and channel bandwidths of 1,728, 3,456, or 6,912kHz. Direct communication between end devices is possible with a mesh network topology. In October 2021, DECT-2020 NR was approved for the IMT-2020 standard, for use in Massive Machine Type Communications (MMTC) industry automation, Ultra-Reliable Low-Latency Communications (URLLC), and professional wireless audio applications with point-to-point or multicast communications; the proposal was fast-tracked by ITU-R following real-world evaluations. The new protocol will be marketed as NR+ (New Radio plus) by the DECT Forum. OFDMA and SC-FDMA modulations were also considered by the ESTI DECT committee.\nOpenD is an open-source framework designed to provide a complete software implementation of DECT ULE protocols on reference hardware from Dialog Semiconductor and DSP Group; the project is maintained by the DECT forum.\nApplication.\nThe DECT standard originally envisaged three major areas of application:\nOf these, the domestic application (cordless home telephones) has been extremely successful. The enterprise PABX market, albeit much smaller than the cordless home market, has been very successful as well, and all the major PABX vendors have advanced DECT access options available. The public access application did not succeed, since public cellular networks rapidly out-competed DECT by coupling their ubiquitous coverage with large increases in capacity and continuously falling costs. There has been only one major installation of DECT for public access: in early 1998 Telecom Italia launched a wide-area DECT network known as \"Fido\" after much regulatory delay, covering major cities in Italy. The service was promoted for only a few months and, having peaked at 142,000 subscribers, was shut down in 2001.\nDECT has been used for wireless local loop as a substitute for copper pairs in the \"last mile\" in countries such as India and South Africa. By using directional antennas and sacrificing some traffic capacity, cell coverage could extend to over . One example is the corDECT standard.\nThe first data application for DECT was Net3 wireless LAN system by Olivetti, launched in 1993 and discontinued in 1995. A precursor to Wi-Fi, Net3 was a micro-cellular data-only network with fast roaming between base stations and 520\u00a0kbit/s transmission rates.\nData applications such as electronic cash terminals, traffic lights, and remote door openers also exist, but have been eclipsed by Wi-Fi, 3G and 4G which compete with DECT for both voice and data.\nCharacteristics.\nThe DECT standard specifies a means for a portable phone or \"Portable Part\" to access a fixed telephone network via radio. Base station or \"Fixed Part\" is used to terminate the radio link and provide access to a fixed line. A gateway is then used to connect calls to the fixed network, such as public switched telephone network (telephone jack), office PBX, ISDN, or VoIP over Ethernet connection.\nTypical abilities of a domestic DECT Generic Access Profile (GAP) system include multiple handsets to one base station and one phone line socket. This allows several cordless telephones to be placed around the house, all operating from the same telephone jack. Additional handsets have a battery charger station that does not plug into the telephone system. Handsets can in many cases be used as intercoms, communicating between each other, and sometimes as walkie-talkies, intercommunicating without telephone line connection.\nDECT operates in the 1880\u20131900\u00a0MHz band and defines ten frequency channels from 1881.792\u00a0MHz to 1897.344\u00a0MHz with a band gap of 1728\u00a0kHz.\nDECT operates as a multicarrier frequency-division multiple access (FDMA) and time-division multiple access (TDMA) system. This means that the radio spectrum is divided into physical carriers in two dimensions: frequency and time. FDMA access provides up to 10 frequency channels, and TDMA access provides 24 time slots per every frame of 10ms. DECT uses time-division duplex (TDD), which means that down- and uplink use the same frequency but different time slots. Thus a base station provides 12 duplex speech channels in each frame, with each time slot occupying any available channel thus 10\u00a0\u00d7\u00a012\u00a0=\u00a0120 carriers are available, each carrying 32\u00a0kbit/s.\nDECT also provides frequency-hopping spread spectrum over TDMA/TDD structure for ISM band applications. If frequency-hopping is avoided, each base station can provide up to 120 channels in the DECT spectrum before frequency reuse. Each timeslot can be assigned to a different channel in order to exploit advantages of frequency hopping and to avoid interference from other users in asynchronous fashion.\nDECT allows interference-free wireless operation to around outdoors. Indoor performance is reduced when interior spaces are constrained by walls.\nDECT performs with fidelity in common congested domestic radio traffic situations. It is generally immune to interference from other DECT systems, Wi-Fi networks, video senders, Bluetooth technology, baby monitors and other wireless devices.\nTechnical properties.\nETSI standards documentation ETSI EN 300 175 parts 1\u20138 (DECT), ETSI EN 300 444 (GAP) and ETSI TS 102 527 parts 1\u20135 (NG-DECT) prescribe the following technical properties:\nPhysical layer.\nThe DECT physical layer uses FDMA/TDMA access with TDD.\nGaussian frequency-shift keying (GFSK) modulation is used: the binary one is coded with a frequency increase by 288\u00a0kHz, and the binary zero with frequency decrease of 288\u00a0kHz. With high quality connections, 2-, 4- or 8-level differential PSK modulation (DBPSK, DQPSK or D8PSK), which is similar to QAM-2, QAM-4 and QAM-8, can be used to transmit 1, 2, or 3 bits per each symbol. QAM-16 and QAM-64 modulations with 4 and 6 bits per symbol can be used for user data (B-field) only, with resulting transmission speeds of up to 5,068Mbit/s.\nDECT provides dynamic channel selection and assignment; the choice of transmission frequency and time slot is always made by the mobile terminal. In case of interference in the selected frequency channel, the mobile terminal (possibly from suggestion by the base station) can initiate either intracell handover, selecting another channel/transmitter on the same base, or intercell handover, selecting a different base station altogether. For this purpose, DECT devices scan all idle channels at regular 30s intervals to generate a received signal strength indication (RSSI) list. When a new channel is required, the mobile terminal (PP) or base station (FP) selects a channel with the minimum interference from the RSSI list.\nThe maximum allowed power for portable equipment as well as base stations is 250\u00a0mW. A portable device radiates an average of about 10\u00a0mW during a call as it is only using one of 24 time slots to transmit. In Europe, the power limit was expressed as effective radiated power (ERP), rather than the more commonly used equivalent isotropically radiated power (EIRP), permitting the use of high-gain directional antennas to produce much higher EIRP and hence long ranges.\nData link control layer.\nThe DECT media access control layer controls the physical layer and provides connection oriented, connectionless and broadcast services to the higher layers.\nThe DECT data link layer uses Link Access Protocol Control (LAPC), a specially designed variant of the ISDN data link protocol called LAPD. They are based on HDLC.\nGFSK modulation uses a bit rate of 1152\u00a0kbit/s, with a frame of 10ms (11520bits) which contains 24 time slots. Each slots contains 480 bits, some of which are reserved for physical packets and the rest is guard space. Slots 0\u201311 are always used for downlink (FP to PP) and slots 12\u201323 are used for uplink (PP to FP).\nThere are several combinations of slots and corresponding types of physical packets with GFSK modulation:\nThe 420/424 bits of a GFSK basic packet (P32) contain the following fields:\nThe resulting full data rate is 32\u00a0kbit/s, available in both directions.\nNetwork layer.\nThe DECT network layer always contains the following protocol entities:\nOptionally it may also contain others:\nAll these communicate through a Link Control Entity (LCE).\nThe call control protocol is derived from ISDN DSS1, which is a Q.931-derived protocol. Many DECT-specific changes have been made.\nThe mobility management protocol includes the management of identities, authentication, location updating, on-air subscription and key allocation. It includes many elements similar to the GSM protocol, but also includes elements unique to DECT.\nUnlike the GSM protocol, the DECT network specifications do not define cross-linkages between the operation of the entities (for example, Mobility Management and Call Control). The architecture presumes that such linkages will be designed into the interworking unit that connects the DECT access network to whatever mobility-enabled fixed network is involved. By keeping the entities separate, the handset is capable of responding to any combination of entity traffic, and this creates great flexibility in fixed network design without breaking full interoperability.\nDECT GAP is an interoperability profile for DECT. The intent is that two different products from different manufacturers that both conform not only to the DECT standard, but also to the GAP profile defined within the DECT standard, are able to interoperate for basic calling. The DECT standard includes full testing suites for GAP, and GAP products on the market from different manufacturers are in practice interoperable for the basic functions.\nSecurity.\nThe DECT media access control layer includes authentication of handsets to the base station using the DECT Standard Authentication Algorithm (DSAA). When registering the handset on the base, both record a shared 128-bit Unique Authentication Key (UAK). The base can request authentication by sending two random numbers to the handset, which calculates the response using the shared 128-bit key. The handset can also request authentication by sending a 64-bit random number to the base, which chooses a second random number, calculates the response using the shared key, and sends it back with the second random number.\nThe standard also provides encryption services with the DECT Standard Cipher (DSC). The encryption is fairly weak, using a 35-bit initialization vector and encrypting the voice stream with 64-bit encryption. While most of the DECT standard is publicly available, the part describing the DECT Standard Cipher was only available under a non-disclosure agreement to the phones' manufacturers from ETSI.\nThe properties of the DECT protocol make it hard to intercept a frame, modify it and send it later again, as DECT frames are based on time-division multiplexing and need to be transmitted at a specific point in time. Unfortunately very few DECT devices on the market implemented authentication and encryption procedures and even when encryption was used by the phone, it was possible to implement a man-in-the-middle attack impersonating a DECT base station and revert to unencrypted mode which allows calls to be listened to, recorded, and re-routed to a different destination.\nAfter an unverified report of a successful attack in 2002, members of the deDECTed.org project actually did reverse engineer the DECT Standard Cipher in 2008, and as of 2010 there has been a viable attack on it that can recover the key.\nIn 2012, an improved authentication algorithm, the DECT Standard Authentication Algorithm 2 (DSAA2), and improved version of the encryption algorithm, the DECT Standard Cipher 2 (DSC2), both based on AES 128-bit encryption, were included as optional in the NG-DECT/CAT-iq suite.\nDECT Forum also launched the DECT Security certification program which mandates the use of previously optional security features in the GAP profile, such as early encryption and base authentication.\nProfiles.\nVarious access profiles have been defined in the DECT standard:\nAdditional specifications.\nDECT 6.0.\nDECT 6.0 is a North American marketing term for DECT devices manufactured for the United States and Canada operating at 1.9\u00a0GHz. The \"6.0\" does not equate to a spectrum band; it was decided the term DECT 1.9 might have confused customers who equate larger numbers (such as the 2.4 and 5.8 in existing 2.4\u00a0GHz and 5.8\u00a0GHz cordless telephones) with later products. The term was coined by Rick Krupka, marketing director at Siemens and the DECT USA Working Group / Siemens ICM.\nIn North America, DECT suffers from deficiencies in comparison to DECT elsewhere, since the UPCS band (1920\u20131930\u00a0MHz) is not free from heavy interference. Bandwidth is half as wide as that used in Europe (1880\u20131900\u00a0MHz), the 4\u00a0mW average transmission power reduces range compared to the 10\u00a0mW permitted in Europe, and the commonplace lack of GAP compatibility among US vendors binds customers to a single vendor.\nBefore 1.9\u00a0GHz band was approved by the FCC in 2005, DECT could only operate in unlicensed 2.4\u00a0GHz and 900\u00a0MHz Region 2 ISM bands; some users of Uniden WDECT 2.4\u00a0GHz phones reported interoperability issues with Wi-Fi equipment.\nNorth-American products may not be used in Europe, Pakistan, Sri Lanka, and Africa, as they cause and suffer from interference with the local cellular networks. Use of such products is prohibited by European Telecommunications Authorities, PTA, Telecommunications Regulatory Commission of Sri Lanka and the Independent Communication Authority of South Africa. European DECT products may not be used in the United States and Canada, as they likewise cause and suffer from interference with American and Canadian cellular networks, and use is prohibited by the Federal Communications Commission and Innovation, Science and Economic Development Canada.\nDECT 8.0 HD is a marketing designation for North American DECT devices certified with CAT-iq 2.0 \"Multi Line\" profile.\nNG-DECT/CAT-iq.\nCordless Advanced Technology\u2014internet and quality (CAT-iq) is a certification program maintained by the DECT Forum. It is based on New Generation DECT (NG-DECT) series of standards from ETSI.\nNG-DECT/CAT-iq contains features that expand the generic GAP profile with mandatory support for high quality wideband voice, enhanced security, calling party identification, multiple lines, parallel calls, and similar functions to facilitate VoIP calls through SIP and H.323 protocols.\nThere are several CAT-iq profiles which define supported voice features:\nCAT-iq allows any DECT handset to communicate with a DECT base from a different vendor, providing full interoperability. CAT-iq 2.0/2.1 feature set is designed to support IP-DECT base stations found in office IP-PBX and home gateways.\nDECT-2020.\nDECT-2020, also called NR+, is a new radio standard by ETSI for the DECT bands worldwide. The standard was designed to meet a subset of the ITU IMT-2020 5G requirements that are applicable to IOT and Industrial internet of things. DECT-2020 is compliant with the requirements for Ultra Reliable Low Latency Communications URLLC and massive Machine Type Communication (mMTC) of IMT-2020.\nDECT-2020 NR has new capabilities compared to DECT and DECT Evolution:\nThe DECT-2020 standard has been designed to co-exist in the DECT radio band with existing DECT deployments. It uses the same Time Division slot timing and Frequency Division center frequencies and uses pre-transmit scanning to minimize co-channel interference.\nDECT for data networks.\nOther interoperability profiles exist in the DECT suite of standards, and in particular the DPRS (DECT Packet Radio Services) bring together a number of prior interoperability profiles for the use of DECT as a wireless LAN and wireless internet access service. With good range (up to indoors and using directional antennae outdoors), dedicated spectrum, high interference immunity, open interoperability and data speeds of around 500\u00a0kbit/s, DECT appeared at one time to be a superior alternative to Wi-Fi. The protocol capabilities built into the DECT networking protocol standards were particularly good at supporting fast roaming in the public space, between hotspots operated by competing but connected providers. The first DECT product to reach the market, Olivetti's Net3, was a wireless LAN, and German firms Dosch &amp; Amand and Hoeft &amp; Wessel built niche businesses on the supply of data transmission systems based on DECT.\nHowever, the timing of the availability of DECT, in the mid-1990s, was too early to find wide application for wireless data outside niche industrial applications. Whilst contemporary providers of Wi-Fi struggled with the same issues, providers of DECT retreated to the more immediately lucrative market for cordless telephones. A key weakness was also the inaccessibility of the U.S. market, due to FCC spectrum restrictions at that time. By the time mass applications for wireless Internet had emerged, and the U.S. had opened up to DECT, well into the new century, the industry had moved far ahead in terms of performance and DECT's time as a technically competitive wireless data transport had passed.\nHealth and safety.\nDECT uses UHF radio, similar to mobile phones, baby monitors, Wi-Fi, and other cordless telephone technologies.\nIn North America, the 4\u00a0mW average transmission power reduces range compared to the 10\u00a0mW permitted in Europe.\nThe UK Health Protection Agency (HPA) claims that due to a mobile phone's adaptive power ability, a European DECT cordless phone's radiation could actually exceed the radiation of a mobile phone. A European DECT cordless phone's radiation has an average output power of 10\u00a0mW but is in the form of 100 bursts per second of 250\u00a0mW, a strength comparable to some mobile phones.\nMost studies have been unable to demonstrate any link to health effects, or have been inconclusive. Electromagnetic fields may have an effect on protein expression in laboratory settings but have not yet been demonstrated to have clinically significant effects in real-world settings. The World Health Organization has issued a statement on medical effects of mobile phones which acknowledges that the longer term effects (over several decades) require further research."}
{"id": "8676", "revid": "5229428", "url": "https://en.wikipedia.org/wiki?curid=8676", "title": "Dhyana", "text": "Dhyana may refer to:"}
{"id": "8677", "revid": "4842600", "url": "https://en.wikipedia.org/wiki?curid=8677", "title": "December 30", "text": ""}
{"id": "8678", "revid": "39640083", "url": "https://en.wikipedia.org/wiki?curid=8678", "title": "Donn", "text": "In Irish mythology, Donn (\"the dark one\", from ) is an ancestor of the Gaels and is believed to have been a god of the dead. Donn is said to dwell in Tech Duinn (the \"house of Donn\" or \"house of the dark one\"), where the souls of the dead gather. He may have originally been an aspect of the Dagda. Folklore about Donn survived into the modern era in parts of Ireland, in which he is said to be a phantom horseman riding a white horse.\nEarly literary sources.\nA 9th-century poem says that Donn's dying wish was that all his descendants would gather at Donn's house or \"Tech Duinn\" (modern Irish \"Teach Duinn\") after death: \"To me, to my house, you shall all come after your deaths\". The 10th-century tale \"Airne F\u00edngein\" (\"F\u00edngen's Vigil\") says that Tech Duinn is where the souls of the dead gather. In their translation of \"Acallam na Sen\u00f3rach\", Ann Dooley and Harry Roe commented that \"to go to the House of Donn in Irish tradition means to die\". This suggests that the pagan Gaels saw Donn as their ancestor and believed they would go to his abode when they died. Tech Duinn may have been thought of as a place where the souls of the dead gathered before travelling to their final destination in the otherworld, or before being reincarnated. According to Julius Caesar, the Gauls also claimed descent from a god whom he likened to D\u012bs Pater, the Roman god of the underworld.\nThe Christian writers who recorded the \"Lebor Gab\u00e1la \u00c9renn\" made Donn into \u00c9ber Donn one of the mythical Milesian ancestors of the Gaels. The Milesians invade Ireland and take it from the Tuatha D\u00e9 Danann. During their invasion, Donn slights \u00c9riu, one of the eponymous goddesses of Ireland, and he drowns in a shipwreck off the southwest coast. Donn is then buried on a rocky island which becomes known as Tech Duinn. In the literature, Tech Duinn is said to lie at or beyond the western edge of Ireland. Tech Duinn is commonly identified with Bull Rock, an islet off the western tip of the Beara Peninsula. Bull Rock resembles a dolmen or portal tomb as it has a natural tunnel through it, allowing the sea to pass under it as if through a portal. In Ireland there was a belief that the souls of the dead departed westwards over the sea with the setting sun.\nThe \"Metrical Dindshenchas\" entry for \"Tech Duinn\" recounts the tale:Through the incantations of the druids a storm came upon them, and the ship wherein Donn was foundered. 'Let his body be carried to yonder high rock', says Amairgen: 'his folk shall come to this spot.' So hence it is called Tech Duinn: and for this cause, according to the heathen, the souls of sinners visit Tech Duinn before they go to hell, and give their blessing, ere they go, to the soul of Donn. But as for the righteous soul of a penitent, it beholds the place from afar, and is not borne astray. Such, at least, is the belief of the heathen. \u2013 Translation by E. Gwynn\"\nIn the tale \"Togail Bruidne D\u00e1 Derga\" (\"The Destruction of D\u00e1 Derga's Hostel\"), king Conaire M\u00f3r meets his death in Bruiden D\u00e1 Derga (the \"great hall or hostel of the red god\"). On his way to the hostel, Conaire meets three red men riding red horses from the otherworld. They foretell his doom and tell him \"we ride the horses of Donn ... although we are alive, we are dead\". Donn is called \"king of the dead\" in the tale. It has been suggested that D\u00e1 Derga and D\u00e1 Derga's Hostel is another name for Donn and his abode. It may be a name for the death god in the context of violent death or sacrifice, hence the name \"red god\".\nIn the tale \"Tochmarc Treblainne\" (\"The Wooing of Treblann\"), the otherworld woman Treblann elopes with the mortal man Fr\u00e1ech, who sends her to safety in Tech Duinn while he embarks on a quest. In this tale, Donn is said to be the son or foster-son of the Dagda. D\u00e1ith\u00ed \u00d3 h\u00d3g\u00e1in notes similarities between the two and suggests that Donn was originally an epithet of the Dagda.\nDonn is the father of Diarmuid Ua Duibhne, whom he gave in fosterage to the god of youth, Aengus mac \u00d3g, to raise.\nModern sources.\nFolklore about Donn survived into the early modern era. In County Limerick, a Donn F\u00edrinne was said to dwell in the sacred hill of Cnoc F\u00edrinne (Knockfeerina or Knockfierna), and folklore told of people being brought into the hill to be with Donn when they died. He was said to appear as a phantom horseman riding a white horse. He was also associated with the weather: thunder and lightning meant that Donn F\u00edrinne was riding his horse through the sky, and if clouds were over the hill it meant that he was gathering them together to make rain. This imagery may have been influenced by the lore of Odin and his horse Sleipnir from the Norse settlers in Limerick. Donn F\u00edrinne was also said to appear and warn anyone who interfered with his hill. On the west coast of County Clare there was a Donn na Duimhche or Donn Dumhach (\"Donn of the dunes\"), who \"was also often encountered as a night-horseman\". In later folklore, the name 'Donn' came to mean an 'otherworld lord' in general.\nIn modern Irish \"donn\" is the most common word for the colour brown, and by extension can also mean \"sturdy (like) wood\". This is one possible etymology of the English colour \"dun\" (greyish brown)."}
{"id": "8681", "revid": "28438779", "url": "https://en.wikipedia.org/wiki?curid=8681", "title": "Data compression ratio", "text": "Data compression ratio, also known as compression power, is a measurement of the relative reduction in size of data representation produced by a data compression algorithm. It is typically expressed as the division of uncompressed size by compressed size.\nDefinition.\nData compression ratio is defined as the ratio between the \"uncompressed size\" and \"compressed size\":\nThus, a representation that compresses a file's storage size from 10\u00a0MB to 2\u00a0MB has a compression ratio of 10/2 = 5, often notated as an explicit ratio, 5:1 (read \"five\" to \"one\"), or as an implicit ratio, 5/1. This formulation applies equally for compression, where the uncompressed size is that of the original; and for decompression, where the uncompressed size is that of the reproduction. \nSometimes the \"space saving\" is given instead, which is defined as the reduction in size relative to the uncompressed size: \nThus, a representation that compresses the storage size of a file from 10\u00a0MB to 2\u00a0MB yields a space saving of 1 - 2/10 = 0.8, often notated as a percentage, 80%.\nFor signals of indefinite size, such as streaming audio and video, the compression ratio is defined in terms of uncompressed and compressed data rates instead of data sizes: \nand instead of space saving, one speaks of data-rate saving, which is defined as the data-rate reduction relative to the uncompressed data rate: \nFor example, uncompressed songs in CD format have a data rate of 16 bits/channel x 2 channels x 44.1\u00a0kHz \u2245 1.4\u00a0Mbit/s, whereas AAC files on an iPod are typically compressed to 128\u00a0kbit/s, yielding a compression ratio of 10.9, for a data-rate saving of 0.91, or 91%. \nWhen the uncompressed data rate is known, the compression ratio can be inferred from the compressed data rate.\nLossless vs. Lossy.\nLossless compression of digitized data such as video, digitized film, and audio preserves all the information, but it does not generally achieve compression ratio much better than 2:1 because of the intrinsic entropy of the data. Compression algorithms which provide higher ratios either incur very large overheads or work only for specific data sequences (e.g. compressing a file with mostly zeros). In contrast, lossy compression (e.g. JPEG for images, or MP3 and Opus for audio) can achieve much higher compression ratios at the cost of a decrease in quality, such as Bluetooth audio streaming, as visual or audio compression artifacts from loss of important information are introduced. A compression ratio of at least 50:1 is needed to get 1080i video into a 20\u00a0Mbit/s MPEG transport stream.\nUses.\nThe data compression ratio can serve as a measure of the complexity of a data set or signal. In particular it is used to approximate the algorithmic complexity. It is also used to see how much of a file is able to be compressed without increasing its original size."}
{"id": "8683", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=8683", "title": "Disc jockey", "text": "A disc jockey, more commonly abbreviated as DJ, is a person who plays recorded music for an audience. Types of DJs include radio DJs (who host programs on music radio stations), club DJs (who work at nightclubs or music festivals), mobile DJs (who are hired to work at public and private events such as weddings, parties, or festivals), and turntablists (who use record players, usually turntables, to manipulate sounds on phonograph records). Originally, the \"disc\" in \"disc jockey\" referred to shellac and later vinyl records, but nowadays DJ is used as an all-encompassing term to also describe persons who mix music from other recording media such as cassettes, CDs or digital audio files on a CDJ, controller, or even a laptop. DJs may adopt the title \"DJ\" in front of their real names, adopted pseudonyms, or stage names.\nDJs commonly use audio equipment that can play at least two sources of recorded music simultaneously. This enables them to blend tracks together to create transitions between recordings and develop unique mixes of songs. This can involve aligning the beats of the music sources so their rhythms and tempos do not clash when played together and enable a smooth transition from one song to another. DJs often use specialized DJ mixers, small audio mixers with crossfader and cue functions to blend or transition from one song to another. Mixers are also used to pre-listen to sources of recorded music in headphones and adjust upcoming tracks to mix with currently playing music. DJ software can be used with a DJ controller device to mix audio files on a computer instead of a console mixer. DJs may also use a microphone to speak to the audience; effects units such as reverb to create sound effects and electronic musical instruments such as drum machines and synthesizers.\nList of popular DJs.\nSome of the most popular DJs are Skrillex, David Guetta, Porter Robinson, deadmau5, Avicii, Calvin Harris, Martin Garrix, Marshmello, Zedd, Alan Walker, Eric Prydz, DJ Snake, R3HAB, Timmy Trumpet, Ti\u00ebsto, Steve Aoki, Diplo, Nicky Romero, Lost Frequencies, Daft Punk.\nEtymology.\nThe term \"disc jockey\" was ostensibly coined by radio gossip commentator Walter Winchell in 1935 to describe the radio work of Martin Block. The phrase first appeared in print in a 1941 \"Variety\" magazine. Originally, the word \"disc\" in \"disc jockey\" referred to phonograph or gramophone records and was used to describe radio personalities who introduced them on the air.\nRole.\n\"DJ\" is used as an all-encompassing term to describe someone who mixes recorded music from any source, including vinyl records, cassettes, CDs, or digital audio files stored on USB stick or laptop. DJs typically perform for a live audience in a nightclub or dance club or a TV, radio broadcast audience, or an online radio audience. DJs also create mixes, remixes and tracks that are recorded for later sale and distribution. In hip hop music, DJs may create beats, using percussion breaks, basslines and other musical content sampled from pre-existing records. In hip hop, rappers and MCs use these beats to rap over. Some DJs adopt the title \"DJ\" as part of their names (e.g., DJ Jazzy Jeff, DJ Qbert, DJ Shadow and DJ Yoda). Professional DJs often specialize in a specific genre of music, such as techno, house or hip hop music. DJs typically have extensive knowledge about the music they specialize in. Many DJs are avid music collectors of vintage, rare or obscure tracks and records.\nTypes.\nClub DJs.\nClub DJs, commonly referred to as DJs in general, play music at musical events, such as parties at music venues or bars, clubs, music festivals, corporate and private events. Typically, club DJs mix music recordings from two or more sources using different mixing techniques to produce a non-stopping flow of music. Mixing began with hip hop in the 1970s and would subsequently expand to other genres in especially (but not exclusively) dance music. \nOne key technique used for seamlessly transitioning from one song to another is beatmatching. A DJ who mostly plays and mixes one specific music genre is often given the title of that genre; for example, a DJ who plays hip hop music is called a hip hop DJ, a DJ who plays house music is a house DJ, a DJ who plays techno is called a techno DJ, and so on.\nThe quality of a DJ performance (often called a DJ mix or DJ set) consists of two main features: technical skills, or how well the DJ can operate the equipment and produce smooth transitions between two or more recordings and a playlist; and the ability of a DJ to select the most suitable recordings, also known as \"reading the crowd\".\nHip hop DJs.\nDJ Kool Herc, Grandmaster Flash, and Afrika Bambaataa were members of a block party at South Bronx from 1973 onwards. Kool Herc played records such as James Brown's \"Give It Up or Turnit a Loose\", Jimmy Castor's \"It's Just Begun\", Booker T. &amp; the M.G.'s' \"Melting Pot\", Incredible Bongo Band's \"Bongo Rock\" and \"Apache\", and UK rock band Babe Ruth's \"The Mexican\". With Bronx clubs struggling with street gangs, uptown DJs catering to an older disco crowd with different aspirations, and commercial radio also catering to a demographic distinct from teenagers in the Bronx, Herc's parties had a ready-made audience.\nDJ Kool Herc developed the style that was the blueprint for hip hop music. Herc used the record to focus on a short, heavily percussive part in it: the \"break\". Since this part of the record was the one the dancers liked best, Herc isolated the break and prolonged it by changing between two record players. As one record reached the end of the break, he cued a second record back to the beginning of the break, which allowed him to extend a relatively short section of music into a \"five-minute loop of fury\". This innovation had its roots in what Herc called \"The Merry-Go-Round\", a technique by which the DJ switched from break to break at the height of the party. This technique is specifically called \"The Merry-Go-Round\" because according to Herc, it takes one \"back and forth with no slack.\"\nRadio DJs.\nRadio DJs or radio personalities introduce and play music broadcasts on AM, FM, digital or Internet radio stations.\nDancehall/reggae deejays.\nIn Jamaican music, a deejay (DJ) is a reggae or dancehall musician who sings and \"toasts\" (recites poetry) to an instrumental riddim. Deejays are not to be confused with DJs from other music genres like hip hop, where they select and play music. Dancehall/reggae DJs who select riddims to play are called selectors. Deejays whose style is nearer to singing are sometimes called singjays.\nThe term deejay originated in the 1960s and 1970s when performers such as U-Roy and King Stitt toasted over the instrumental (dub music) versions of popular records. These versions were often released on the flip side to the song's 45 record. This gave the deejays the chance to create on-the-fly lyrics to the music. Big Youth, and I-Roy were famous deejays in Jamaica.\nTurntablists.\nTurntablists, also called battle DJs, use turntables and DJ mixer to manipulate recorded sounds to produce new music. In essence, they use DJ equipment as a musical instrument. Perhaps the best-known turntablist technique is scratching. Turntablists often participate in DJ contests like DMC World DJ Championships and Red Bull 3Style.\nResidents.\nA resident DJ performs at a venue on a regular basis or permanently. They would perform regularly (typically under an agreement) in a particular discotheque, a particular club, a particular event, or a particular broadcasting station. Residents have a decisive influence on the club or a series of events. Per agreement with the management or company, the DJ would have to perform under agreed times and dates. Typically, DJs perform as residents for two or three times in a week, for example, on Friday and Saturday. DJs who make a steady income from a venue are also considered resident DJs.\nExamples for resident DJs are:\nWomen DJs.\nIn Western popular music, even though there are relatively few women DJs and turntablists, women musicians have achieved great success in singing and songwriting roles, however, they are given much less representation than men DJs. Part of this may stem from a generally low percentage of women in audio technology-related jobs. A 2013 \"Sound on Sound\" article by Rosina Ncube stated that there are \"... few women in record production and sound engineering.\" Ncube states that \"[n]inety-five percent of music producers are male, and although there are women producers achieving great things in music, they are less well-known than their counterparts.\" The vast majority of students in music technology programs are male. In hip hop music, the low percentage of women DJs and turntablists may stem from the overall men's domination of the entire hip hop music industry. Most of the top rappers, MCs, DJs, record producers and music executives are men. There are a small number of high-profile women, but they are rare. \nIn 2007, Mark Katz's article \"Men, Women, and Turntables: Gender and the DJ Battle\", stated that \"very few women [do turntablism] battle[s]; the matter has been a topic of conversation among hip-hop DJs for years.\" In 2010, Rebekah Farrugia said \"the male-centricity of electronic dance music (EDM) culture\" contributes to \"a marginalisation of women in these [EDM] spaces.\" While turntablism and broader DJ practices should not be conflated, Katz suggests use or lack of use of the turntable broadly by women across genres and disciplines is impacted upon by what he defines as \"male technophilia\". Historian Ruth Oldenziel concurs in her writing on engineering with this idea of socialization as a central factor in the lack of engagement with technology. She says:an exclusive focus on women's supposed failure to enter the field \u2013 is insufficient for understanding how our stereotypical notions have come into being; it tends to put the burden of proof entirely on women and to [unreasonably] blame them for their supposedly inadequate socialization, their lack of aspiration, and their want of masculine values. An equally challenging question is why and how boys have come to love things technical, how boys have historically been socialized as technophiles.\nLucy Green has focused on gender in relation to musical performers and creators, and specifically on educational frameworks as they relate to both. She suggests that women's alienation from \"areas that have a strong technological tendency such as DJing, sound engineering and producing\" are \"not necessarily about their dislike of these instruments but relates to the interrupting effect of their dominantly masculine delineations\". Despite this, women and girls do increasingly engage in turntable and DJ practices, individually and collectively, and \"carve out spaces for themselves in EDM and DJ Culture\". A 2015 article cited a number of prominent women DJs: Hannah Wants, Ellen Allien, Miss Kittin, Monika Kruse, Nicole Moudaber, B.Traits, Magda, Nina Kraviz, Nervo, and Annie Mac. Two years later, another article brings out a list with world-famous women DJs including Nastia, tINY, Nora En Pure, Anja Schneider, Peggy Gou, Maya Jane Coles, and Eli &amp; Fur.\nAmerican DJ The Blessed Madonna has been called \"one of the world's most exciting turntablists\". Her stage name is a tribute to her mother's favorite Catholic saint, Black Madonna. In 2018, The Blessed Madonna played herself as an in-residence DJ for the video game \"Grand Theft Auto Online\", as part of the \"After Hours\" DLC.\nThere are various projects dedicated to the promotion and support of these practices such as Female DJs London. Some artists and collectives go beyond these practices to be more gender inclusive. For example, Discwoman, a New York-based collective and booking agency, describe themselves as \"representing and showcasing cis women, trans women and genderqueer talent.\"\nIn Japan, the newest Bushiroad franchise: \"D4DJ\" focuses on an all-women DJ unit.\nEquipment.\nDJs use equipment that enables them to play multiple sources of recorded music and mix them to create seamless transitions and unique arrangements of songs. An important tool for DJs is the specialized DJ mixer, a small audio mixer with a crossfader and cue functions. The crossfader enables the DJ to blend or transition from one song to another. The cue knobs or switches allow the DJ to \"listen\" to a source of recorded music in headphones before playing it for the live club or broadcast audience. Previewing the music in headphones helps the DJ pick the next track they want to play, cue up the track to the desired starting location, and align the two tracks' beats in traditional situations where auto-sync technology is not being used. This process ensures that the selected song will mix well with the currently playing music. DJs may align the beats of the music sources so their rhythms do not clash when they are played together to help create a smooth transition from one song to another. Other equipment may include a microphone, effects units such as reverb, and electronic musical instruments such as drum machines and synthesizers.\nAs music technology has progressed, DJs have adopted different types of equipment to play and mix music, all of which are still commonly used. Traditionally, DJs used two turntables plugged into a DJ mixer to mix music on vinyl records. As compact discs became popular media for publishing music, specialized high-quality CD players known as CDJs were developed for DJs. CDJs can take the place of turntables or be used together with turntables. Many CDJs can now play digital music files from USB flash drives or SD cards in addition to CDs. With the spread of portable laptops, tablets, and smartphone computers, DJs began using software together with specialized sound cards and DJ controller hardware. DJ software can be used in conjunction with a hardware DJ mixer or be used instead of a hardware mixer.\nTurntables.\nTurntables allow DJs to play vinyl records. By adjusting the playback speed of the turntable, either by adjusting the speed knob or by manipulating the platter (e.g., by slowing down the platter by putting a finger gently along the side), DJs can match the tempos of different records so their rhythms can be played together at the same time without clashing or make a smooth, seamless transition from one song to another. This technique is known as beatmatching. DJs typically replace the rubber mat on turntables that keep the record moving in sync with the turntable with a slipmat that facilitates manipulating the playback of the record by hand. With the slipmat, the DJ can stop or slow down the record while the turntable is still spinning. Direct-drive turntables are the type preferred by DJs. Belt-drive turntables are less expensive, but they are not suitable for turntablism and DJing, because the belt-drive motor can be damaged by this type of manipulation. Some DJs, most commonly those who play hip hop music, go beyond merely mixing records and use turntables as musical instruments for scratching, beat juggling, and other turntablism techniques.\nCDJs/media players.\nCDJs / media players are high-quality digital media players made for DJing. They often have large jog wheels and pitch controls to allow DJs to manipulate the playback of digital files for beatmatching similar to how DJs manipulate vinyl records on turntables. CDJs often have features such as loops and waveform displays similar to DJ software. Originally designed to play music from compact discs, they now can play digital music files stored on USB flash drives and SD cards. Some CDJs can also connect to a computer running DJ software to act as a DJ controller. Modern media players have the ability to stream music from online music providers such as Beatport, Beatsource, Tidal and SoundCloud GO.\nDJ mixers.\nDJ mixers are small audio mixing consoles specialized for DJing. Most DJ mixers have far fewer channels than a mixer used by a record producer or audio engineer; whereas standard live sound mixers in small venues have 12 to 24 channels, and standard recording studio mixers have even more (as many as 72 on large boards), basic DJ mixers may have only two channels. While DJ mixers have many of the same features found on larger mixers (faders, equalization knobs, gain knobs, effects units, etc.), DJ mixers have a feature that is usually only found on DJ mixers: the crossfader. The crossfader is a type of fader that is mounted horizontally. DJs used the crossfader to mix two or more sound sources. The midpoint of the crossfader's travel is a 50/50 mix of the two channels (on a two-channel mixer). The far left side of the crossfader provides only the channel A sound source. The far right side provides only the channel B sound source (e.g., record player number 2). Positions in between the two extremes provide different mixes of the two channels. Some DJs use a computer with DJ software and a DJ controller instead of an analog DJ mixer to mix music, although DJ software can be used in conjunction with a hardware DJ mixer.\nHeadphones.\nDJs generally use higher-quality headphones than those designed for music consumers. DJ headphones have other properties useful for DJs, such as designs that acoustically isolate the sounds of the headphones from the outside environment (hard shell headphones), flexible headbands and pivot joints to allow DJs to listen to one side of the headphones while turning the other headphone away (so they can monitor the mix in the club), and replaceable cables. Replaceable cables enable DJs to buy new cables if a cable becomes frayed, worn, or damaged, or if a cable is accidentally cut.\nClosed-back headphones are highly recommended for DJs to block outside noise as the environment of DJ usually tends to be very noisy. Standard headphones have a 3.5mm jack but DJ equipment usually requires \u00bc inch jack. Most specialized DJ Headphones have an adapter to switch between a 3.5mm jack and \u00bc inch jack. Detachable coiled cables are perfect for DJ Headphones.\nSoftware.\nDJs have changed their equipment as new technologies are introduced. The earliest DJs in pop music, in 1970s discos, used record turntables, vinyl records and audio consoles. In the 1970s, DJs would have to lug heavy direct-drive turntables and crates of records to clubs and shows. In the 1980s, many DJs transitioned to compact cassettes. In the 1990s and 2000s, many DJs switched to using digital audio such as CDs and MP3 files. As technological advances made it practical to store large collections of digital music files on a laptop computer, DJ software was developed so DJs could use a laptop as a source of music instead of transporting CDs or vinyl records to gigs. Unlike most music player software designed for regular consumers, DJ software can play at least two audio files simultaneously, display the waveforms of the files on screen and enable the DJ to listen to either source.\nThe waveforms allow the DJ to see what is coming next in the music and how the playback of different files is aligned. The software analyzes music files to identify their tempo and where the beats are. The analyzed information can be used by the DJ to help manually beatmatch like with vinyl records or the software can automatically synchronize the beats. Digital signal processing algorithms in software allow DJs to adjust the tempo of recordings independently of their pitch (and musical key, a feature known as \"keylock\". Some software analyzes the loudness of the music for automatic normalization with ReplayGain and detects the musical key. Additionally, DJ software can store cue points, set loops, and apply effects.\nAs tablet computers and smartphones became widespread, DJ software was written to run on these devices in addition to laptops.\nDJ software requires specialized hardware in addition to a computer to fully take advantage of its features. The consumer-grade, regular sound card integrated into most computer motherboards can only output two channels (one stereo pair). However, DJs need to be able to output at least four channels (two stereo pairs, thus Left and Right for input 1 and Left and Right for input 2), either unmixed signals to send to a DJ mixer or the main output plus a headphone output. Additionally, DJ sound cards output higher-quality signals than the sound cards built into consumer-grade computer motherboards.\nTimecode.\nSpecial vinyl records (or CDs/digital files played with CDJs) can be used with DJ software to play digital music files with DJ software as if they were pressed onto vinyl, allowing turntablism techniques to be used with digital files. These vinyl records do not have music recordings pressed onto them. Instead, they are pressed with a special signal, referred to as \"timecode\", to control DJ software. The DJ software interprets changes in the playback speed, direction, and position of the timecode signal and manipulates the digital files it is playing in the same way that the turntable manipulates the timecode record.\nThis requires a specialized DJ sound card with at least 4 channels (2 stereo pairs) of inputs and outputs. With this setup, the DJ software typically outputs unmixed signals from the music files to an external hardware DJ mixer. Some DJ mixers have integrated USB sound cards that allow DJ software to connect directly to the mixer without requiring a separate sound card.\nDJ controllers.\nA DJ software can be used to mix audio files on the computer instead of a separate hardware mixer. When mixing on a computer, DJs often use a DJ controller device that mimics the layout of two turntables plus a DJ mixer to control the software rather than the computer keyboard &amp; touchpad on a laptop, or the touchscreen on a tablet computer or smartphone. Many DJ controllers have an integrated sound card with 4 output channels (2 stereo pairs) that allow the DJ to use headphones to preview music before playing it on the main output.\nTechniques.\nSeveral techniques are used by DJs as a means to better mix and blend recorded music. These techniques primarily include the cueing, equalization and audio mixing of two or more sound sources. The complexity and frequency of special techniques depend largely on the setting in which a DJ is working. Radio DJs are less likely to focus on advanced music-mixing procedures than club DJs, who rely on a smooth transition between songs using a range of techniques. However, some radio DJs are experienced club DJs, so they use the same sophisticated mixing techniques.\nClub DJ turntable techniques include beatmatching, phrasing and slip-cueing to preserve energy on a dance floor. Turntablism embodies the art of cutting, beat juggling, scratching, needle drops, phase shifting, back spinning and more to perform the transitions and overdubs of samples in a more creative manner (although turntablism is often considered a use of the turntable as a musical instrument rather than a tool for blending recorded music). Professional DJs may use harmonic mixing to choose songs that are in compatible musical keys. Other techniques include chopping, screwing and looping.\nRecent advances in technology in both DJ hardware and software can provide assisted or automatic completion of some traditional DJ techniques and skills. Examples include phrasing and beatmatching, which can be partially or completely automated by using DJ software that performs automatic synchronization of sound recordings, a feature commonly labelled \"sync\". Most DJ mixers now include a beat counter which analyzes the tempo of an incoming sound source and displays its tempo in beats per minute (BPM), which may assist with beatmatching analog sound sources.\nIn the past, being a DJ has largely been a self-taught craft but with the complexities of new technologies and the convergence with music production methods, there are a growing number of schools and organizations that offer instruction on the techniques.\nMiming.\nIn DJ culture, miming refers to the practice of DJ's pantomiming the actions of live-mixing a set on stage while a pre-recorded mix plays over the sound system. Miming mixing in a live performance is considered to be controversial within DJ culture. Some within the DJ community say that miming is increasingly used as a technique by celebrity model DJs who may lack mixing skills, but can draw big crowds to a venue.\nDuring a DJ tour for the release of the French group Justice's \"A Cross the Universe\" in November 2008, controversy arose when a photograph of Aug\u00e9 DJing with an unplugged Akai MPD24 surfaced. The photograph sparked accusations that Justice's live sets were faked. Aug\u00e9 has since said that the equipment was unplugged very briefly before being reattached and the band put a three-photo set of the incident on their MySpace page. After a 2013 Disclosure concert, the duo was criticized for pretending to live mix to a playback of a pre-recorded track. Disclosure's Guy Lawrence said they did not deliberately intend to mislead their audience, and cited miming by other DJs such as David Guetta.\nHistory.\nPlaying recorded music for dancing and parties rose with the mass marketing of home phonographs in the late 19th century. \nBritish radio disc jockey Jimmy Savile hosted his first live dance party in 1943 using a single turntable and a makeshift sound system. Four years later, Savile began using two turntables welded together to form a single DJ console. In 1947, the Whisky \u00e0 Gogo opened in Paris as the first discotheque. In 1959, one of the first discos in Germany, the Scotch Club, opened in Aachen and visiting journalist Klaus Quirini (later DJ Heinrich) made comments, conducted audience games, and announced songs while playing records. The first song he played was the hit \"Ein Schiff wird kommen\" by Lale Andersen.\nIn the 1960s, Rudy Bozak began making the first DJ mixers, mixing consoles specialized for DJing.\nIn the late 1960s to early 1970s Jamaican sound system culture, producer and sound system operator (DJ), (Jamaican) King Tubby and producer Lee \"Scratch\" Perry were pioneers of the genre known as dub music. They experimented with tape-based composition; emphasized repetitive rhythmic structures (often stripped of their harmonic elements); electronically manipulated spatiality; sonically manipulated pre-recorded musical materials from mass media; and remixed music among other innovative techniques. It is widely known that the Jamaican dancehall culture has had and continues to have a significant impact on the American hip hop culture.\nDJ turntablism has origins in the invention of direct-drive turntables. Early belt-drive turntables were unsuitable for turntablism and mixing, since they had a slow start-up time, and they were prone to wear-and-tear and breakage, as the belt would break from backspinning or scratching. The first direct-drive turntable was invented by engineer Shuichi Obata at Matsushita (now Panasonic), based in Osaka, Japan. It eliminated belts, and instead employed a motor to directly drive a platter on which a vinyl record rests. In 1969, Matsushita released it as the SP-10, the first direct-drive turntable on the market, and the first in their influential Technics series of turntables.\nIn 1972, Technics started making their SL-1200 turntable, featuring high torque direct drive design. The SL-1200 had a rapid start and its durable direct drive enabled DJs to manipulate the platter, as with scratching techniques. Hip hop DJs began using the Technics SL-1200s as musical instruments to manipulate records with turntablism techniques such as scratching and beat juggling rather than merely mixing records. These techniques were developed in the 1970s by DJ Kool Herc, Grand Wizard Theodore, and Afrika Bambaataa, as they experimented with Technics direct-drive decks, finding that the motor would continue to spin at the correct RPM even if the DJ wiggled the record back and forth on the platter. \nIn 1980, Japanese company Roland released the TR-808, an analog rhythm/drum machine, which has unique artificial sounds, such as its booming bass and sharp snare, and a metronome-like rhythm. Yellow Magic Orchestra's use of the instrument in 1980 influenced hip hop pioneer Afrika Bambaataa, after which the TR-808 would be widely adopted by hip hop DJs, with 808 sounds remaining central to hip-hop music ever since. The Roland TB-303, a bass synthesizer released in 1981, had a similar impact on electronic dance music genres such as techno and house music, along with Roland's TR-808 and TR-909 drum machines.\nIn 1982, the Compact Disc (CD) format was released, popularizing digital audio. In 1998, the first MP3 digital audio player, the Eiger Labs MPMan F10, was introduced. In January of that same year at the BeOS Developer Conference, N2IT demonstrated FinalScratch, the first digital DJ system to allow DJs control of MP3 files through special time-coded vinyl records or CDs. While it would take some time for this novel concept to catch on with the \"die-hard Vinyl DJs\", this would become the first step in the Digital DJ revolution. Manufacturers joined with computer DJing pioneers to offer professional endorsements, the first being Professor Jam (a.k.a. William P. Rader), who went on to develop the industry's first dedicated computer DJ convention and learning program, the \"CPS (Computerized Performance System) DJ Summit\", to help spread the word about the advantages of this emerging technology.\nIn 2001, Pioneer DJ began producing the CDJ-1000 CD player, making the use of digital music recordings with traditional DJ techniques practical for the first time. As the 2000s progressed, laptop computers became more powerful and affordable. DJ software, specialized DJ sound cards, and DJ controllers were developed for DJs to use laptops as a source of music rather than turntables or CDJs. In the 2010s, like laptops before them, tablet computers and smartphones became more powerful &amp; affordable. DJ software was written to run on these more portable devices instead of laptops, although laptops remain the more common type of computer for DJing.\nHealth concerns.\nThe risk of DJs working in nightclubs with loud music includes noise-induced hearing loss and tinnitus. Nightclubs constantly exceed safe levels of noise exposure with average sound levels ranging from 93.2 to 109.7 dB. Constant music exposure creates temporary and permanent auditory dysfunction for professional DJs with average levels at 96dB being above the recommended level, at which ear protection is mandatory for industry. Three-quarters of DJs have tinnitus and are at risk of tenosynovitis in the wrists and other limbs. Tenosynovitis results from staying in the same position over multiple gigs for scratching motion and cueing, this would be related to a repetitive strain injury. Gigs can last 4-5 hours in the nightlife and hospitality industry, as a result, there are potential complications of prolonged standing which include slouching, varicose veins, cardiovascular disorders, joint compression, and muscle fatigue. This is common for other staff to experience as well including bartenders and security staff for example."}
{"id": "8685", "revid": "17482101", "url": "https://en.wikipedia.org/wiki?curid=8685", "title": "Athenian Empire", "text": ""}
{"id": "8687", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=8687", "title": "Detroit", "text": "Detroit ( , ) is the most populous city in the U.S. state of Michigan. It is the largest U.S. city on the Canadian border and the county seat of Wayne County. Detroit had a population of 639,111 at the 2020 census, making it the 26th-most populous city in the United States. The Metro Detroit area, home to 4.3 million people, is the second-largest in the Midwest after the Chicago metropolitan area and the 14th-largest in the United States. A significant cultural center, Detroit is known for its contributions to music, art, architecture and design, in addition to its historical automotive background.\nIn 1701, Royal French explorers Antoine de la Mothe Cadillac and Alphonse de Tonty founded Fort Pontchartrain du D\u00e9troit. During the late 19th and early 20th century, it became an important industrial hub at the center of the Great Lakes region in the Midwestern United States. The city's population rose to be the fourth-largest in the nation by 1920, with the expansion of the automotive industry in the early 20th century. One of its main features, the Detroit River, became the busiest commercial hub in the world. In the mid-20th century, Detroit entered a state of urban decay which has continued to the present, as a result of industrial restructuring, the loss of jobs in the auto industry, and rapid suburbanization. Since reaching a peak of 1.85\u00a0million at the 1950 census, Detroit's population has declined by more than 65 percent. In 2013, Detroit became the largest U.S. city to file for bankruptcy, but successfully exited in 2014.\nDetroit is a port on the Detroit River, one of the four major straits that connect the Great Lakes system to the St. Lawrence Seaway. The city anchors the third-largest regional economy in the Midwest and the 16th-largest in the United States. It is also best known as the center of the U.S. automotive industry, and the \"Big Three\" auto manufacturers\u2014General Motors, Ford, and Stellantis North America (Chrysler)\u2014are all headquartered in Metro Detroit. It houses the Detroit Metropolitan Airport, one of the most important hub airports in the United States. Detroit and its neighboring Canadian city Windsor constitute the second-busiest international crossing in North America, after San Diego\u2013Tijuana.\nDetroit's culture is marked with diversity, having both local and international influences. Detroit gave rise to the music genres of Motown and techno, and also played an important role in the development of jazz, hip-hop, rock, and punk. A globally unique stock of architectural monuments and historic places was the result of the city's rapid growth in its boom years. Since the 2000s, conservation efforts have managed to save many architectural pieces and achieve several large-scale revitalizations, including the restoration of several historic theaters and entertainment venues, high-rise renovations, new sports stadiums, and a riverfront revitalization project. Detroit is an increasingly popular tourist destination which caters to about 16\u00a0million visitors per year. In 2015, Detroit was given a name called \"City of Design\" by UNESCO, the first and only U.S. city to receive that designation.\nHistory.\nToponymy.\nDetroit is named after the Detroit River, connecting Lake Huron with Lake Erie. The name comes from the French language word meaning as the city was situated on a narrow north\u2013south passage of water linking the two lakes. The river was known as in the French language, which means . In the historical context, the strait included the St. Clair River, Lake St. Clair, and the Detroit River.\nEarly settlement.\nPaleo-Indians inhabited areas near Detroit as early as 11,000 years ago including the culture referred to as the Mound Builders. By the 17th century, the region was inhabited by Huron, Odawa, Potawatomi, and Iroquois peoples. The area is known by the Anishinaabe people as \"Waawiiyaataanong\", translating to 'where the water curves around'.\nThe first Europeans did not penetrate into the region and reach the straits of Detroit until French missionaries and traders worked their way around the Iroquois League, with whom they were at war in the 1630s. The Huron and Neutral people held the north side of Lake Erie until the 1650s, when the Iroquois pushed them and the Erie people away from the lake and its beaver-rich feeder streams in the Beaver Wars of 1649\u20131655. By the 1670s, the war-weakened Iroquois laid claim to as far south as the Ohio River valley in northern Kentucky as hunting grounds, and had absorbed many other Iroquoian peoples after defeating them in war. For the next hundred years, virtually no British or French action was contemplated without consultation with the Iroquois or consideration of their likely response.\nFrench settlement.\nOn July 24, 1701, the French explorer Antoine de la Mothe Cadillac (1658\u20131730), with his lieutenant\u00a0Alphonse de Tonty (1659\u20131727), and more than a hundred other Royal French settlers traveling south and west from New France (modern Province of Quebec), along the St. Lawrence River valley to the Great Lakes region, began constructing a small fort on the north bank of the Detroit River. Cadillac named the settlement Fort Pontchartrain du D\u00e9troit, after Louis Ph\u00e9lypeaux, comte de Pontchartrain (1643\u20131727), the Secretary of State of the Navy under King Louis XIV (1638\u20131715, reigned 1643\u20131718) in the Royal government in Paris. \"Sainte-Anne-de-D\u00e9troit\" was founded on July 26 and is the second-oldest continuously operating Roman Catholic parish in the United States. France offered free land to colonists to attract families further west into the Great Lakes region interior of the North American continent to Detroit; when it eventually reached a population of about 800 by 1765, after the colonial conflict of the French and Indian War (1753\u20131763), (Seven Years' War in Europe), it became the largest European settlement between the important towns of Montreal and New Orleans, both also French settlements, in the former colonies of New France and La Louisiane (further south on the Mississippi River, on the coast of the Gulf of Mexico), respectively.\nBritish rule.\nDuring the French and Indian War (1753\u201363)\u2014the North American front of the Seven Years' War in Europe between the Kingdom of Great Britain and the Kingdom of France\u2014British troops gained control of the settlement a few years into the conflict in 1760 and shortened its name to Detroit. Several regional Native American tribes, such as the Potowatomi, Ojibwe and Huron, launched Pontiac's War (1763\u20131766), and laid siege in 1763 to Fort Detroit along the Detroit River in the Great Lakes but failed to capture it. In defeat, France ceded its territory in North America of New France and south of the lakes east of the Mississippi to the Appalachian Mountains to Britain following the war.\nWhen Great Britain evicted France from its colonial possessions in New France (Canada) in the peace terms of the Treaty of Paris of 1763, it also removed one barrier to American colonists migrating west across the mountains. British negotiations with the Iroquois would both prove critical and lead to the Royal Proclamation of 1763, which limited settlements South of and below the Great Lakes and west of the Alleghenies / Appalachians. Many colonists and pioneers in the Thirteen Colonies along the East Coast, resented and then simply defied this restraint, later becoming supporters of the rebellious American Revolution. By 1773, after the addition of increasing numbers of the Anglo-American settlers, the population of Detroit and Fort Detroit, was edging up to 1,400 (doubled in the previous decade). During the American Revolutionary War (1775\u20131783), the indigenous and loyalist raids of 1778 and the resultant 1779 decisive Sullivan Expedition reopened the Ohio Country (north of the Ohio River and west of the mountains) to even more westward emigration, which began almost immediately to get away from the eastern warfare. By 1778, its population had doubled again, reaching 2,144 and it was the third-largest town in what was known then as the Province of Quebec since the British takeover of former French colonial possessions in North America in 1763.\nAfter the American Revolutionary War (1775\u20131783) and the establishment and recognition of the United States as an independent country, Britain (United Kingdom) ceded Detroit and other territories in the interior region of the continent, south of the Great Lakes and west of the Appalachian Mountains chain to the Mississippi River under the peace of the terms of the 1783 Treaty of Paris, which established the southern border with its still continuing colonial provinces of what remained of British North America, later provinces of Upper Canada and Lower Canada. However, the disputed border area remained under British control with several military forts and trading posts for another decade, and its forces did not fully withdraw until 1796, following the negotiations and ratification of the subsequent Jay Treaty of 1794 between the British and Americans. By the turn of the 19th century, white American settlers began pouring westwards across the Appalachians and through the Great Lakes.\nThe region's then colonial economy was based on the lucrative fur trade, in which numerous Native American people had important roles as trappers and traders. Today the municipal flag of Detroit reflects its both its French and English colonial heritage. Descendants of the earliest French and French-Canadian settlers formed a cohesive community, who gradually were superseded as the dominant population after more Anglo-American settlers arrived in the early 19th century with American westward migration. Living along the shores of Lake St. Clair and south to Monroe and downriver suburbs, the ethnic French Canadians of Detroit, also known as Muskrat French in reference to the fur trade, remain a subculture in the region up into the 21st century.\n19th century.\nThe Great Detroit Fire of 1805 destroyed most of the Detroit settlement, which had primarily buildings made of wood. One stone fort, a river warehouse, and brick chimneys of former wooden homes were the sole structures to survive. Of the 600 Detroit residents in this area, none died in the fire. The legacy of the fire of 1805 lives on in many aspects of modern Detroit heritage. The cities motto, \"Speramus Meliora; Resurget Cineribus\" was coined by Roman Catholic priest Father Gabriel Richard (1767\u20131832), as he looked out at the ruins of the city in the fire's aftermath. The city seal, designed by J.O. Lewis in 1827, directly depicts the Great Fire of 1805. Two women stand in the foreground while on the left, the city burns in the background and a woman weeps over the destruction. The woman on the right consoles her by gesturing to a new city that will rise in its place. The city seal also forms the center of the flag of the city.\nFrom 1805 to 1847, Detroit was the territorial capital city of the old federal Michigan Territory (1805\u20131837), and later first state capital, in January 1837, when after 32 years, the old federal territory was admitted by act of the United States Congress and approved by seventh President Andrew Jackson (1769\u20131845, served 1829\u20131837), as the 26th state to the federal Union on its northern border.\nWilliam Hull (1753\u20131825), the United States Army elderly commander at Fort Detroit, surrendered without a fight to attacking British Army troops from adjacent Upper Canada in the northeast and their Native American allies laying siege during the first months of the War of 1812 (1812\u20131815) in the siege of Detroit in mid-August 1812, only two months after the Congress in Washington declared war. He was fooled and led to believing his forces were vastly outnumbered by the attacking / surrounding \"Redcoats\". Five months later in the conflict, the Battle of Frenchtown in January 1813, was part of a U.S. military effort to retake the fort and town, and U.S. troops suffered their highest number of fatalities of any battle in the so-called Second War for Independence. This battle is commemorated at the nearby River Raisin National Battlefield Park south of Detroit in Monroe County. Detroit was eventually recaptured by the strengthened and better prepared United States military forces later that year.\nThe settlement was incorporated as a city in 1815. As the city expanded, a radial geometric street plan with a logical grid and broad avenues, developed by U.S. Judge for the old Michigan Territory (Chief Justice) Augustus B. Woodward (1774\u20131827, served 1805\u20131824), was followed (where Detroit gets its namesake and major downtown street of Woodward Avenue), featuring grand boulevards and plazas, inspired and influenced by the design of the American federal national capital city of Washington, D.C. in the 1790s by French engineer and architect, Pierre L'Enfant, plus a later example as later laid out across the Atlantic Ocean by the Emperor Napoleon I (Napoleon Bonaparte) in the capital city of Paris in his First French Empire (France) to improve and transform the old Middle Ages / medieval city of Europe, and subsequently further developed by his nephew and later successor Emperor Napoleon III in the 1850s. In 1817, Woodward went on to establish the Catholepistemiad, or University of Michigania in the city. Intended to be a centralized system of schools, libraries, and other cultural and scientific institutions for the old federal Michigan Territory (1805\u20131837), the Catholepistemiad evolved into the modern University of Michigan at Ann Arbor.\nPrior to the American Civil War 1861\u20131865), the city's access to the Canada\u2013U.S. international border made it a key stop for refugee slaves gaining freedom in the North along the Underground Railroad. Many simply went further north across the Detroit River to Canada to escape pursuit by rampaging Southern slave catchers. An estimated 20,000 to 30,000 African-American refugees settled in Canada. Two prominent African American activists and abolitionists George DeBaptiste (c.1815-1875), was considered to be the \"president\" of the Detroit Underground Railroad, William Lambert (1817\u20131890), the \"vice president\" or \"secretary\", plus the elderly Quaker white woman Laura Smith Haviland (1808\u20131898), the \"superintendent\".\nNumerous men from Detroit volunteered to fight for the Federal Union and enlisted in its Union Army (United States Army) during the American Civil War, including the 24th Michigan Infantry Regiment. It was part of the famous Iron Brigade, which fought with distinction and suffered 82% casualties at the Battle of Gettysburg in 1863. When the First Volunteer Infantry Regiment arrived to fortify the federal national capital city of Washington, D.C. in the early days of the War in April 1861, newly elected and inaugurated 16th President Abraham Lincoln (1809\u20131865, served 1861\u20131865), was quoted as saying, \"Thank God for Michigan!!\" George Armstrong Custer (1839\u20131876), led the Michigan Brigade during the Civil War and called them the \"Wolverines\". The city's tensions over race, and nationally, the draft led to the Detroit race riot of 1863, in which violence erupted, leaving some dead and over 200 Black residents homeless. This prompted the establishment of a full-time police force in 1865.\nDuring the late 19th century, wealthy industry and shipping magnates commissioned the design and construction of several Gilded Age mansions east and west of the current downtown, along the major avenues of the Woodward plan. Most notable among them was the David Whitney House at 4421 Woodward Avenue, and the grand avenue became a favored address for mansions. During this period, some referred to Detroit as the \"Paris of the West\" for its architecture, grand avenues in the Paris style, and for Washington Boulevard, recently electrified by Thomas Edison. The city had grown steadily from the 1830s with the rise of shipping, shipbuilding, and manufacturing industries. Strategically located along the Great Lakes waterway, Detroit emerged as a major port and transportation hub. \nIn 1896, a thriving carriage trade prompted Henry Ford to build his first automobile in a rented workshop on Mack Avenue. During this growth period, Detroit expanded its borders by annexing all or part of several surrounding villages and townships.\n20th century.\nIn 1903, Henry Ford founded the Ford Motor Company. Ford's manufacturing\u2014and those of automotive pioneers William C. Durant, Horace and John Dodge, James and William Packard, and Walter Chrysler\u2014established the Big Three automakers and cemented Detroit's status in the early 20th century as the world's automotive capital. The growth of the auto industry was reflected by changes in businesses throughout the Midwest and nation, with the development of garages to service vehicles and gas stations, as well as factories for parts and tires. Because of the booming auto industry, Detroit became the fourth-largest city in the nation by 1920, following New York City, Chicago, and Philadelphia.\nIn 1907, the Detroit River carried 67,292,504 tons of shipping commerce through Detroit to locations all over the world. For comparison, London shipped 18,727,230 tons, and New York shipped 20,390,953 tons. The river was dubbed \"the Greatest Commercial Artery on Earth\" by \"The Detroit News\" in 1908. The prohibition of alcohol from 1920 to 1933 resulted in the Detroit River becoming a major conduit for smuggling of illegal Canadian spirits.\nWith the rapid growth of industrial workers in the auto factories, labor unions such as the American Federation of Labor and the United Auto Workers (UAW) fought to organize workers to gain them better working conditions and wages. They initiated strikes and other tactics in support of improvements such as the 8-hour day/40-hour work week, increased wages, greater benefits, and improved working conditions. The labor activism during those years increased the influence of union leaders in the city such as Jimmy Hoffa of the Teamsters and Walter Reuther of the UAW.\nDetroit, like many places in the United States, developed racial conflict and discrimination in the 20th century following the rapid demographic changes as hundreds of thousands of new workers were attracted to the industrial city. The Great Migration brought rural blacks from the South; they were outnumbered by southern whites who also migrated to the city. Immigration brought southern and eastern Europeans of Catholic, Jewish, and Orthodox Christian faith; these new groups competed with native-born whites for jobs and housing in the booming city.\nDetroit was one of the major Midwest cities that was a site for the dramatic urban revival of the Ku Klux Klan (KKK) beginning in 1915. \"By the 1920s the city had become a stronghold of the KKK\", whose members primarily opposed Catholic and Jewish immigrants but also practiced discrimination against Black Americans. Even after the decline of the KKK in the late 1920s, the Black Legion, a secret vigilante group, was active in the Detroit area in the 1930s. One-third of its estimated 20,000 to 30,000 members in Michigan were based in the city. It was defeated after numerous prosecutions following the kidnapping and murder in 1936 of Charles Poole, a Catholic organizer with the federal Works Progress Administration. Some 49 men of the Black Legion were convicted of numerous crimes, with many sentenced to life in prison for murder.\nBy 1940, 80% of Detroit deeds contained restrictive covenants prohibiting African Americans from buying houses they could afford. These discriminatory tactics were successful as a majority of black people in Detroit resorted to living in all-black neighborhoods such as Black Bottom and Paradise Valley. At this time, white people still made up about 90.4% of the city's population. White residents attacked black homes: breaking windows, starting fires, and detonating bombs.\nWorld War II.\nIn the 1940s the world's \"first urban depressed freeway\" ever built, the Davison, was constructed. During World War II, the government encouraged retooling of the American automobile industry in support of the Allied powers, leading to Detroit's key role in the American Arsenal of Democracy. Jobs expanded so rapidly due to the defense buildup in World War II that 400,000 people migrated to the city from 1941 to 1943, including 50,000 blacks in the second wave of the Great Migration, and 350,000 whites, many of them from the South. Whites, including ethnic Europeans, feared black competition for jobs and scarce housing. The federal government prohibited discrimination in defense work, but when in June 1943 Packard promoted three black people to work next to whites on its assembly lines, 25,000 white workers walked off the job. The 1943 Detroit race riot took place in June, three weeks after the Packard plant protest, beginning with an altercation at Belle Isle. A total of 34 people were killed, 25 of them black and most at the hands of the white police force, while 433 were wounded (75% of them black), and property valued at $2 million (worth $30.4 million in 2020) was destroyed. Rioters moved through the city, and young whites traveled across town to attack more settled blacks in their neighborhood of Paradise Valley.\nPostwar era.\nIndustrial mergers in the 1950s, especially in the automobile sector, increased oligopoly in the American auto industry. Detroit manufacturers such as Packard and Hudson merged into other companies and eventually disappeared. At its peak population of 1,849,568, in the 1950 Census, the city was the fifth-largest in the United States.\nIn this postwar era, the auto industry continued to create opportunities for many African Americans from the South, who continued with their Great Migration to Detroit and other northern and western cities to escape the strict Jim Crow laws and racial discrimination policies of the South. Postwar Detroit was a prosperous industrial center of mass production. The auto industry comprised about 60% of all industry in the city, allowing space for a plethora of separate booming businesses including stove making, brewing, furniture building, oil refineries, pharmaceutical manufacturing, and more. The expansion of jobs created unique opportunities for black Americans, who saw novel high employment rates: there was a 103% increase in the number of blacks employed in postwar Detroit. \nBlack Americans who immigrated to northern industrial cities from the south still faced intense racial discrimination in the employment sector. Racial discrimination kept the workforce and better jobs predominantly white, while many black Detroiters held lower-paying factory jobs. Despite changes in demographics as the city's black population expanded, Detroit's police force, fire department, and other city jobs continued to be held by predominantly white residents. This created an unbalanced racial power dynamic.\nUnequal opportunities in employment resulted in unequal housing opportunities for the majority of the black community: with overall lower incomes and facing the backlash of discriminatory housing policies, the black community was limited to lower cost, lower quality housing in the city. The surge in the black population augmented the strain on housing scarcity. The livable areas available to the black community were limited, and as a result, families often crowded together in unsanitary, unsafe, and illegal quarters. Such discrimination became increasingly evident in the policies of redlining implemented by banks and federal housing groups, which almost completely restricted the ability of blacks to improve their housing and encouraged white people to guard the racial divide that defined their neighborhoods. As a result, black people were often denied bank loans to obtain better housing, and interest rates and rents were unfairly inflated to prevent their moving into white neighborhoods. White residents and political leaders largely opposed the influx of black Detroiters to white neighborhoods, believing that their presence would lead to neighborhood deterioration. This perpetuated a cyclical exclusionary process that marginalized the agency of black Detroiters by trapping them in the unhealthiest, least safe areas of the city.\nAs in other major American cities in the postwar era, modernist planning ideology drove the construction of a federally subsidized, extensive highway and freeway system around Detroit, and pent-up demand for new housing stimulated suburbanization; highways made commuting by car for higher-income residents easier. However, this construction had negative implications for many lower-income urban residents. Highways were constructed through and completely demolished neighborhoods of poor residents and black communities who had less political power to oppose them. The neighborhoods were mostly low income, considered blighted, or made up of older housing where investment had been lacking due to racial redlining, so the highways were presented as a kind of urban renewal. These neighborhoods (such as Black Bottom and Paradise Valley) were extremely important to the black communities of Detroit, providing spaces for independent black businesses and social/cultural organizations. Their destruction displaced residents with little consideration of the effects of breaking up functioning neighborhoods and businesses.\nIn 1956, Detroit's last heavily used electric streetcar line, which traveled along the length of Woodward Avenue, was removed and replaced with gas-powered buses. It was the last line of what had once been a 534-mile network of electric streetcars. In 1941, at peak times, a streetcar ran on Woodward Avenue every 60 seconds.\nAll of these changes in the area's transportation system favored low-density, auto-oriented development rather than high-density urban development. Industry also moved to the suburbs, seeking large plots of land for single-story factories. By the 21st century, the metro Detroit area had developed as one of the most sprawling job markets in the United States; combined with poor public transport, this resulted in many new jobs being beyond the reach of urban low-income workers.\nIn 1950, the city held about one-third of the state's population. Over the next 60 years, the city's population declined to less than 10 percent of the state's population. During the same time period, the sprawling metropolitan area grew to contain more than half of Michigan's population. The shift of population and jobs eroded Detroit's tax base.\n1960s and racial tension.\nIn June 1963, Rev. Martin Luther King Jr. gave a major speech as part of a civil rights march in Detroit that foreshadowed his \"I Have a Dream\" speech in Washington, D.C., two months later. While the civil rights movement gained significant federal civil rights laws in 1964 and 1965, longstanding inequities resulted in confrontations between the police and inner-city black youth who wanted change.\nLongstanding tensions in Detroit culminated in the Twelfth Street riot in July 1967. Governor George W. Romney ordered the Michigan National Guard into Detroit, and President Lyndon B. Johnson sent in U.S. Army troops. The result was 43 dead, 467 injured, over 7,200 arrests, and more than 2,000 buildings destroyed, mostly in black residential and business areas. Thousands of small businesses closed permanently or relocated to safer neighborhoods. The affected district lay in ruins for decades. According to the \"Chicago Tribune\", it was the 3rd most costly riot in the United States.\nOn August 18, 1970, the NAACP filed suit against Michigan state officials, including Governor William Milliken, charging \"de facto\" public school segregation. The NAACP argued that although schools were not legally segregated, the city of Detroit and its surrounding counties had enacted policies to maintain racial segregation in public schools. The NAACP also suggested a direct relationship between unfair housing practices and educational segregation, as the composition of students in the schools followed segregated neighborhoods. The District Court found in favor of the NAACP, and held all levels of government accountable for the segregation in its ruling. The Sixth Circuit Court of Appeals affirmed some of the decision, holding that it was the state's responsibility to integrate across the segregated metropolitan area. The U.S. Supreme Court took up the case February 27, 1974. The subsequent \"Milliken v. Bradley\" decision had nationwide influence. In a narrow decision, the Supreme Court found schools were a subject of local control, and suburbs could not be forced to aid with the desegregation of the city's school district.\n\"Milliken was perhaps the greatest missed opportunity of that period\", said Myron Orfield, professor of law at the University of Minnesota Law School. \"Had that gone the other way, it would have opened the door to fixing nearly all of Detroit's current problems.\" John Mogk, a professor of law and an expert in urban planning at Wayne State University Law School in Detroit, says, \nEverybody thinks that it was the riots [in 1967] that caused the white families to leave. Some people were leaving at that time but, really, it was after Milliken that you saw mass flight to the suburbs. If the case had gone the other way, it is likely that Detroit would not have experienced the steep decline in its tax base that has occurred since then.\n1970s and decline.\nIn November 1973, the city elected Coleman Young as its first black mayor. After taking office, Young emphasized increasing racial diversity in the police department, which was predominantly white. Young also worked to improve Detroit's transportation system, but the tension between Young and his suburban counterparts over regional matters was problematic throughout his mayoral term.\nIn 1976, the federal government offered $600\u00a0million (~$ in ) for building a regional rapid transit system, under a single regional authority. But the inability of Detroit and its suburban neighbors to solve conflicts over transit planning resulted in the region losing the majority of funding for rapid transit. The city then moved forward with construction of the elevated downtown circulator portion of the system, which became known as the Detroit People Mover.\nThe gasoline crises of 1973 and 1979 affected auto industry. Buyers chose smaller, more fuel-efficient cars made by foreign makers as the price of gas rose. Efforts to revive the city were stymied by the struggles of the auto industry, as their sales and market share declined. Automakers laid off thousands of employees and closed plants in the city, further eroding the tax base. To counteract this, the city used eminent domain to build two large new auto assembly plants in the city.\nYoung sought to revive the city by seeking to increase investment in the city's declining downtown. The Renaissance Center, a mixed-use office and retail complex, opened in 1977. This group of skyscrapers was an attempt to keep businesses in downtown. Young also gave city support to other large developments to attract middle and upper-class residents back to the city. Despite the Renaissance Center and other projects, the downtown area continued to lose businesses to the automobile-dependent suburbs. Major stores and hotels closed, and many large office buildings went vacant. Young was criticized for being too focused on downtown development and not doing enough to lower the city's high crime rate and improve city services to residents.\nHigh unemployment was compounded by middle-class flight to the suburbs, and some residents leaving the state to find work. The result for the city was a higher proportion of poor in its population, reduced tax base, depressed property values, abandoned buildings, abandoned neighborhoods, and high crime rates.\n1990s.\nIn 1993, Coleman Young retired as Detroit's longest-serving mayor, deciding not to seek a sixth term. He was succeeded by Dennis Archer. Archer prioritized downtown development, easing tensions with its suburban neighbors. A referendum to allow casino gambling in the city passed in 1996; several temporary casino facilities opened in 1999, and permanent downtown casinos with hotels opened in 2007\u201308.\n21st century.\nCampus Martius, a reconfiguration of downtown's main intersection as a new park, was opened in 2004. The park has been cited as one of the best public spaces in the United States. In 2001, the first portion of the International Riverfront redevelopment was completed as a part of the city's 300th-anniversary celebration.\nIn September 2008, Mayor Kwame Kilpatrick (who had served for six years) resigned following felony convictions. In 2013, Kilpatrick was convicted on 24 federal felony counts, including mail fraud, wire fraud, and racketeering, and was sentenced to 28 years in federal prison. The former mayor's activities cost the city an estimated $20\u00a0million. Roughly half of the owners of Detroit's 305,000 properties failed to pay their 2011 tax bills, resulting in about $246\u00a0million (~$ in ) in taxes and fees going uncollected, nearly half of which was due to Detroit. The rest of the money would have been earmarked for Wayne County, Detroit Public Schools, and the library system.\nThe city's financial crisis resulted in Michigan taking over administrative control of its government. Governor Rick Snyder declared a financial emergency in March 2013, stating the city had a $327\u00a0million budget deficit and faced more than $14\u00a0billion in long-term debt. It had been making ends meet on a month-to-month basis with the help of bond money held in a state escrow account and had instituted mandatory unpaid days off for many city workers. Those troubles, along with underfunded city services, such as police and fire departments, and ineffective turnaround plans from Mayor Bing and the City Council led the state of Michigan to appoint an emergency manager for Detroit. On June 14, 2013, Detroit defaulted on $2.5\u00a0billion of debt by withholding $39.7\u00a0million in interest payments, while Emergency Manager Kevyn Orr met with bondholders and other creditors in an attempt to restructure the city's $18.5\u00a0billion debt and avoid bankruptcy. On July 18, 2013, Detroit became the largest U.S. city to file for bankruptcy. It was declared bankrupt by U.S. District Court on December 3, with its $18.5\u00a0billion debt. On November 7, 2014, the city's plan for exiting bankruptcy was approved. On December 11 the city officially exited bankruptcy. The plan allowed the city to eliminate $7\u00a0billion in debt and invest $1.7\u00a0billion into improved city services.\nOne way the city obtained this money was through the Detroit Institute of Arts (DIA). Holding over 60,000 pieces of art worth billions of dollars, some saw it as the key to funding this investment. The city came up with a plan to monetize the art and sell it, leading to the DIA becoming a private organization. After months of legal battles, the city finally got hundreds of millions of dollars towards funding a new Detroit.\nOne of the largest post-bankruptcy efforts to improve city services has been to fix the city's broken street lighting system. At one time it was estimated that 40% of lights were not working, which resulted in public safety issues and abandonment of housing. The plan called for replacing outdated high-pressure sodium lights with 65,000 LED lights. Construction began in late 2014 and finished in December 2016; Detroit is the largest U.S. city with all LED street lighting.\nIn the 2010s, several initiatives were taken by Detroit's citizens and new residents to improve the cityscape by renovating and revitalizing neighborhoods. Such projects include volunteer renovation groups and various urban gardening movements. Miles of associated parks and landscaping have been completed in recent years. In 2011, the Port Authority Passenger Terminal opened, with the riverwalk connecting Hart Plaza to the Renaissance Center.\nOne symbol of the city's decades-long decline, the Michigan Central Station, was long vacant. The city renovated it with new windows, elevators and facilities, completing the work in December 2015. In 2018, Ford Motor Company purchased the building and plans to use it for mobility testing with a potential return of train service. Several other landmark buildings have been privately renovated and adapted as condominiums, hotels, offices, or for cultural uses. Detroit was mentioned as a city of renaissance and has reversed many of the trends of the prior decades.\nThe city has seen a rise in gentrification. In downtown, for example, the construction of Little Caesars Arena brought with it high class shops and restaurants along Woodward Avenue. Office tower and condominium construction has led to an influx of wealthy families but also a displacement of long-time residents and culture. Areas outside of downtown and other recently revived areas have an average household income of about 25% less than the gentrified areas, a gap that is continuing to grow. Rents and cost of living in these gentrified areas rise every year, pushing minorities and the poor out, causing more and more racial disparity and separation in the city. In 2019, the cost of a one-bedroom loft in Rivertown reached $300,000 (~$ in ), with a five-year sale price change of over 500% and average income rising by 18%.\nGeography.\nMetropolitan area.\nDetroit is the center of a three-county urban area (with a population of 3,734,090 within an area of according to the 2010 United States census), six-county metropolitan statistical area (population of 5,322,219 in an area of as of the 2010 census), and a nine-county Combined Statistical Area (population of 5.3\u00a0million within ).\nTopography.\nAccording to the U.S. Census Bureau, the city has a total area of , of which is land and is water. Detroit is the principal city in Metro Detroit and Southeast Michigan. It is situated in the Midwestern United States and the Great Lakes region.\nThe Detroit River International Wildlife Refuge is the only international wildlife preserve in North America and is uniquely located in the heart of a major metropolitan area. The refuge includes islands, coastal wetlands, marshes, shoals, and waterfront lands along of the Detroit River and western Lake Erie shoreline.\nThe city slopes gently from the northwest to southeast on a till plain composed largely of glacial and lake clay. The most notable topographical feature in the city is the Detroit Moraine, a broad clay ridge on which the older portions of Detroit and Windsor are located, rising approximately above the river at its highest point. The highest elevation in the city is directly north of Gorham Playground on the northwest side approximately three blocks south of 8 Mile Road, at a height of . Detroit's lowest elevation is along the Detroit River, at a surface height of .\nBelle Isle Park is a island park in the Detroit River, between Detroit and Windsor, Ontario. It is connected to the mainland by the MacArthur Bridge. Belle Isle Park contains such attractions as the James Scott Memorial Fountain, the Belle Isle Conservatory, the Detroit Yacht Club on an adjacent island, a half-mile (800 m) beach, a golf course, a nature center, monuments, and gardens. Both the Detroit and Windsor skylines can be viewed at the island's Sunset Point.\nThree road systems cross the city: the original French template, with avenues radiating from the waterfront, and true north\u2013south roads based on the Northwest Ordinance township system. The city is north of Windsor, Ontario. Detroit is the only major city along the Canada\u2013U.S. border in which one travels south in order to cross into Canada.\nDetroit has four border crossings: the Ambassador Bridge and the Detroit\u2013Windsor tunnel provide motor vehicle thoroughfares, with the Michigan Central Railway Tunnel providing railroad access to and from Canada. The fourth border crossing is the Detroit\u2013Windsor Truck Ferry, near the Windsor Salt Mine and Zug Island. Near Zug Island, the southwest part of the city was developed over a salt mine that is below the surface. The Detroit salt mine run by the Detroit Salt Company has over of roads within.\nCityscape.\nArchitecture.\nSeen in panorama, Detroit's waterfront shows a variety of architectural styles. The postmodern Neo-Gothic spires of Ally Detroit Center were designed to refer to the city's Art Deco skyscrapers. Together with the Renaissance Center, these buildings form a distinctive and recognizable skyline. Examples of the Art Deco style include the Guardian Building and Penobscot Building downtown, as well as the Fisher Building and Cadillac Place in the New Center area near Wayne State University. Among the city's prominent structures are United States' largest Fox Theatre, the Detroit Opera House, and the Detroit Institute of Arts, all built in the early 20th century.\nWhile the Downtown and New Center areas contain high-rise buildings, the majority of the surrounding city consists of low-rise structures and single-family homes. Outside of the city's core, residential high-rises are found in upper-class neighborhoods such as the East Riverfront, extending toward Grosse Pointe, and the Palmer Park neighborhood just west of Woodward. The University Commons-Palmer Park district in northwest Detroit, near the University of Detroit Mercy and Marygrove College, anchors historic neighborhoods including Palmer Woods, Sherwood Forest, and the University District.\nForty-two significant structures or sites are listed on the National Register of Historic Places. Neighborhoods constructed prior to World War II feature the architecture of the times, with wood-frame and brick houses in the working-class neighborhoods, larger brick homes in middle-class neighborhoods, and ornate mansions in upper-class neighborhoods such as Brush Park, Woodbridge, Indian Village, Palmer Woods, Boston-Edison, and others.\nSome of the oldest neighborhoods are along the major Woodward and East Jefferson corridors, which formed spines of the city. Some newer residential construction may also be found along the Woodward corridor and in the far west and northeast. The oldest extant neighborhoods include West Canfield and Brush Park. There have been multi-million dollar restorations of existing homes and construction of new homes and condominiums here.\nThe city has one of the United States' largest surviving collections of late 19th- and early 20th-century buildings. Architecturally significant churches and cathedrals in the city include St. Joseph's, Old St. Mary's, the Sweetest Heart of Mary, and the Cathedral of the Most Blessed Sacrament.\nThe city has substantial activity in urban design, historic preservation, and architecture. A number of downtown redevelopment projects\u2014of which Campus Martius Park is one of the most notable\u2014have revitalized parts of the city. Grand Circus Park and historic district is near the city's theater district; Ford Field, home of the Detroit Lions, and Comerica Park, home of the Detroit Tigers. Little Caesars Arena, a new home for the Detroit Red Wings and the Detroit Pistons, with attached residential, hotel, and retail use, opened in 2017. The plans for the project call for mixed-use residential on the blocks surrounding the arena and the renovation of the vacant 14-story Eddystone Hotel. It will be a part of The District Detroit, a group of places owned by Olympia Entertainment Inc., including Comerica Park and the Detroit Opera House, among others.\nThe Detroit International Riverfront includes a partially completed three-and-one-half-mile riverfront promenade with a combination of parks, residential buildings, and commercial areas. It extends from Hart Plaza to the MacArthur Bridge, which connects to Belle Isle Park, the largest island park in a U.S. city. The riverfront includes Tri-Centennial State Park and Harbor, Michigan's first urban state park. The second phase is a extension from Hart Plaza to the Ambassador Bridge for a total of of parkway from bridge to bridge. Civic planners envision the pedestrian parks will stimulate residential redevelopment of riverfront properties condemned under eminent domain.\nOther major parks include River Rouge (in the southwest side), the largest park in Detroit; Palmer (north of Highland Park) and Chene Park (on the east river downtown).\nNeighborhoods.\nDetroit has a variety of neighborhood types. The revitalized Downtown, Midtown, Corktown, New Center areas feature many historic buildings and are high density, while further out, particularly in the northeast and on the fringes, high vacancy levels are problematic, for which a number of solutions have been proposed. In 2007, Downtown Detroit was recognized as the best city neighborhood in which to retire among the United States' largest metro areas by CNNMoney editors.\nLafayette Park is a revitalized neighborhood on the city's east side, part of the Ludwig Mies van der Rohe residential district. The development was originally called the Gratiot Park. Planned by Mies van der Rohe, Ludwig Hilberseimer and Alfred Caldwell it includes a landscaped, park with no through traffic, in which these and other low-rise apartment buildings are situated. Immigrants have contributed to the city's neighborhood revitalization, especially in southwest Detroit. Southwest Detroit has experienced a thriving economy in recent years, as evidenced by new housing, increased business openings and the recently opened Mexicantown International Welcome Center.\nThe city has numerous neighborhoods consisting of vacant properties resulting in low inhabited density in those areas, stretching city services and infrastructure. These neighborhoods are concentrated in the northeast and on the city's fringes. A 2009 parcel survey found about a quarter of residential lots in the city to be undeveloped or vacant, and about 10% of the city's housing to be unoccupied. The survey also reported that most (86%) of the city's homes are in good condition with a minority (9%) in fair condition needing only minor repairs.\nTo deal with vacancy issues, the city has begun demolishing the derelict houses, razing 3,000 of the total 10,000 in 2010, but the resulting low density creates a strain on the city's infrastructure. To remedy this, a number of solutions have been proposed including resident relocation from more sparsely populated neighborhoods and converting unused space to urban agricultural use, including Hantz Woodlands, though the city expects to be in the planning stages for up to another two years.\nPublic funding and private investment have been made with promises to rehabilitate neighborhoods. In April 2008, the city announced a $300 million (~$ in ) stimulus plan to create jobs and revitalize neighborhoods, financed by city bonds and paid for by earmarking about 15% of the wagering tax. The city's working plans for neighborhood revitalizations include 7-Mile/Livernois, Brightmoor, East English Village, Grand River/Greenfield, North End, and Osborn. Private organizations have pledged substantial funding to the efforts. Additionally, the city has cleared a section of land for large-scale neighborhood construction, which the city is calling the \"Far Eastside Plan\". In 2011, Mayor Dave Bing announced a plan to categorize neighborhoods by their needs and prioritize the most needed services for those neighborhoods.\nParks.\nDetroit Parks &amp; Recreation maintains 308 public parks, totaling 4,950 (2,003 ha) acres or about 5.6% of the city's land area. Belle Isle Park, Detroit's largest and most visited park is the largest city-owned island park in the U.S., covering 982 acres (397 ha).\nGrand Circus, the city's first municipal park, opened in 1847. In the early 20th century, the city enlisted landscape architect Augustus Woodward to conceive a framework for Detroit's modern parks system. Augustus Woodward's plan for the city imagined grand boulevards, spacious and elegant common parks, and an orderly, hub-and-spoke city layout.\nThe Huron-Clinton Metropolitan Authority was created in 1940 by the citizens of Southeast Michigan to serve as a regional park system the park system includes 13 parks totaling more than 24,000 acres (97\u00a0km2) arranged along the Huron River and Clinton River forming a partial ring around the Detroit metro area.\nClimate.\nDetroit and the rest of southeastern Michigan have a hot-summer humid continental climate (K\u00f6ppen: \"Dfa\") which is influenced by the Great Lakes like other places in the state; the city and close-in suburbs are part of USDA Hardiness zone 6b, while the more distant northern and western suburbs generally are included in zone 6a. Winters are cold, with moderate snowfall and temperatures not rising above freezing on an average 44\u00a0days annually, while dropping to or below on an average 4.4\u00a0days a year; summers are warm to hot with temperatures exceeding on 12\u00a0days. The warm season runs from May to September. The monthly daily mean temperature ranges from in January to in July. Official temperature extremes range from on July 24, 1934, down to on January 21, 1984; the record low maximum is on January 19, 1994, while, conversely the record high minimum is on August 1, 2006, the most recent of five occurrences. A decade or two may pass between readings of or higher, which last occurred July 17, 2012. The average window for freezing temperatures is October 20 through April 22, allowing a growing season of 180 days.\nPrecipitation is moderate and somewhat evenly distributed throughout the year, although the warmer months such as May and June average more, averaging annually, but historically ranging from in 1963 to in 2011. Snowfall, which typically falls in measurable amounts between November 15 through April 4 (occasionally in October and very rarely in May), averages per season, although historically ranging from in 1881\u201382 to in 2013\u201314. A thick snowpack is not often seen, with an average of only 27.5\u00a0days with or more of snow cover. Thunderstorms are frequent in the Detroit area. These usually occur during spring and summer.\nDemographics.\nIn the 2020 United States census, the city had 639,111 residents, ranking it the 27th-most populous city in the US. Of the large shrinking cities in the US, Detroit has had the most dramatic decline in the population of the past 70 years (down 1,210,457) and the second-largest percentage decline (down 65.4%). In 1950, Detroit was the fourth-largest city in the US behind New York, Chicago, and Philadelphia. While the drop in Detroit's population has been ongoing since 1950, the most dramatic period was the significant 25% decline between the 2000 and 2010 census.\nDetroit's 639,111 residents represent 269,445 households, and 162,924 families residing in the city. The population density was . There were 349,170 housing units at an average density of . Of the 269,445 households, 34.4% had children under the age of 18 living with them, 21.5% were married couples living together, 31.4% had a female householder with no husband present, 39.5% were non-families, 34.0% were made up of individuals, and 3.9% had someone living alone who was 65 years of age or older. The average household size was 2.59, and the average family size was 3.36.\nThere was a wide distribution of age in the city, with 31.1% under the age of 18, 9.7% from 18 to 24, 29.5% from 25 to 44, 19.3% from 45 to 64, and 10.4% 65 years of age or older. The median age was 31 years. For every 100 females, there were 89.1 males. For every 100 females age 18 and over, there were 83.5 males.\nReligion.\nAccording to a 2014 study, 67% of the population of the city identified themselves as Christians, with 49% professing adherence to Protestant churches, and 16% professing Roman Catholic beliefs, while 24% claim no religious affiliation. Other religions collectively make up about 8% of the population.\nIncome and employment.\nThe loss of industrial and working-class jobs in the city has resulted in high rates of poverty and associated problems. From 2000 to 2009, the city's estimated median household income fell from $29,526 to $26,098. , the mean income of Detroit is below the overall U.S. average by several thousand dollars. Of every three Detroit residents, one lives in poverty. Luke Bergmann, author of \"Getting Ghost: Two Young Lives and the Struggle for the Soul of an American City\", said in 2010, \"Detroit is now one of the poorest big cities in the country\".\nIn the 2018 American Community Survey, median household income in the city was $31,283, compared with the median for Michigan of $56,697. The median income for a family was $36,842, well below the state median of $72,036. 33.4% of families had income at or below the federally defined poverty level. Out of the total population, 47.3% of those under the age of 18 and 21.0% of those 65 and older had income at or below the federally defined poverty line.\nRace and ethnicity.\nBeginning with the rise of the automobile industry, Detroit's population increased more than sixfold during the first half of the 20th century as an influx of European, Middle Eastern (Lebanese, Assyrian), and Southern migrants brought their families to the city. With this economic boom following World War I, the African American population grew from a mere 6,000 in 1910 to more than 120,000 by 1930. Perhaps one of the most overt examples of neighborhood discrimination occurred in 1925 when African American physician Ossian Sweet found his home surrounded by an angry mob of his hostile white neighbors violently protesting his new move into a traditionally white neighborhood. Sweet and ten of his family members and friends were put on trial for murder as one of the mob members throwing rocks at the newly purchased house was shot and killed by someone firing out of a second-floor window.\nDetroit has a relatively large Mexican-American population. In the early 20th century, thousands of Mexicans came to Detroit to work in agricultural, automotive, and steel jobs. During the Mexican Repatriation of the 1930s many Mexicans in Detroit were willingly repatriated or forced to repatriate. By the 1940s much of the Mexican community began to settle what is now Mexicantown. Immigration from Jalisco significantly increased the Latino population in the 1990s. By 2010 Detroit had 48,679 Hispanics, including 36,452 Mexicans: a 70% increase from 1990. Per the 2023 American Community Survey five-year estimates, the Mexican American population was 35,273 comprising over 75% of the Latino population with Puerto Ricans as the next largest group at 5,887.\nAfter World War II, many people from Appalachia also settled in Detroit. Appalachians formed communities and their children acquired southern accents. Many Lithuanians also settled in Detroit during the World War II era, especially on the city's Southwest side in the West Vernor area, where the renovated Lithuanian Hall reopened in 2006.\nWhile African Americans in 2020 comprised 13.5% of Michigan's population, they made up nearly 77.2% of Detroit's population. The next largest population groups were non-Hispanic whites, at 10.1%, and Hispanics, at 8.0%. In 2001, 103,000 Jews, or about 1.9% of the population, were living in the Detroit area. According to the 2010 census, segregation in Detroit decreased in absolute and relative terms and in the first decade of the 21st century, about two-thirds of the total black population in the metropolitan area resided within the city limits of Detroit. The number of integrated neighborhoods increased from 100 in 2000 to 204 in 2010. After being ranked the most segregated metropolitan area in the United States in 2000, Detroit was ranked fourth most-segregated in 2010. A 2011 op-ed in \"The New York Times\" attributed the decreased segregation rating to the overall exodus from the city, cautioning that these areas may soon become more segregated. \nThere are four areas of Detroit with significant Asian and Asian American populations. Northeast Detroit has a large population of Hmong with a smaller group of Lao people. A portion of Detroit next to eastern Hamtramck includes Bangladeshi Americans, Indian Americans, and Pakistani Americans; nearly all of the Bangladeshi population in Detroit lives in that area. The area north of downtown has transient Asian national origin residents who are university students or hospital workers. Few of them have permanent residency after schooling ends. They are mostly Chinese and Indian but the population also includes Filipinos, Koreans, and Pakistanis. In Southwest and western Detroit there are smaller, scattered Asian communities.\nCrime.\nDetroit has gained notoriety for its high amount of crime, having struggled with it for decades. The number of homicides in 1974 was 714. The homicide rate in 2022 was the third highest in the nation at 50.0 per 100,000. Downtown typically has lower crime than national and state averages. According to a 2007 analysis, Detroit officials note about 65 to 70 percent of homicides in the city were drug related, with the rate of unsolved murders roughly 70%.\nAlthough the rate of violent crime dropped 11% in 2008, violent crime in Detroit has not declined as much as the national average from 2007 to 2011. The violent crime rate is one of the highest in the United States. Neighborhoodscout.com reported a crime rate of 62.18 per 1,000 residents for property crimes, and 16.73 per 1,000 for violent crimes (compared to national figures of 32 per 1,000 for property crimes and 5 per 1,000 for violent crime in 2008). In 2012, crime in the city was among the reasons for more expensive car insurance.\nAreas of the city adjacent to the Detroit River are also patrolled by the United States Border Patrol.\nEconomy.\nSeveral major corporations are based in the city, including three Fortune 500 companies. The most heavily represented sectors are manufacturing (particularly automotive), finance, technology, and health care. The most significant companies based in Detroit include General Motors, Rocket Mortgage, Ally Financial, Compuware, Shinola, American Axle, Little Caesars, DTE Energy, Lowe Campbell Ewald, Blue Cross Blue Shield of Michigan, and Rossetti Architects.\nAbout 80,500 people work in downtown Detroit, comprising one-fifth of the city's employment base. Aside from the numerous Detroit-based companies listed above, downtown contains large offices for Comerica, Chrysler, Fifth Third Bank, HP Enterprise, Deloitte, PricewaterhouseCoopers, KPMG, and Ernst &amp; Young. Ford Motor Company is in the adjacent city of Dearborn.\nThousands more employees work in Midtown, north of the central business district. Midtown's anchors are the city's largest single employer Detroit Medical Center, Wayne State University, and the Henry Ford Health System in New Center. Midtown is also home to watchmaker Shinola and an array of small and startup companies. New Center bases TechTown, a research and business incubator hub that is part of the Wayne State University system. Like downtown, Corktown Is experiencing growth with the new Ford Corktown Campus under development.\nMany downtown employers are relatively new, as there has been a marked trend of companies moving from satellite suburbs into the downtown core. Compuware completed its world headquarters in downtown in 2003. OnStar, Blue Cross Blue Shield, and HP Enterprise Services are at the Renaissance Center. PricewaterhouseCoopers Plaza offices are adjacent to Ford Field, and Ernst &amp; Young completed its office building at One Kennedy Square in 2006. Perhaps most prominently, in 2010, Quicken Loans, one of the largest mortgage lenders, relocated its world headquarters and 4,000 employees to downtown Detroit, consolidating its suburban offices. In July 2012, the U.S. Patent and Trademark Office opened its Elijah J. McCoy Satellite Office in the Rivertown/Warehouse District as its first location outside Washington, D.C.'s metropolitan area.\nIn April 2014, the United States Department of Labor reported the city's unemployment rate at 14.5%.\nThe city of Detroit and other public\u2013private partnerships have attempted to catalyze the region's growth by facilitating the building and historical rehabilitation of residential high-rises in the downtown, creating a zone that offers many business tax incentives, creating recreational spaces such as the Detroit RiverWalk, Campus Martius Park, Dequindre Cut Greenway, and Green Alleys in Midtown. The city has cleared sections of land while retaining some historically significant vacant buildings in order to spur redevelopment; even though it has struggled with finances, the city issued bonds in 2008 to provide funding for ongoing work to demolish blighted properties. Two years earlier, downtown reported $1.3\u00a0billion in restorations and new developments which increased the number of construction jobs in the city. In the decade prior to 2006, downtown gained more than $15\u00a0billion in new investment from private and public sectors.\nDespite the city's recent financial issues, many developers remain unfazed by Detroit's problems. Midtown is one of the most successful areas within Detroit to have a residential occupancy rate of 96%. Numerous developments have been recently completed or are in various stages of construction. These include the $82\u00a0million reconstruction of downtown's David Whitney Building (now an Aloft Hotel and luxury residences), the Woodward Garden Block Development in Midtown, the residential conversion of the David Broderick Tower in downtown, the rehabilitation of the Book Cadillac Hotel (now a Westin and luxury condos) and Fort Shelby Hotel (now Doubletree) also in downtown, and various smaller projects.\nDowntown's population of young professionals is growing, and retail is expanding. A study in 2007 found out that Downtown's new residents are predominantly young professionals (57% are ages 25 to 34, 45% have bachelor's degrees, and 34% have a master's or professional degree), a trend which has hastened over the last decade. Since 2006, $9\u00a0billion has been invested in downtown and surrounding neighborhoods; $5.2\u00a0billion of which has come in 2013 and 2014. Construction activity, particularly rehabilitation of historic downtown buildings, has increased markedly. As of 2014, the number of vacant downtown buildings has dropped from nearly 50 to around 13.\nIn 2013 Meijer, a midwestern retail chain, opened its first supercenter store in Detroit; this was a $20\u00a0million, 190,000-square-foot store in the northern portion of the city and it also is the centerpiece of a new $72\u00a0million shopping center named Gateway Marketplace. In 2015 Meijer opened its second supercenter store in the city. In 2019 JPMorgan Chase announced plans to invest $50\u00a0million more in affordable housing, job training, and entrepreneurship by the end of 2022, growing its investment to $200\u00a0million.\nArts and culture.\nIn the central portions of Detroit, the population of young professionals, artists, and other transplants is growing and retail is expanding. This dynamic is luring additional new residents, and former residents returning from other cities, to the city's Downtown along with the revitalized Midtown and New Center areas.\nA desire to be closer to the urban scene has attracted some young professionals to reside in inner ring suburbs such as Ferndale and Royal Oak. The proximity to Windsor provides for views and nightlife, along with Ontario's minimum drinking age of 19. A 2011 study by Walk Score recognized Detroit for its above average walkability among large U.S. cities. About two-thirds of suburban residents occasionally dine and attend cultural events or take in professional games in the city.\nNicknames.\nKnown as the world's automotive center, \"Detroit\" is a metonym for that industry. It is an important source of popular music legacies celebrated by the city's two familiar nicknames, the \"Motor City\" and \"Motown\". Other nicknames arose in the 20th century, including \"City of Champions\", beginning in the 1930s for its successes in individual and team sport; \"The D\"; \"Hockeytown\" (a trademark owned by the Detroit Red Wings); \"Rock City\" (after the Kiss song \"Detroit Rock City\"); and \"The 313\" (its telephone area code).\nMusic.\nLive music has been a prominent feature of Detroit's nightlife since the late 1940s, bringing the city recognition under the nickname \"Motown\". The metropolitan area has many nationally prominent live music venues. Concerts hosted by Live Nation perform throughout the Detroit area. The theater venue circuit is the United States' second largest and hosts Broadway performances.\nThe city has a rich musical heritage and has contributed to many genres over the decades. Important music events include the Detroit International Jazz Festival, the Detroit Electronic Music Festival, the Motor City Music Conference (MC2), the Urban Organic Music Conference, the Concert of Colors, and the hip-hop Summer Jamz festival.\nIn the 1940s, Detroit blues artist John Lee Hooker became a long-term resident in the Delray neighborhood. Hooker, among other important blues musicians, migrated from his home in Mississippi, bringing the Delta blues to Detroit. Hooker recorded for Fortune Records, the biggest pre-Motown blues/soul label. During the 1950s, the city became a center for jazz, with stars performing in the Black Bottom neighborhood. Prominent emerging jazz musicians included trumpeter Donald Byrd (who attended Cass Tech and performed with Art Blakey and the Jazz Messengers early in his career) and saxophonist Pepper Adams (who enjoyed a solo career and accompanied Byrd on several albums). The Graystone International Jazz Museum documents jazz in Detroit.\nOther prominent Motor City R&amp;B stars in the 1950s and early 1960s were Nolan Strong, Andre Williams, and Nathaniel Mayer\u2014who all scored local and national hits on the Fortune Records label. According to Smokey Robinson, Strong was a primary influence on his voice as a teenager. The Fortune label, a family-operated label on Third Avenue, was owned by the husband-and-wife team of Jack Brown and Devora Brown. Fortune\u2014which also released country, gospel and rockabilly LPs and 45s\u2014laid the groundwork for Motown, which became Detroit's most legendary record label.\nBerry Gordy, Jr. founded Motown Records, which rose to prominence during the 1960s and early 1970s with acts such as Stevie Wonder, the Temptations, the Four Tops, Smokey Robinson &amp; the Miracles, Diana Ross &amp; the Supremes, the Jackson 5, Martha and the Vandellas, the Spinners, Gladys Knight &amp; the Pips, the Marvelettes, the Elgins, the Monitors, the Velvelettes, and Marvin Gaye. Artists were backed by in-house vocalists the Andantes and the Funk Brothers.\n\"The Motown sound\" played an important role in the crossover appeal with popular music, since it was the first African American\u2013owned record label to primarily feature African-American artists. Gordy moved Motown to Los Angeles in 1972 to pursue film production, but the company has since returned to Detroit. Aretha Franklin, another Detroit R&amp;B star, carried the Motown sound; however, she did not record with Berry's Motown label.\nLocal artists and bands rose to prominence in the 1960s and 70s, including the MC5, Glenn Frey, the Stooges, Bob Seger, Amboy Dukes featuring Ted Nugent, Mitch Ryder and The Detroit Wheels, Rare Earth, Alice Cooper, and Suzi Quatro. The group Kiss emphasized the city's connection with rock in the song \"Detroit Rock City\" and the movie produced in 1999. In the 1980s, Detroit was an important center of the hardcore punk rock underground with many nationally known bands coming out of the city and its suburbs, such as the Necros, the Meatmen, and Negative Approach.\nIn the 1990s and 2000s, the city produced many influential hip hop artists, including Eminem, the hip-hop artist with the highest cumulative sales, his rap group D12, hip-hop rapper and producer Royce da 5'9\", hip-hop producer Denaun Porter, hip-hop producer J Dilla, rapper and musician Kid Rock and rappers Big Sean and Danny Brown. The band Sponge toured and produced music. The city also has an active garage rock scene that has generated national attention with acts such as the White Stripes, the Von Bondies, the Detroit Cobras, the Dirtbombs, Electric Six, and the Hard Lessons. Detroit is cited as the birthplace of techno music in the early 1980s. The city also lends its name to an early and pioneering genre of electronic dance music, \"Detroit techno\". Featuring science fiction imagery and robotic themes, its futuristic style was greatly influenced by the geography of Detroit's urban decline and its industrial past. Prominent Detroit techno artists include Juan Atkins, Derrick May, Kevin Saunderson, and Jeff Mills. The Detroit Electronic Music Festival, now known as Movement, occurs annually in late May on Memorial Day Weekend, and takes place in Hart Plaza.\nPerforming arts.\nMajor theaters in Detroit include the Fox Theatre (5,174 seats), Music Hall Center for the Performing Arts (1,770 seats), the Gem Theatre (451 seats), Masonic Temple Theatre (4,404 seats), the Detroit Opera House (2,765 seats), the Fisher Theatre (2,089 seats), The Fillmore Detroit (2,200 seats), Saint Andrew's Hall, the Majestic Theater, and Orchestra Hall (2,286 seats), which hosts the renowned Detroit Symphony Orchestra. The Nederlander Organization, the largest controller of Broadway productions in New York City, originated with the purchase of the Detroit Opera House in 1922 by the Nederlander family.\nMotown Motion Picture Studios with produces movies in Detroit and the surrounding area based at the Pontiac Centerpoint Business Campus for a film industry expected to employ over 4,000 people in the metro area.\nTourism.\nDetroit is home to the world's first destination marketing organization, the Detroit Metro Convention and Visitor's Bureau, also known as Visit Detroit. Founded in 1896, the organization now operates at 211 West Fort Street as Visit Detroit.\nBecause of its unique culture, distinctive architecture, and revitalization and urban renewal efforts in the 21st century, Detroit has enjoyed increased prominence as a tourist destination in recent years. \"The New York Times\" listed Detroit as the ninth-best destination in its list of \"52 Places to Go in 2017\", while travel guide publisher \"Lonely Planet\" named Detroit the second-best city in the world to visit in 2018.\"Time\" named Detroit as one of the 50 World's Greatest Places of 2022 to explore.\nMany of the area's prominent museums are in the historic cultural center neighborhood around Wayne State University and the College for Creative Studies. These museums include the Detroit Institute of Arts, the Detroit Historical Museum, Charles H. Wright Museum of African American History, the Detroit Science Center, as well as the main branch of the Detroit Public Library. Other cultural highlights include Motown Historical Museum, the Ford Piquette Avenue Plant museum, the Pewabic Pottery studio and school, the Tuskegee Airmen Museum, Fort Wayne, the Dossin Great Lakes Museum, the Museum of Contemporary Art Detroit, the Contemporary Art Institute of Detroit, and the Belle Isle Conservatory.\nIn 2010, the G.R. N'Namdi Gallery opened in a complex in Midtown. Important history of America and the Detroit area are exhibited at The Henry Ford in Dearborn, the United States' largest indoor-outdoor museum complex. The Detroit Historical Society provides information about tours of area churches, skyscrapers, and mansions. Inside Detroit hosts tours, educational programming, and a downtown welcome center. Other sites of interest are the Detroit Zoo in Royal Oak, the Cranbrook Art Museum in Bloomfield Hills, the Anna Scripps Whitcomb Conservatory on Belle Isle, and Walter P. Chrysler Museum in Auburn Hills.\nGreektown and three downtown casino resort hotels serve as part of an entertainment hub. The Eastern Market farmer's distribution center is the largest open-air flowerbed market in the United States and has more than 150 foods and specialty businesses. On Saturdays, about 45,000 people shop there. The annual Detroit Festival of the Arts in Midtown draws about 350,000 people.\nAnnual summer events include the Electronic Music Festival, International Jazz Festival, the Woodward Dream Cruise, the African World Festival, the country music Hoedown, Noel Night, and Dally in the Alley. Within downtown, Campus Martius Park hosts large events, including the annual Motown Winter Blast. As the world's traditional automotive center, the city hosts the North American International Auto Show. Held since 1924, America's Thanksgiving Parade is one of the nation's largest. River Days, a five-day summer festival on the International Riverfront lead up to the Windsor\u2013Detroit International Freedom Festival fireworks, which draw super sized-crowds ranging from hundreds of thousands to over three million people.\nAn important civic sculpture is \"The Spirit of Detroit\" by Marshall Fredericks at the Coleman Young Municipal Center. The image is often used as a symbol of Detroit, and the statue is occasionally dressed in sports jerseys to celebrate when a Detroit team is doing well. A memorial to Joe Louis is located at the intersection of Jefferson and Woodward Avenues. The sculpture, commissioned by \"Sports Illustrated\" and executed by Robert Graham, is a long arm with a fisted hand suspended by a pyramidal framework.\nSports.\nDetroit is one of four U.S. cities that have venues within the city representing the four major sports in North America. Detroit is the only city to have its four major sports teams play within its downtown district. Venues include: Comerica Park (home of MLB's Detroit Tigers), Ford Field (home of the NFL's Detroit Lions), and Little Caesars Arena (home of the NHL's Detroit Red Wings and the NBA's Detroit Pistons).\nDetroit has won titles in all four of the major professional sports leagues. The Tigers have won four World Series titles (1935, 1945, 1968, and 1984). The Red Wings have won 11 Stanley Cups (1935\u201336, 1936\u201337, 1942\u201343, 1949\u201350, 1951\u201352, 1953\u201354, 1954\u201355, 1996\u201397, 1997\u201398, 2001\u201302, 2007\u201308) (the most by an American NHL franchise). The Lions have won 4 NFL titles (1935, 1952, 1953, 1957). The Pistons have won three NBA titles (1989, 1990, 2004). In the years following the mid-1930s, Detroit was referred to as the \"City of Champions\" after the Tigers, Lions, and Red Wings captured the three major professional sports championships in existence at the time in a seven-month period (the Tigers won the World Series in October 1935; the Lions won the NFL championship in December 1935; the Red Wings won the Stanley Cup in April 1936).\nFounded in 2012 as a semi-professional soccer club, Detroit City FC plays professional soccer in the USL Championship. Nicknamed, \"Le Rouge\", the club are two-time champions of NISA since joining in 2020. They play their home matches in Keyworth Stadium, which is located in the enclave of Hamtramck.\nIn college sports, Detroit's central location within the Mid-American Conference (MAC) has made it a frequent site for the league's championship events. While the MAC Basketball Tournament moved permanently to Cleveland starting in 2000, the MAC Football Championship Game has been played at Ford Field since 2004 and annually attracts 25,000 to 30,000 fans. The University of Detroit Mercy has an NCAA Division I program, and Wayne State University has both NCAA Division I and II programs. The NCAA football GameAbove Sports Bowl (formerly, Quick Lane Bowl) is held at Ford Field each December.\nThe city hosted the 2005 MLB All-Star Game, Super Bowl XL in 2006, the 2006 and 2012 World Series, WrestleMania 23 in 2007, and the NCAA Final Four in April 2009. The Detroit Indy Grand Prix is held in Belle Isle Park. In 2007, open-wheel racing returned to Belle Isle with both Indy Racing League and American Le Mans Series Racing. From 1982 to 1988, Detroit held the Detroit Grand Prix, at the Detroit street circuit.\nIn 1932, Eddie \"The Midnight Express\" Tolan from Detroit won the 100- and 200-meter races and two gold medals at the 1932 Summer Olympics. Joe Louis won the heavyweight championship of the world in 1937. Detroit has made the most bids to host the Summer Olympics without ever being awarded the games, with seven unsuccessful bids for the 1944, 1952, 1956, 1960, 1964, 1968, and 1972 summer games.\nIn 2024, Detroit hosted the NFL Draft. Over 775,000 people were present in downtown Detroit over the course of the three-day event, making it the highest attended draft on record.\nGovernment.\nThe city is governed pursuant to the home rule \"Charter of the City of Detroit\". The government is run by a mayor, the nine-member Detroit City Council, the eleven-member Board of Police Commissioners, and a clerk. All of these officers are elected on a nonpartisan ballot, with the exception of four of the police commissioners, who are appointed by the mayor. Detroit has a \"strong mayoral\" system, with the mayor approving departmental appointments. The council approves budgets, but the mayor is not obligated to adhere to any earmarking. The city clerk supervises elections and is formally charged with the maintenance of municipal records. City ordinances and substantially large contracts must be approved by the council. The \"Detroit City Code\" is the codification of Detroit's local ordinances.\nPresently three Community Advisory Councils advise City Council representatives. Residents of each of Detroit's seven districts have the option of electing Community Advisory Councils. The city clerk supervises elections and is formally charged with the maintenance of municipal records. Municipal elections for mayor, city council and city clerk are held at four-year intervals, in the year after presidential elections. Following a November 2009 referendum, seven council members will be elected from districts beginning in 2013 while two will continue to be elected at-large.\nDetroit's courts are state-administered and elections are nonpartisan. The Probate Court for Wayne County is in the Coleman A. Young Municipal Center in downtown. The Circuit Court is across Gratiot Avenue in the Frank Murphy Hall of Justice. The city is home to the Thirty-Sixth District Court, as well as the First District of the Michigan Court of Appeals and the United States District Court for the Eastern District of Michigan. The city provides law enforcement through the Detroit Police Department and emergency services through the Detroit Fire Department.\nPolitics.\nBeginning with its incorporation in 1802, Detroit has had a total of 74 mayors. Detroit's last mayor from the Republican Party was Louis Miriani, who served from 1957 to 1962. In 1973, the city elected its first black mayor, Coleman Young. Despite development efforts, his combative style during his five terms in office was not well received by many suburban residents. Mayor Dennis Archer, a former Michigan Supreme Court Justice, refocused the city's attention on redevelopment with a plan to permit three casinos downtown. By 2008, three major casino resort hotels established operations in the city.\nIn 2000, the city requested an investigation by the United States Justice Department into the Detroit Police Department which was concluded in 2003 over allegations regarding its use of force and civil rights violations. The city proceeded with a major reorganization of the Detroit Police Department. In 2013, felony bribery charges were brought against seven building inspectors. In 2016, further corruption charges were brought against 12 principals, a former school superintendent and supply vendor for a $12\u00a0million (~$ in ) kickback scheme. However, law professor Peter Henning argues Detroit's corruption is not unusual for a city its size, especially when compared with Chicago.\nDetroit is sometimes referred to as a sanctuary city because it has \"anti-profiling ordinances that generally prohibit local police from asking about the immigration status of people who are not suspected of any crime\". The city in recent years has been a stronghold for the Democratic Party, with around 94% of votes in the city going to Joe Biden, the Democratic candidate in the 2020 Presidential election.\nEducation.\nColleges and universities.\nDetroit is home to several institutions of higher learning, including Wayne State University and the University of Detroit Mercy. Grand Valley State University's Detroit Center hosts workshops, seminars, professional development, and other large gatherings. Sacred Heart Major Seminary, founded in 1919, is affiliated with Pontifical University of Saint Thomas Aquinas, \"Angelicum\" in Rome and offers pontifical degrees as well as civil undergraduate and graduate degrees. Other institutions in the city include the College for Creative Studies and Wayne County Community College. In June 2009, the Michigan State University College of Osteopathic Medicine which is based in East Lansing opened a satellite campus at the Detroit Medical Center.\nPrimary and secondary schools.\n many K-12 students in Detroit frequently change schools, with some children having been enrolled in seven schools before finishing their K-12 careers. There is a concentration of senior high schools and charter schools in the downtown area, which had wealthier residents and more gentrification relative to other parts of Detroit: Downtown, northwest Detroit, and northeast Detroit have 1,894, 3,742, and 6,018 students of high school age, respectively, while they have 11, three, and two high schools, respectively. because of the lack of public transportation and the lack of school bus services, many Detroit families have to rely on themselves to transport children to school.\nWith about 66,000 public school students (2011\u201312), the Detroit Public Schools (DPS) district is the largest school district in Michigan. Detroit has an additional 56,000 charter school students for a combined enrollment of about 122,000 students. there are about as many students in charter schools as there are in district schools. DPS continues to have the majority of the special education pupils. In addition, some Detroit students, as of 2016, attend public schools in other municipalities.\nWith growing charter schools enrollment as well as a continued exodus of population, the city planned to close many public schools. State officials report a 68% graduation rate for Detroit's public schools adjusted for those who change schools. Traditional public and charter school students in the city have performed poorly on standardized tests. and 2011, while Detroit traditional public schools scored a record low on national tests, the publicly funded charter schools did even worse than the traditional public schools. there were 30,000 excess openings in Detroit traditional public and charter schools, bearing in mind the number of K-12-aged children in the city. In 2016, Kate Zernike of \"The New York Times\" stated school performance did not improve despite the proliferation of charters, describing the situation as \"lots of choice, with no good choice\".\nDetroit public schools students scored the lowest on tests of reading and writing of all major cities in the United States in 2015. Among eighth-graders, only 27% showed basic proficiency in math and 44% in reading. Nearly half of Detroit's adults are functionally illiterate.\nDetroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side. The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs. Of the three Catholic high schools, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.\nMedia.\nThe \"Detroit Free Press\" and \"The Detroit News\" are the major daily newspapers, both broadsheet publications published together under a joint operating agreement called the Detroit Media Partnership. Media philanthropy includes the \"Detroit Free Press\" high school journalism program and the Old Newsboys' Goodfellow Fund of Detroit. In March 2009, the two newspapers reduced home delivery to three days per week, print reduced newsstand issues of the papers on non-delivery days and focus resources on Internet-based news delivery. The \"Metro Times\", founded in 1980, is a weekly publication, covering news, arts &amp; entertainment.\nFounded in 1935 and based in Detroit, the \"Michigan Chronicle\" is one of the oldest and most respected African-American weekly newspapers in America, covering politics, entertainment, sports and community events. The Detroit television market is the 11th largest in the United States; according to estimates that do not include audiences in large areas of Ontario (Windsor and its surrounding area on broadcast and cable TV, as well as several other cable markets in Ontario, such as Ottawa) which receive and watch Detroit television stations.\nDetroit has the 11th largest radio market in the United States, though this ranking does not take into account Canadian audiences. Nearby Canadian stations such as Windsor's CKLW (whose jingles formerly proclaimed \"CKLW-the Motor City\") are popular in Detroit.\nInfrastructure.\nHealth systems.\nThere are over a dozen major hospitals, which include the Detroit Medical Center (DMC), Henry Ford Health System, St. John Health System, and the John D. Dingell VA Medical Center. DMC, a regional Level I trauma center, consists of Detroit Receiving Hospital and University Health Center, Children's Hospital of Michigan, Harper University Hospital, Hutzel Women's Hospital, Kresge Eye Institute, Rehabilitation Institute of Michigan, Sinai-Grace Hospital, and the Karmanos Cancer Institute. DMC has more than 2,000 licensed beds and 3,000 affiliated physicians. It is the largest private employer in the city. The center is staffed by physicians from the Wayne State University School of Medicine, the largest single-campus medical school in the United States and the fourth largest medical school overall.\nDMC formally became a part of Vanguard Health Systems on December 30, 2010, as a for-profit corporation. Vanguard has agreed to invest nearly $1.5 B in the DMC complex. Vanguard has agreed to assume all debts and pension obligations. The metro area has many other hospitals including William Beaumont Hospital, St. Joseph's, and University of Michigan Medical Center.\nIn 2011, DMC and Henry Ford Health System substantially increased investments in medical research facilities and hospitals in the city's Midtown and New Center. In 2012, two major construction projects were begun in New Center. The Henry Ford Health System started the first phase of a $500\u00a0million, 300-acre revitalization project, with the construction of a new $30\u00a0million, 275,000-square-foot, \"Medical Distribution Center\" for Cardinal Health, Inc. and Wayne State University started construction on a new $93\u00a0million, 207,000-square-foot, Integrative Biosciences Center (IBio). As many as 500 researchers and staff will work out of the IBio Center.\nTransportation.\nWith its proximity to Canada and its facilities, ports, major highways, rail connections and international airports, Detroit is an important transportation hub. The city has three international border crossings, the Ambassador Bridge, Detroit\u2013Windsor Tunnel and Michigan Central Railway Tunnel, linking Detroit to Windsor. The Ambassador Bridge is the single busiest border crossing in North America, carrying 27% of the total trade between the U.S. and Canada.\nIn 2015 Canadian Transport Minister Lisa Raitt announced Canada agreed to pay the entire cost to build a $250\u00a0million U.S. Customs plaza adjacent to the planned new Detroit\u2013Windsor bridge, now the Gordie Howe International Bridge. Canada had already planned to pay for 95% of the bridge, which will cost $2.1\u00a0billion and is expected to open in 2024. \"This allows Canada and Michigan to move the project forward immediately to its next steps which include further design work and property acquisition on the U.S. side of the border\", Raitt said in a statement issued after she spoke in the House of Commons.\nTransit systems.\nMass transit in the region is provided by bus services. The Detroit Department of Transportation provides service within city limits up to the outer edges of the city. From there, the Suburban Mobility Authority for Regional Transportation (SMART) provides service to the suburbs and the city regionally with local routes and SMART's FAST service. FAST is a new service provided by SMART which offers limited stops along major corridors throughout the Detroit metropolitan area connecting the suburbs to downtown. The new high-frequency service travels along three of Detroit's busiest corridors, Gratiot, Woodward, and Michigan, and only stops at designated FAST stops. Cross border service between the downtown areas of Windsor and Detroit is provided by Transit Windsor via the Tunnel Bus.\nAn elevated rail system known as the People Mover, completed in 1987, provides daily service around a loop downtown. The QLINE serves as a link between the People Mover and the Amtrak station via Woodward Avenue. The Ann Arbor\u2013Detroit Regional Rail line will extend from New Center, connecting to Ann Arbor via Dearborn, Wayne, and Ypsilanti when it is opened.\nThe Regional Transit Authority (RTA) was established by an act of the Michigan legislature in 2012 to oversee and coordinate all existing regional mass transit operations, and to develop new transit services in the region. The RTA's first project was the introduction of RelfeX, a limited-stop, cross-county bus service connecting downtown and midtown Detroit with Oakland county via Woodward avenue.\nAmtrak provides service to Detroit, operating its \"Wolverine\" service between Chicago and Pontiac. The Amtrak station is in New Center north of downtown. Intercity bus service is offered at the Detroit Bus Station. Greyhound Lines, Flixbus, Indian Trails, and Barons Bus Lines connect Detroit with numerous cities across the Midwest.\nCar ownership.\nThe city of Detroit has a higher than average percentage of households without a car. In 2016, 24.7% of Detroit households lacked a car, much higher than the national average of 8.7%. Detroit averaged 1.15 cars per household in 2016, compared to a national average of 1.8.\nFreight railroads.\nFreight railroad operations in the city of Detroit are provided by Canadian National Railway, Canadian Pacific Railway, Conrail Shared Assets, CSX Transportation and Norfolk Southern Railway, each of which have local yards within the city. Detroit is also served by the Delray Connecting Railroad and Detroit Connecting Railroad shortlines.\nAirports.\nDetroit Metropolitan Wayne County Airport (DTW), the principal airport serving Detroit, is in nearby Romulus. DTW is a primary hub for Delta Air Lines (following its acquisition of Northwest Airlines), and a secondary hub for Spirit Airlines. The airport is connected to Downtown Detroit by the Suburban Mobility Authority for Regional Transportation (SMART) FAST Michigan route.\nColeman A. Young International Airport (DET), previously called Detroit City Airport, is on Detroit's northeast side; the airport now maintains only charter service and general aviation. Willow Run Airport, in western Wayne County near Ypsilanti, is a general aviation and cargo airport.\nFreeways.\nMetro Detroit has an extensive toll-free network of freeways administered by the Michigan Department of Transportation. Four major Interstate Highways surround the city. Detroit is connected via I-75 and I-96 to Kings Highway 401 and to major Southern Ontario cities such as London, Ontario and the Greater Toronto Area. I-75 (Chrysler and Fisher freeways) is the region's main north\u2013south route, serving Flint, Pontiac, Troy, and Detroit, before continuing south (as the Detroit\u2013Toledo and Seaway Freeways) to serve many of the communities along the shore of Lake Erie.\nI-94 (Edsel Ford Freeway) runs east\u2013west through Detroit and serves Ann Arbor to the west (where it continues to Chicago) and Port Huron to the northeast. The stretch of the I-94 freeway from Ypsilanti to Detroit was one of America's earlier limited-access highways. Henry Ford built it to link the factories at Willow Run and Dearborn during World War II. A portion was known as the Willow Run Expressway. The I-96 freeway runs northwest\u2013southeast through Livingston, Oakland and Wayne counties and (as the Jeffries Freeway through Wayne County) has its eastern terminus in downtown Detroit.\nI-275 runs north\u2013south from I-75 in the south to the junction of I-96 and I-696 in the north, providing a bypass through the western suburbs of Detroit. I-375 is a short spur route in downtown Detroit, an extension of the Chrysler Freeway. I-696 (Reuther Freeway) runs east\u2013west from the junction of I-96 and I-275, providing a route through the northern suburbs of Detroit. Taken together, I-275 and I-696 form a semicircle around Detroit. Michigan state highways designated with the letter M serve to connect major freeways.\nFloating post office.\nDetroit has a floating post office, the \"J. W. Westcott II\", which serves lake freighters along the Detroit River. Its ZIP Code is 48222. The ZIP Code is used exclusively for the \"J. W. Westcott II\", which makes it the only floating ZIP Code in the United States. It has a land-based office at 12 24th Street, just south of the Ambassador Bridge. The J.W. Westcott Company was established in 1874 by Captain John Ward Westcott as a maritime reporting agency to inform other vessels about port conditions, and the \"J. W. Westcott II\" vessel began service in 1949 and is still in operation today.\nSister cities.\nDetroit's sister cities include the following:"}
{"id": "8688", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=8688", "title": "Deccan Traps", "text": "The Deccan Traps are a large igneous province of west-central India (17\u201324\u00b0N, 73\u201374\u00b0E). They are one of the largest volcanic features on Earth, taking the form of a large shield volcano. They consist of many layers of solidified flood basalt that together are more than about thick, cover an area of about , and have a volume of about . Originally, the Deccan Traps may have covered about , with a correspondingly larger original volume. This volume overlies the Archean age Indian Shield, which is likely the lithology the province passed through during eruption. The province is commonly divided into four subprovinces: the main Deccan, the Malwa Plateau, the Mandla Lobe, and the Saurashtran Plateau.\nThe eruptions occurred over a 600\u2013800,000 year time period between around 66.3 to 65.6 million years ago, spanning the Cretaceous\u2013Paleogene boundary. While some authors have suggested that the eruptions were the primary cause of the Cretaceous\u2013Paleogene mass extinction event, which dates to around 66.05 million years ago, this has been strongly disputed, with many authors suggesting that the Chicxulub impact was the primary cause of the extinction. While some scholars suggest that the eruptions may have been a contributing factor in the extinctions, others suggest that the role of the Deccan Traps in the extinction were negligible or even partially negated the effects of the impact.\nThe Deccan Traps are thought to have been produced in major part by the still active R\u00e9union hotspot, responsible for the creation of the modern Mascarene Islands in the Indian Ocean.\nEtymology.\nThe term \"trap\" has been used in geology since 1785\u20131795 for such rock formations. It is derived from the Swedish word for stairs () and refers to the step-like hills forming the landscape of the region. The name \"Deccan\" has Sanskrit origins meaning \"southern\".\nHistory.\nThe Deccan Traps began forming 66.25\u00a0million years ago, at the end of the Cretaceous period, although it is possible that some of the oldest material may underlie younger material. The bulk of the volcanic eruption occurred at the Western Ghats between 66 and 65 million years ago when lava began to extrude in fissure eruptions. Determining the exact age for Deccan rock is difficult due to a number of limitations, one being that the transition between eruption events may have lasted only a few thousand years and the resolution of dating methods is not sufficient to pinpoint these events. In this way, determining the rate of magma emplacement is also difficult to constrain. This series of eruptions may have lasted for less than 30,000\u00a0years.\nThe original area covered by the lava flows is estimated to have been as large as , approximately half the size of modern India. The Deccan Traps region was reduced to its current size by erosion and plate tectonics; the present area of directly observable lava flows is around .\nThe Deccan Traps are segmented into three stratigraphic units: the Upper, Middle, and Lower traps. While it was previously interpreted that these groups represented their own key points in the sequence of events in Deccan extrusion, it is now more widely accepted that these horizons relate more closely to paleotopography and distance from the eruption site.\nEffect on mass extinctions and climate.\nThe release of volcanic gases, particularly sulfur dioxide, during the formation of the traps may have contributed to climate change. An average drop in temperature of about was recorded during this period.\nBecause of its magnitude, some scientists (notably Gerta Keller) have speculated that the gases released during the formation of the Deccan Traps played a major role in the Cretaceous\u2013Paleogene (K\u2013Pg) extinction event (also known as the Cretaceous\u2013Tertiary or K\u2013T extinction). It has been theorized that sudden cooling due to sulfurous volcanic gases released by the formation of the traps and toxic gas emissions may have contributed significantly to the K\u2013Pg mass extinction. However, the current consensus among the scientific community is that the extinction was primarily triggered by the Chicxulub impact event in North America, which would have produced a sunlight-blocking dust cloud that killed much of the plant life and reduced global temperature (this cooling is called an impact winter).\nA 2014 study suggested the extinction may have been caused by both the volcanism and the impact event. This was followed by a similar study in 2015, both of which consider the hypothesis that the impact exacerbated or induced the Deccan volcanism, since the events occurred approximately at antipodes. A 2020 study questioned the idea that the Deccan Traps were a contributory factor at all, suggesting that the Deccan Traps eruptions may have even partially negated the climatic change induced by the impact.\nA major criticism of the Deccan Traps as the primary cause of the extinctions is that the extinction event appears to be globally geologically instantaneous and simultaneous in both marine and terrestrial environments, as would be expected from an impact cause, rather than staggered as would be expected from an LIP cause.\nA more recent discovery appears to demonstrate the scope of the destruction from the impact alone, however. In a March 2019 article in the Proceedings of the National Academy of Sciences, an international team of twelve scientists revealed the contents of the Tanis fossil site discovered near Bowman, North Dakota, that appeared to show a devastating mass destruction of an ancient lake and its inhabitants at the time of the Chicxulub impact. In the paper, the group reports that the geology of the site is strewn with fossilized trees and remains of fish and other animals. The lead researcher, Robert A. DePalma of the University of Kansas, was quoted in the New York Times as stating that \"You would be blind to miss the carcasses sticking out... It is impossible to miss when you see the outcrop\". Evidence correlating this find to the Chicxulub impact included tektites bearing \"the unique chemical signature of other tektites associated with the Chicxulub event\" found in the gills of fish fossils and embedded in amber, an iridium-rich top layer that is considered another signature of the event, and an atypical lack of evidence for scavenging, perhaps suggesting that there were few survivors. The exact mechanism of the site's destruction has been debated as either an impact-caused tsunami or lake and river seiche activity triggered by post-impact earthquakes, though there has yet been no firm conclusion upon which researchers have settled. \nA 2024 study of glycerol dialkyl glycerol tetraether levels in fossilized peat found that the Deccan Traps caused long-term warming of around 3\u00b0C over the course of the final 100,000 years of the Maastrichtian, as well as about 5\u00b0C drop in temperature for less than 10,000 years around 30,000 years prior to the K-Pg boundary (coinciding with the peak of the Poladpur eruptive phase), but by the time of the K-Pg boundary, global temperatures had returned to previous levels. This suggests that the Deccan Traps were not the primary cause of extinction.\nPetrology.\nWithin the Deccan Traps, at least 95% of the lavas are tholeiitic basalts. Major mineral constituents are olivine, pyroxenes, and plagioclase, as well as certain Fe-Ti-rich oxides. These magmas are &lt;7% MgO. However, many of these minerals are observed as highly altered forms. Other rock types present include alkali basalt, nephelinite, lamprophyre, and carbonatite.\nMantle xenoliths have been described from Kachchh (northwestern India) and elsewhere in the western Deccan and contain spinel lherzolite and pyroxenite constituents.\nWhile the Deccan traps have been categorized in many different ways including the three different stratigraphic groups, geochemically the province can be split into as many as eleven different formations. Many of the petrologic differences in these units are a product of varying degrees of crustal contamination.\nFossils.\nThe Deccan Traps are famous for the beds of fossils that have been found between layers of lava. Particularly well-known species include the frog \"Oxyglossus pusillus\" (Owen) of the Eocene of India and the toothed frog \"Indobatrachus\", an early lineage of modern frogs, which is now placed in the Australian family Myobatrachidae. The Infratrappean Beds (Lameta Formation) and Intertrappean Beds also contain fossil freshwater molluscs.\nTheories of formation.\nIt is postulated that the Deccan Traps eruption was associated with a deep mantle plume. High 3He/4He ratios of the main pulse of the eruption are often seen in magmas with mantle plume origin. The area of long-term eruption (the hotspot), known as the R\u00e9union hotspot, is suspected of both causing the Deccan Traps eruption and opening the rift that separated the Mascarene Plateau from India. Regional crustal thinning supports the theory of this rifting event and likely encouraged the rise of the plume in this area. Seafloor spreading at the boundary between the Indian and African Plates subsequently pushed India north over the plume, which now lies under R\u00e9union island in the Indian Ocean, southwest of India. The mantle plume model has, however, been challenged.\nData continues to emerge that supports the plume model. The motion of the Indian tectonic plate and the eruptive history of the Deccan traps show strong correlations. Based on data from marine magnetic profiles, a pulse of unusually rapid plate motion began at the same time as the first pulse of Deccan flood basalts, which is dated at 67\u00a0million years ago. The spreading rate rapidly increased and reached a maximum at the same time as the peak basaltic eruptions. The spreading rate then dropped off, with the decrease occurring around 63\u00a0million years ago, by which time the main phase of Deccan volcanism ended. This correlation is seen as driven by plume dynamics.\nThe motions of the Indian and African plates have also been shown to be coupled, the common element being the position of these plates relative to the location of the R\u00e9union plume head. The onset of accelerated motion of India coincides with a large slowing of the rate of counterclockwise rotation of Africa. The close correlations between the plate motions suggest that they were both driven by the force of the R\u00e9union plume.\nWhen comparing the Na8, Fe8, and Si8 contents of the Deccan to other major igneous provinces, the Deccan appears to have undergone the greatest degree of melting suggesting a deep plume origin. Olivine appears to have fractionated at near-Moho depths with additional fractionation of gabbro ~6\u00a0km below the surface. Features such as widespread faulting, frequent diking events, high heat flux, and positive gravity anomalies suggest that the extrusive phase of the Deccan Traps is associated with the existence of a triple junction which may have existed during the Late Cretaceous, having been caused by a deep mantle plume. Not all of these diking events are attributed to large-scale contributions to the overall flow volume. It can be difficult, however, to locate the largest dikes as they are often located towards the west coast and are therefore believed to currently reside under water.\nSuggested link to impact events.\nChicxulub crater.\nAlthough the Deccan Traps began erupting well before the impact, in a 2015 study it was proposed based on argon\u2013argon dating that the impact may have caused an increase in permeability that allowed magma to reach the surface and produced the most voluminous flows, accounting for around 70% of the volume. The combination of the asteroid impact and the resulting increase in eruptive volume may have been responsible for the mass extinctions that occurred at the time that separates the Cretaceous and Paleogene periods, known as the K\u2013Pg boundary. However this proposal has been questioned by other authors, who describe the suggestion as being \"convenient interpretations based on superficial and cursory observations.\"\nShiva crater.\nA geological structure that exists in the sea floor off the west coast of India has been suggested as a possible impact crater, in this context called the Shiva crater. It was also dated approximately 66\u00a0million years ago, potentially matching the Deccan traps. The researchers claiming that this feature is an impact crater suggest that the impact may have been the triggering event for the Deccan Traps as well as contributing to the acceleration of the Indian plate in the early Paleogene. However, the current consensus in the Earth science community is that this feature is unlikely to be an actual impact crater."}
{"id": "8690", "revid": "9036255", "url": "https://en.wikipedia.org/wiki?curid=8690", "title": "Don't ask, don't tell", "text": "\"Don't ask, don't tell\" (DADT) was the official United States policy on military service of homosexual people. Instituted during the Clinton administration, the policy was issued under Department of Defense Directive 1304.26 on December 21, 1993, and was in effect from February 28, 1994, until September 20, 2011. The policy prohibited military personnel from discriminating against or harassing closeted homosexual or bisexual service members or applicants, while barring openly gay, lesbian, or bisexual persons from military service. This relaxation of legal restrictions on service by gays and lesbians in the armed forces was mandated by Public Law 103\u2013160 (Title 10 of the United States Code \u00a7654), which was signed November 30, 1993. The policy prohibited people who \"demonstrate a propensity or intent to engage in homosexual acts\" from serving in the armed forces of the United States, because their presence \"would create an unacceptable risk to the high standards of morale, good order and discipline, and unit cohesion that are the essence of military capability\".\nThe act prohibited any non-heterosexual person from disclosing their sexual orientation or from speaking about any same-sex relationships, including marriages or other familial attributes, while serving in the United States armed forces. The act specified that service members who disclose that they are homosexual or engage in homosexual conduct should be separated (discharged) except when a service member's conduct was \"for the purpose of avoiding or terminating military service\" or when it \"would not be in the best interest of the armed forces\". Since DADT ended in 2011, persons who are openly homosexual and bisexual have been able to serve.\nThe \"don't ask\" section of the DADT policy specified that superiors should not initiate an investigation of a service member's orientation without witnessing disallowed behaviors. However, evidence of homosexual behavior deemed credible could be used to initiate an investigation. Unauthorized investigations and harassment of suspected servicemen and women led to an expansion of the policy to \"don't ask, don't tell, don't pursue, don't harass\".\nBeginning in the early 2000s, several legal challenges to DADT were filed, and legislation to repeal DADT was enacted in December 2010, specifying that the policy would remain in place until the President, the Secretary of Defense, and the Chairman of the Joint Chiefs of Staff certified that repeal would not harm military readiness, followed by a 60-day waiting period. A July 6, 2011, ruling from a federal appeals court barred further enforcement of the U.S. military's ban on openly gay service members. President Barack Obama, Secretary of Defense Leon Panetta, and Chairman of the Joint Chiefs of Staff Admiral Mike Mullen sent that certification to Congress on July 22, 2011, which set the end of DADT to September 20, 2011.\nEven with DADT repealed, the legal definition of marriage as being one man and one woman under the Defense of Marriage Act (DOMA) meant that, although same-sex partners could get married, their marriage was not recognized by the federal government. This barred partners from access to the same benefits afforded to heterosexual couples such as base access, health care, and United States military pay, including family separation allowance and Basic Allowance for Housing with dependents. The Department of Defense attempted to allow some of the benefits that were not restricted by DOMA, but the Supreme Court decision in \"United States v. Windsor\" (2013) made these efforts unnecessary.\nBackground.\nEngaging in homosexual activity had been grounds for discharge from the American military since the Revolutionary War. Policies based on sexual orientation appeared as the United States prepared to enter World War II. When the military added psychiatric screening to its induction process, it included homosexuality as a disqualifying trait, then seen as a form of psychopathology. When the army issued revised mobilization regulations in 1942, it distinguished \"homosexual\" recruits from \"normal\" recruits for the first time. Before the buildup to the war, gay service members were court-martialed, imprisoned, and dishonorably discharged; but in wartime, commanding officers found it difficult to convene court-martial boards of commissioned officers and the administrative blue discharge became the military's standard method for handling gay and lesbian personnel. In 1944, a new policy directive decreed that homosexuals were to be committed to military hospitals, examined by psychiatrists, and discharged under Regulation 615\u2013360, section 8.\nIn 1947, blue discharges were discontinued and two new classifications were created: \"general\" and \"undesirable\". Under such a system, a serviceman or woman found to be gay but who had not committed any sexual acts while in service would tend to receive an undesirable discharge. Those found guilty of engaging in sexual conduct were usually dishonorably discharged. A 1957 U.S. Navy study known as the Crittenden Report dismissed the charge that homosexuals constitute a security risk, but nonetheless did not advocate for an end to anti-gay discrimination in the navy on the basis that \"The service should not move ahead of civilian society nor attempt to set substantially different standards in attitude or action with respect to homosexual offenders.\" It remained secret until 1976. Fannie Mae Clackum was the first service member to successfully appeal such a discharge, winning eight years of back pay from the US Court of Claims in 1960.\nFrom the 1950s through the Vietnam War, some notable gay service members avoided discharges despite pre-screening efforts, and when personnel shortages occurred, homosexuals were allowed to serve.\nThe gay and lesbian rights movement in the 1970s and 1980s raised the issue by publicizing several noteworthy dismissals of gay service members. Air Force TSgt Leonard Matlovich, the first service member to purposely out himself to challenge the ban, appeared on the cover of \"Time\" in 1975. In 1982 the Department of Defense issued a policy stating that, \"Homosexuality is incompatible with military service.\" It cited the military's need \"to maintain discipline, good order, and morale\" and \"to prevent breaches of security\". In 1988, in response to a campaign against lesbians at the Marines' Parris Island Depot, activists launched the Gay and Lesbian Military Freedom Project (MFP) to advocate for an end to the exclusion of gays and lesbians from the armed forces. In 1989, reports commissioned by the Personnel Security Research and Education Center (PERSEREC), an arm of the Pentagon, were discovered in the process of Joseph Steffan's lawsuit fighting his forced resignation from the U.S. Naval Academy. One report said that \"having a same-gender or an opposite-gender orientation is unrelated to job performance in the same way as is being left- or right-handed.\" Other lawsuits fighting discharges highlighted the service record of service members like Tracy Thorne and Margarethe (Grethe) Cammermeyer. The MFP began lobbying Congress in 1990, and in 1991 Senator Brock Adams (D-Washington) and Rep. Barbara Boxer introduced the Military Freedom Act, legislation to end the ban completely. Adams and Rep. Pat Schroeder (D-Colorado) re-introduced it the next year. In July 1991, Secretary of Defense Dick Cheney, in the context of the outing of his press aide Pete Williams, dismissed the idea that gays posed a security risk as \"a bit of an old chestnut\" in testimony before the House Budget Committee. In response to his comment, several major newspapers endorsed ending the ban, including \"USA Today\", the \"Los Angeles Times\", and the \"Detroit Free Press\". In June 1992, the General Accounting Office released a report that members of Congress had requested two years earlier estimating the costs associated with the ban on gays and lesbians in the military at $27\u00a0million annually.\nDuring the 1992 U.S. presidential election campaign, the civil rights of gays and lesbians, particularly their open service in the military, attracted some press attention, and all candidates for the Democratic presidential nomination supported ending the ban on military service by gays and lesbians, but the Republicans did not make a political issue of that position. In an August cover letter to all his senior officers, General Carl Mundy Jr., Commandant of the Marine Corps, praised a position paper authored by a Marine Corps chaplain that said that \"In the unique, intensely close environment of the military, homosexual conduct can threaten the lives, including the physical (e.g. AIDS) and psychological well-being of others\". Mundy called it \"extremely insightful\" and said it offered \"a sound basis for discussion of the issue\". The murder of gay U.S. Navy petty officer Allen R. Schindler Jr. on October 27, 1992 brought calls from advocates for allowing open service by gays and lesbians in the US military, and requested prompt action from the incoming Clinton administration.\nOrigin.\nThe policy was introduced as a compromise measure in 1993 by President Bill Clinton who campaigned in 1992 on the promise to allow all citizens to serve in the military regardless of sexual orientation. Commander Craig Quigley, a Navy spokesman, expressed the opposition of many in the military at the time when he said, \"Homosexuals are notoriously promiscuous\" and that in shared shower situations, heterosexuals would have an \"uncomfortable feeling of someone watching\".\nDuring the 1993 policy debate, the National Defense Research Institute prepared a study for the Office of the Secretary of Defense published as \"Sexual Orientation and U.S. Military Personnel Policy: Options and Assessment\". It concluded that \"circumstances could exist under which the ban on homosexuals could be lifted with little or no adverse consequences for recruitment and retention\" if the policy were implemented with care, principally because many factors contribute to individual enlistment and re-enlistment decisions. On May 5, 1993, Gregory M. Herek, associate research psychologist at the University of California at Davis and an authority on public attitudes toward lesbians and gay men, testified before the House Armed Services Committee on behalf of several professional associations. He stated, \"The research data show that there is nothing about lesbians and gay men that makes them inherently unfit for military service, and there is nothing about heterosexuals that makes them inherently unable to work and live with gay people in close quarters.\" Herek added, \"The assumption that heterosexuals cannot overcome their prejudices toward gay people is a mistaken one.\"\nIn Congress, Democratic Senator Sam Nunn of Georgia and Chair of the Senate Armed Services Committee led the contingent that favored maintaining the absolute ban on gays. Reformers were led by Democratic Congressman Barney Frank of Massachusetts, who favored modification (but ultimately voted for the defense authorization bill with the gay ban language), and 1964 Republican presidential nominee Barry Goldwater, a former Senator and a retired Major General, who argued on behalf of allowing service by open gays and lesbians but was not allowed to appear before the Committee by Nunn. In a June 1993 \"Washington Post\" opinion piece, Goldwater wrote: \"You don't have to be straight to shoot straight\". The White House was also reportedly upset when LGBT activist David Mixner openly described Nunn as an \"old-fashioned bigot\" for opposing Clinton's plan to lift the ban on gays in the military.\nCongress rushed to enact the existing gay ban policy into federal law, outflanking Clinton's planned repeal effort. Clinton called for legislation to overturn the ban, but encountered intense opposition from the Joint Chiefs of Staff, members of Congress, and portions of the public. DADT emerged as a compromise policy. Congress included text in the National Defense Authorization Act for Fiscal Year 1994 (passed in 1993) requiring the military to abide by regulations essentially identical to the 1982 absolute ban policy. The Clinton administration on December 21, 1993, issued Defense Directive 1304.26, which directed that military applicants were not to be asked about their sexual orientation. This policy is now known as \"Don't Ask, Don't Tell\". The phrase was coined by Charles Moskos, a military sociologist.\nIn accordance with the December 21, 1993, Department of Defense Directive 1332.14, it was legal policy (10 U.S.C. \u00a7 654) that homosexuality was incompatible with military service and that persons who engaged in homosexual acts or stated that they are homosexual or bisexual were to be discharged. The Uniform Code of Military Justice, passed by Congress in 1950 and signed by President Harry S Truman, established the policies and procedures for discharging service members.\nThe full name of the policy at the time was \"Don't Ask, Don't Tell, Don't Pursue\". The \"Don't Ask\" provision mandated that military or appointed officials not ask about or require members to reveal their sexual orientation. The \"Don't Tell\" stated that a member may be discharged for claiming to be a homosexual or bisexual or making a statement indicating a tendency towards or intent to engage in homosexual activities. The \"Don't Pursue\" established what was minimally required for an investigation to be initiated. A \"Don't Harass\" provision was added to the policy later. It ensured that the military would not allow harassment or violence against service members for any reason.\nThe Servicemembers Legal Defense Network was founded in 1993 to advocate an end to discrimination on the basis of sexual orientation in the U.S. Armed Forces.\nCourt challenges.\nDADT was upheld by five federal Courts of Appeal. The Supreme Court, in \"Rumsfeld v. Forum for Academic and Institutional Rights, Inc.\" (2006), unanimously held that the federal government could constitutionally withhold funding from universities, no matter what their nondiscrimination policies might be, for refusing to give military recruiters access to school resources. An association of law schools had argued that allowing military recruiting at their institutions compromised their ability to exercise their free speech rights in opposition to discrimination based on sexual orientation as represented by DADT.\n\"McVeigh v. Cohen\".\nIn January 1998, Senior Chief Petty Officer Timothy R. McVeigh (not to be confused with convicted Oklahoma City bomber, Timothy J. McVeigh) won a preliminary injunction from a U.S. district court that prevented his discharge from the U.S. Navy for \"homosexual conduct\" after 17 years of service. His lawsuit did not challenge the DADT policy but asked the court to hold the military accountable for adhering to the policy's particulars. The Navy had investigated McVeigh's sexual orientation based on his AOL email account name and user profile. District Judge Stanley Sporkin ruled in \"McVeigh v. Cohen\" that the Navy had violated its own DADT guidelines: \"Suggestions of sexual orientation in a private, anonymous email account did not give the Navy a sufficient reason to investigate to determine whether to commence discharge proceedings.\" He called the Navy's investigation \"a search and destroy mission\" against McVeigh. The case also attracted attention because a navy paralegal had misrepresented himself when querying AOL for information about McVeigh's account. Frank Rich linked the two issues: \"McVeigh is as clear-cut a victim of a witch hunt as could be imagined, and that witch hunt could expand exponentially if the military wants to add on-line fishing to its invasion of service members' privacy.\" AOL apologized to McVeigh and paid him damages. McVeigh reached a settlement with the Navy that paid his legal expenses and allowed him to retire with full benefits in July. \"The New York Times\" called Sporkin's ruling \"a victory for gay rights, with implications for the millions of people who use computer on-line services\".\n\"Witt v. Department of the Air Force\".\nIn April 2006, Margaret Witt, a major in the United States Air Force who was being investigated for homosexuality, filed suit in the United States District Court for the Western District of Washington seeking declaratory and injunctive relief on the grounds that DADT violates substantive due process, the Equal Protection Clause, and procedural due process. In July 2007 the Secretary of the Air Force ordered her honorable discharge. Dismissed by the district court, the case was heard on appeal, and the Ninth Circuit issued its ruling on May 21, 2008. Its decision in \"Witt v. Department of the Air Force\" reinstated Witt's substantive-due-process and procedural-due-process claims and affirmed the dismissal of her Equal Protection claim. The Ninth Circuit, analyzing the Supreme Court decision in \"Lawrence v. Texas\" (2003), determined that DADT had to be subjected to heightened scrutiny, meaning that there must be an \"important\" governmental interest at issue, that DADT must \"significantly\" further the governmental interest, and that there can be no less intrusive way for the government to advance that interest.\nThe Obama administration declined to appeal, allowing a May 3, 2009, deadline to pass, leaving \"Witt\" as binding on the entire Ninth Circuit, and returning the case to the District Court. On September 24, 2010, District Judge Ronald B. Leighton ruled that Witt's constitutional rights had been violated by her discharge and that she must be reinstated to the Air Force.\nThe government filed an appeal with the Ninth Circuit on November 23, but did not attempt to have the trial court's ruling stayed pending the outcome. In a settlement announced on May 10, 2011, the Air Force agreed to drop its appeal and remove Witt's discharge from her military record. She will retire with full benefits.\n\"Log Cabin Republicans v. United States of America\".\nIn 2010, a lawsuit filed in 2004 by the Log Cabin Republicans (LCR), the nation's largest Republican gay organization, went to trial. Challenging the constitutionality of DADT, the plaintiffs stated that the policy violates the rights of gay military members to free speech, due process and open association. The government argued that DADT was necessary to advance a legitimate governmental interest. Plaintiffs introduced statements by President Barack Obama, from prepared remarks, that DADT \"doesn't contribute to our national security\", \"weakens our national security\", and that reversal is \"essential for our national security\". According to plaintiffs, these statements alone satisfied their burden of proof on the due process claims.\nOn September 9, 2010, Judge Virginia A. Phillips ruled in \"Log Cabin Republicans v. United States of America\" that the ban on service by openly gay service members was an unconstitutional violation of the First and Fifth Amendments. On October 12, 2010, she granted an immediate worldwide injunction prohibiting the Department of Defense from enforcing the \"Don't Ask Don't Tell\" policy and ordered the military to suspend and discontinue any investigation or discharge, separation, or other proceedings based on it. The Department of Justice appealed her decision and requested a stay of her injunction, which Phillips denied but which the Ninth Circuit Court of Appeals granted on October 20 and stayed pending appeal on November 1. The U.S. Supreme Court refused to overrule the stay. District Court neither anticipated questions of constitutional law nor formulated a rule broader than is required by the facts. The constitutional issues regarding DADT are well-defined, and the District Court focused specifically on the relevant inquiry of whether the statute impermissibly infringed upon substantive due process rights with regard to a protected area of individual liberty. Engaging in a careful and detailed review of the facts presented to it at trial, the District Court concluded that the Government put forward no persuasive evidence to demonstrate that the statute is a valid exercise of congressional authority to legislate in the realm of protected liberty interests. See Log Cabin, 716 F. Supp. 2d at 923. Hypothetical questions were neither presented nor answered in reaching this decision. On October 19, 2010, military recruiters were told they could accept openly gay applicants. On October 20, 2010, Lt. Dan Choi, an openly gay man honorably discharged under DADT, re-enlisted in the U.S. Army.\nFollowing the passage of the Don't Ask, Don't Tell Repeal Act of 2010, the Justice Department asked the Ninth Circuit to suspend LCR's suit in light of the legislative repeal. LCR opposed the request, noting that gay personnel were still subject to discharge. On January 28, 2011, the Court denied the Justice Department's request. The Obama administration responded by requesting that the policy be allowed to stay in place while they completed the process of assuring that its end would not impact combat readiness. On March 28, the LCR filed a brief asking that the court deny the administration's request.\nIn 2011, while waiting for certification, several service members were discharged under DADT at their own insistence, until July 6 when a three-judge panel of the Ninth Circuit Court of Appeals re-instated Judge Phillips' injunction barring further enforcement of the U.S. military's ban on openly gay service members. On July 11, the appeals court asked the DOJ to inform the court if it intended to proceed with its appeal. On July 14, the Justice Department filed a motion \"to avoid short-circuiting the repeal process established by Congress during the final stages of the implementation of the repeal\". and warning of \"significant immediate harms on the government\". On July 15, the Ninth Circuit restored most of the DADT policy, but continued to prohibit the government from discharging or investigating openly gay personnel. Following the implementation of DADT's repeal, a panel of three judges of the Ninth Circuit Court of Appeals vacated the Phillips ruling.\nDebate.\nFollowing the July 1999 murder of Army Pfc. Barry Winchell, apparently motivated by anti-gay bias, President Clinton issued an executive order modifying the Uniform Code of Military Justice to permit evidence of a hate crime to be admitted during the sentencing phase of a trial. In December, Secretary of Defense William Cohen ordered a review of DADT to determine if the policy's anti-gay harassment component was being observed. When that review found anti-gay sentiments were widely expressed and tolerated in the military, the DOD adopted a new anti-harassment policy in July 2000, though its effectiveness was disputed. On December 7, 1999, Hillary Clinton told an audience of gay supporters that \"Gays and lesbians already serve with distinction in our nation's armed forces and should not face discrimination. Fitness to serve should be based on an individual's conduct, not their sexual orientation.\" Later that month, retired General Carl E. Mundy Jr. defended the implementation of DADT against what he called the \"politicization\" of the issue by both Clintons. He cited discharge statistics for the Marines for the past five years that showed 75% were based on \"voluntary admission of homosexuality\" and 49% occurred during the first six months of service, when new recruits were most likely to reevaluate their decision to enlist. He also argued against any change in the policy, writing in the \"New York Times\": \"Conduct that is widely rejected by a majority of Americans can undermine the trust that is essential to creating and maintaining the sense of unity that is critical to the success of a military organization operating under the very different and difficult demands of combat.\" The conviction of Winchell's murderer, according to the \"New York Times\", \"galvanized opposition\" to DADT, an issue that had \"largely vanished from public debate\". Opponents of the policy focused on punishing harassment in the military rather than the policy itself, which Senator Chuck Hagel defended on December 25: \"The U.S. armed forces aren't some social experiment.\"\nThe principal candidates for the Democratic presidential nomination in 2000, Al Gore and Bill Bradley, both endorsed military service by open gays and lesbians, provoking opposition from high-ranking retired military officers, notably the recently retired commandant of the Marine Corps, General Charles C. Krulak. He and others objected to Gore's statement that he would use support for ending DADT as a \"litmus test\" when considering candidates for the Joint Chiefs of Staff. The 2000 Democratic Party platform was silent on the issue, while the Republican Party platform that year said: \"We affirm that homosexuality is incompatible with military service.\" Following the election of George W. Bush in 2000, observers expected him to avoid any changes to DADT, since his nominee for Secretary of State Colin Powell had participated in its creation.\nIn February 2004, members of the British Armed Forces, Lt Rolf Kurth and Lt Cdr Craig Jones, along with Aaron Belkin, Director of the Center for the Study of Sexual Minorities in the Military met with members of Congress and spoke at the National Defense University. They spoke about their experience of the current situation in the UK. The UK lifted the ban on gay members serving in their forces in 2000.\nIn July 2004, the American Psychological Association issued a statement that DADT \"discriminates on the basis of sexual orientation\" and that \"Empirical evidence fails to show that sexual orientation is germane to any aspect of military effectiveness including unit cohesion, morale, recruitment and retention.\" It said that the U.S. military's track record overcoming past racial and gender discrimination demonstrated its ability to integrate groups previously excluded. The Republican Party platform that year reiterated its support for the policy\u2014\"We affirm traditional military culture, and we affirm that homosexuality is incompatible with military service.\"\u2014while the Democratic Party maintained its silence.\nIn February 2005, the Government Accountability Office released estimates of the cost of DADT. It reported at least $95.4\u00a0million in recruiting costs and at least $95.1\u00a0million for training replacements for the 9,488 troops discharged from 1994 through 2003, while noting that the true figures might be higher. In September, as part of its campaign to demonstrate that the military allowed open homosexuals to serve when its workforce requirements were greatest, the Center for the Study of Sexual Minorities in the Military (now the Palm Center) reported that army regulations allowed the active-duty deployment of Army Reservists and National Guard troops who claim to be or who are accused of being gay. A U.S. Army Forces Command spokesperson said the regulation was intended to prevent Reservists and National Guard members from pretending to be gay to escape combat. Advocates of ending DADT repeatedly publicized discharges of highly trained gay and lesbian personnel, especially those in positions with critical shortages, including fifty-nine Arabic speakers and nine Persian speakers. Elaine Donnelly, president of the Center for Military Readiness, later argued that the military's failure to ask about sexual orientation at recruitment was the cause of the discharges: [Y]ou could reduce this number to zero or near zero if the Department of Defense dropped Don't Ask, Don't Tell.\u00a0... We should not be training people who are not eligible to be in the Armed Forces.\"\nIn February 2006, a University of California Blue Ribbon Commission that included Lawrence Korb, a former assistant defense secretary during the Reagan administration, William Perry, Secretary of Defense in the Clinton administration, and professors from the United States Military Academy released their assessment of the GAO's analysis of the cost of DADT released a year earlier. The commission report stated that the GAO did not take into account the value the military lost from the departures. They said that that total cost was closer to $363\u00a0million, including $14.3\u00a0million for \"separation travel\" following a service member's discharge, $17.8\u00a0million for training officers, $252.4\u00a0million for training enlistees, and $79.3\u00a0million in recruiting costs.\nIn 2006, Soulforce, a national LGBT rights organization, organized its Right to Serve Campaign, in which gay men and lesbians in several cities attempted to enlist in the Armed Forces or National Guard. Donnelly of the Center for Military Readiness stated in September: \"I think the people involved here do not have the best interests of the military at heart. They never have. They are promoting an agenda to normalize homosexuality in America using the military as a battering ram to promote that broader agenda.\" She said that \"pro-homosexual activists\u00a0... are creating media events all over the country and even internationally.\"\nIn 2006, a speaking tour of gay former service members, organized by SLDN, Log Cabin Republicans, and Meehan, visited 18 colleges and universities. Patrick Guerriero, executive director of Log Cabin, thought the repeal movement was gaining \"new traction\" but \"Ultimately\", said, \"we think it's going to take a Republican with strong military credentials to make a shift in the policy.\" Elaine Donnelly called such efforts \"a big P.R. campaign\" and said that \"The law is there to protect good order and discipline in the military, and it's not going to change.\"\nIn December 2006, Zogby International released the results of a poll of military personnel conducted in October 2006 that found that 26% favored allowing gays and lesbians to serve openly in the military, 37% were opposed, while 37% expressed no preference or were unsure. Of respondents who had experience with gay people in their unit, 6% said their presence had a positive impact on their personal morale, 66% said no impact, and 28% said negative impact. Regarding overall unit morale, 3% said positive impact, 64% no impact, and 27% negative impact.\nRetired Chairman of the Joint Chiefs of Staff General John Shalikashvili and former Senator and Secretary of Defense William Cohen opposed the policy in January 2007: \"I now believe that if gay men and lesbians served openly in the United States military, they would not undermine the efficacy of the armed forces\" Shalikashvili wrote. \"Our military has been stretched thin by our deployments in the Middle East, and we must welcome the service of any American who is willing and able to do the job.\" Shalikashvili cited the recent \"Zogby poll of more than 500 service members returning from Afghanistan and Iraq, three-quarters of whom said they were comfortable interacting with gay people. The debate took a different turn in March when General Peter Pace, Chairman of the Joint Chiefs of Staff, told the editorial board of the \"Chicago Tribune\" he supported DADT because \"homosexual acts between two individuals are immoral and\u00a0... we should not condone immoral acts.\" His remarks became, according to the \"Tribune\", \"a huge news story on radio, television and the Internet during the day and showed how sensitive the Pentagon's policy has become.\" Senator John Warner, who backed DADT, said \"I respectfully, but strongly, disagree with the chairman's view that homosexuality is immoral\", and Pace expressed regret for expressing his personal views and said that DADT \"does not make a judgment about the morality of individual acts.\" Massachusetts Governor Mitt Romney, then in the early stages of his campaign for the 2008 Republican presidential nomination, defended DADT:\nThat summer, after U.S. Senator Larry Craig was arrested for lewd conduct in a men's restroom, conservative commentator Michael Medved argued that any liberalization of DADT would \"compromise restroom integrity and security\". He wrote: \"The national shudder of discomfort and queasiness associated with any introduction of homosexual eroticism into public men's rooms should make us more determined than ever to resist the injection of those lurid attitudes into the even more explosive situation of the U.S. military.\"\nIn November 2007, 28 retired generals and admirals urged Congress to repeal the policy, citing evidence that 65,000 gay men and women were serving in the armed forces and that there were over a million gay veterans. On November 17, 2008, 104 retired generals and admirals signed a similar statement. In December, SLDN arranged for \"60 Minutes\" to interview Darren Manzella, an Army medic who served in Iraq after coming out to his unit.\nIn 2008, former U.S. Senator Sam Nunn, who previously stalled efforts to lift the ban on gays serving in military when he was Chairman of the Senate Armed Forces Committee, hinted a shift from his previous political views by endorsing a new Pentagon study to examine the issue of homosexuals serving openly in the military, stating \"I think [when] 15 years go by on any personnel policy, it's appropriate to take another look at it\u2014see how it's working, ask the hard questions, hear from the military. Start with a Pentagon study.\"\nOn May 4, 2008, while Chairman of the Joint Chiefs of Staff Admiral Mike Mullen addressed the graduating cadets at West Point, a cadet asked what would happen if the next administration were supportive of legislation allowing gays to serve openly. Mullen responded, \"Congress, and not the military, is responsible for DADT.\" Previously, during his Senate confirmation hearing in 2007, Mullen told lawmakers, \"I really think it is for the American people to come forward, really through this body, to both debate that policy and make changes, if that's appropriate.\" He went on to say, \"I'd love to have Congress make its own decisions\" with respect to considering repeal.\nIn May 2009, when a committee of military law experts at the Palm Center, an anti-DADT research institute, concluded that the President could issue an Executive Order to suspend homosexual conduct discharges, Obama rejected that option and said he wanted Congress to change the law.\nOn July 5, 2009, Colin Powell told CNN that the policy was \"correct for the time\" but that \"sixteen years have now gone by, and I think a lot has changed with respect to attitudes within our country, and therefore I think this is a policy and a law that should be reviewed.\" Interviewed for the same broadcast, Mullen said the policy would continue to be implemented until the law was repealed, and that his advice was to \"move in a measured way.\u00a0... At a time when we're fighting two conflicts there is a great deal of pressure on our forces and their families.\" In September, \"Joint Force Quarterly\" published an article by an Air Force colonel that disputed the argument that unit cohesion is compromised by the presence of openly gay personnel.\nIn October 2009, the Commission on Military Justice, known as the Cox Commission, repeated its 2001 recommendation that Article 125 of the Uniform Code of Military Justice, which bans sodomy, be repealed, noting that \"most acts of consensual sodomy committed by consenting military personnel are not prosecuted, creating a perception that prosecution of this sexual behavior is arbitrary.\"\nIn January 2010, the White House and congressional officials started work on repealing the ban by inserting language into the 2011 defense authorization bill. During Obama's State of the Union Address on January 27, 2010, he said that he would work with Congress and the military to enact a repeal of the gay ban law and for the first time set a timetable for repeal.\nAt a February 2, 2010, congressional hearing, Senator John McCain read from a letter signed by \"over one thousand former general and flag officers\". It said: \"We firmly believe that this law, which Congress passed to protect good order, discipline and morale in the unique environment of the armed forces, deserves continued support.\" The signature campaign had been organized by Elaine Donnelly of the Center for Military Readiness, a longtime supporter of a traditional all-male and all-heterosexual military. Servicemembers United, a veterans group opposed to DADT, issued a report critical of the letter's legitimacy. They said that among those signing the letter were officers who had no knowledge of their inclusion or who had refused to be included, and even one instance of a general's widow who signed her husband's name to the letter though he had died before the survey was published. The average age of the officers whose names were listed as signing the letter was 74, the oldest was 98, and Servicemembers United noted that \"only a small fraction of these officers have even served in the military during the 'Don't Ask, Don't Tell' period, much less in the 21st century military.\"\nThe Center for American Progress issued a report in March 2010 that said a smooth implementation of an end to DADT required eight specified changes to the military's internal regulations. On March 25, 2010, Defense Secretary Gates announced new rules mandating that only flag officers could initiate discharge proceedings and imposing more stringent rules of evidence on discharge proceedings.\nRepeal.\nThe underlying justifications for DADT had been subjected to increasing suspicion and outright rejection by the early 21st century. Mounting evidence obtained from the integration efforts of foreign militaries, surveys of U.S. military personnel, and studies conducted by the DoD\u00a0gave credence to the view that the presence of open homosexuals within the military would not be detrimental at all to the armed forces. A DoD study conducted at the behest of Secretary of Defense Robert Gates in 2010 supports this most.\nThe DoD working group conducting the study considered the impact that lifting the ban would have on unit cohesion and effectiveness, good order and discipline, and military morale. The study included a survey that revealed significant differences between respondents who believed they had served with homosexual troops and those who did not believe they had. In analyzing such data, the DoD working group concluded that it was actually generalized perceptions of homosexual troops that led to the perceived unrest that would occur without DADT. Ultimately, the study deemed the overall risk to military effectiveness of lifting the ban to be low. Citing the ability of the armed forces to adjust to the previous integration of African-Americans and women, the DoD study asserted that the United States military could adjust as had it before in history without an impending serious effect.\nIn March 2005, Rep. Martin T. Meehan introduced the Military Readiness Enhancement Act in the House. It aimed \"to amend title 10, United States Code, to enhance the readiness of the Armed Forces by replacing the current policy concerning homosexuality in the Armed Forces, referred to as 'Don't ask, don't tell,' with a policy of nondiscrimination on the basis of sexual orientation\". As of 2006, it had 105 Democrats and 4 Republicans as co-sponsors. He introduced the bill again in 2007 and 2009.\nDuring the 2008 U.S. presidential election campaign, Senator Barack Obama advocated a full repeal of the laws barring gays and lesbians from serving in the military. Nineteen days after his election, Obama's advisers announced that plans to repeal the policy might be delayed until 2010, because Obama \"first wants to confer with the Joint Chiefs of Staff and his new political appointees at the Pentagon to reach a consensus, and then present legislation to Congress\". As president he advocated a policy change to allow gay personnel to serve openly in the armed forces, stating that the U.S. government has spent millions of dollars replacing troops expelled from the military, including language experts fluent in Arabic, because of DADT. On the eve of the National Equality March in Washington, D.C., October 10, 2009, Obama stated in a speech before the Human Rights Campaign that he would end the ban, but he offered no timetable. Obama said in his 2010 State of the Union Address: \"This year, I will work with Congress and our military to finally repeal the law that denies gay Americans the right to serve the country they love because of who they are.\" This statement was quickly followed up by Defense Secretary Robert Gates and Joint Chiefs chairman Michael Mullen voicing their support for a repeal of DADT.\nDon't Ask, Don't Tell Repeal Act of 2010.\nDemocrats in both houses of Congress first attempted to end DADT by amending the Defense Authorization Act. On May 27, 2010, on a 234\u2013194 vote, the U.S. House of Representatives approved the Murphy amendment to the National Defense Authorization Act for Fiscal Year 2011. It provided for repeal of the DADT policy and created a process for lifting the policy, including a U.S. Department of Defense study and certification by key officials that the change in policy would not harm military readiness followed by a waiting period of 60 days. The amended defense bill passed the House on May 28, 2010. On September 21, 2010, John McCain led a successful filibuster against on the Defense Authorization Act, in which 56 Senators voted to end debate, four short of the 60 votes required. Some advocates for repeal, including the Palm Center, OutServe, and Knights Out, opposed any attempt to block the passage of NDAA if it failed to include DADT repeal language. The Human Rights Campaign, the Center for American Progress, Servicemembers United and SLDN refused to concede that possibility.\nThe American Civil Liberties Union (ACLU) filed a lawsuit, \"Collins v. United States\", against the Department of Defense in November 2010 seeking full compensation for those discharged under the policy.\nOn November 30, 2010, the Joint Chiefs of Staff released the \"Don't Ask, Don't Tell\" Comprehensive Review Working Group (CRWG) report authored by Jeh C. Johnson, General Counsel of the Department of Defense, and Army General Carter F. Ham. It outlined a path to the implementation of repeal of DADT. The report indicated that there was a low risk of service disruptions due to repealing the ban, provided time was provided for proper implementation and training. It included the results of a survey of 115,000 active-duty and reserve service members. Across all service branches, 30 percent thought that integrating gays into the military would have negative consequences. In the Marine Corps and combat specialties, the percentage with that negative assessment ranged from 40 to 60 percent. The CRWG also said that 69 percent of all those surveyed believed they had already worked with a gay or lesbian and of those, 92 percent reported that the impact of that person's presence was positive or neutral. The same day, in response to the CRWG, 30 professors and scholars, most from military institutions, issued a joint statement saying that the CRWG \"echoes more than 20 studies, including studies by military researchers, all of which reach the same conclusion: allowing gays and lesbians to serve openly will not harm the military\u00a0... We hope that our collective statement underscores that the debate about the evidence is now officially over\". The Family Research Council's president, Tony Perkins, interpreted the CRWG data differently, writing that it \"reveals that 40 percent of Marines and 25 percent of the Army could leave\".\nGates encouraged Congress to act quickly to repeal the law so that the military could carefully adjust rather than face a court decision requiring it to lift the policy immediately. The United States Senate held two days of hearings on December 2 and 3, 2010, to consider the CRWG report. Defense Secretary Robert Gates, Joint Chiefs chairman Michael Mullen urged immediate repeal. The heads of the Marine Corps, Army, and Navy all advised against immediate repeal and expressed varied views on its eventual repeal. Oliver North, writing in \"National Review\" the next week, said that Gates' testimony showed \"a deeply misguided commitment to political correctness\". He interpreted the CRWG's data as indicating a high risk that large numbers of resignations would follow the repeal of DADT. Service members, especially combat troops, he wrote, \"deserve better than to be treated like lab rats in Mr. Obama's radical social experiment\".\nOn December 9, 2010, another filibuster prevented debate on the Defense Authorization Act. In response to that vote, Senators Joe Lieberman and Susan Collins introduced a bill that included the policy-related portions of the Defense Authorization Act that they considered more likely to pass as a stand-alone bill. It passed the House on a vote of 250 to 175 on December 15, 2010. On December 18, 2010, the Senate voted to end debate on its version of the bill by a cloture vote of 63\u201333. The final Senate vote was held later that same day, with the measure passing by a vote of 65\u201331.\nU.S. Secretary of Defense Robert Gates released a statement following the vote indicating that the planning for implementation of a policy repeal would begin right away and would continue until Gates certified that conditions were met for orderly repeal of the policy. President Obama signed the repeal into law on December 22, 2010.\nImplementation of repeal.\nThe repeal act established a process for ending the DADT policy. The President, the Secretary of Defense and the Chairman of the Joint Chiefs of Staff were required to certify in writing that they had reviewed the Pentagon's report on the effects of DADT repeal, that the appropriate regulations had been reviewed and drafted, and that implementation of repeal regulations \"is consistent with the standards of military readiness, military effectiveness, unit cohesion, and recruiting and retention of the Armed Forces\". Once certification was given, DADT would be lifted after a 60-day waiting period.\nRepresentative Duncan D. Hunter announced plans in January 2011 to introduce a bill designed to delay the end of DADT. His proposed legislation required all of the chiefs of the armed services to submit the certification at the time required only of the President, Defense Secretary and Joint Chiefs chairman. In April, Perkins of the Family Research Council argued that the Pentagon was misrepresenting its own survey data and that hearings by the House Armed Services Committee, now under Republican control, could persuade Obama to withhold certification. Congressional efforts to prevent the change in policy from going into effect continued into May and June 2011.\nOn January 29, 2011, Pentagon officials stated that the training process to prepare troops for the end of DADT would begin in February and would proceed quickly, though they suggested that it might not be completed in 2011. On the same day, the DOD announced it would not offer any additional compensation to service members who had been discharged under DADT, who received half of the separation pay other honorably discharged service members received.\nIn May 2011, the U.S. Army reprimanded three colonels for performing a skit in March 2011 at a function at Yongsan Garrison, South Korea, that mocked the repeal.\nIn May 2011, revelations that an April Navy memo relating to its DADT training guidelines contemplated allowing same-sex weddings in base chapels and allowing chaplains to officiate if they so chose resulted in a letter of protest from 63 Republican congressman, citing the Defense of Marriage Act (DOMA) as controlling the use of federal property. Tony Perkins of the Family Research Council said the guidelines \"make it even more uncomfortable for men and women of faith to perform their duties\". A Pentagon spokesperson replied that DOMA \"does not limit the type of religious ceremonies a chaplain may perform in a chapel on a military installation\", and a Navy spokesperson said that \"A chaplain can conduct a same-sex ceremony if it is in the tenets of his faith\". A few days later the Navy rescinded its earlier instructions \"pending additional legal and policy review and interdepartmental coordination\".\nWhile waiting for certification, several service members were discharged at their own insistence until a July 6 ruling from a federal appeals court barred further enforcement of the U.S. military's ban on openly gay service members, which the military promptly did.\nAnticipating the lifting of DADT, some active duty service members wearing civilian clothes marched in San Diego's gay pride parade on July 16. The DOD noted that participation \"does not constitute a declaration of sexual orientation\".\nPresident Obama, Secretary of Defense Leon Panetta, and Admiral Mike Mullen, Chairman of the Joint Chiefs of Staff, sent the certification required by the Repeal Act to Congress on July 22, 2011, setting the end of DADT for September 20, 2011. A Pentagon spokesman said that service members discharged under DADT would be able to re-apply to rejoin the military then.\nAt the end of August 2011, the DOD approved the distribution of the magazine produced by OutServe, an organization of gay and lesbian service members, at Army and Air Force base exchanges beginning with the September 20 issue, coinciding with the end of DADT.\nOn September 20, Air Force officials announced that 22 Air Force Instructions were \"updated as a result of the repeal of DADT\". On September 30, 2011, the Department of Defense modified regulations to reflect the repeal by deleting \"homosexual conduct\" as a ground for administrative separation.\nDay of repeal and aftermath.\nOn the eve of repeal, US Air Force 1st Lt. Josh Seefried, one of the founders of OutServe, an organization of LGBT troops, revealed his identity after two years of hiding behind a pseudonym. Senior Airman Randy Phillips, after conducting a social media campaign seeking encouragement coming out and already out to his military co-workers, came out to his father on the evening of September 19. When the video of their conversation he posted on YouTube went viral, it made him, in one journalist's estimation, \"the poster boy for the DADT repeal\". The moment the repeal took effect at midnight on September 19, US Navy Lt. Gary C. Ross married his same-sex partner of eleven and a half years, Dan Swezy, making them the first same-sex military couple to legally marry in the United States. Retired Rear Adm. Alan M. Steinman became the highest-ranking person to come out immediately following the end of DADT. HBO produced a World of Wonder documentary, \"The Strange History of Don't Ask, Don't Tell\", and premiered it on September 20. \"Variety\" called it \"an unapologetic piece of liberal advocacy\" and \"a testament to what formidable opponents ignorance and prejudice can be\". Discharge proceedings on the grounds of homosexuality, some begun years earlier, came to an end.\nIn the weeks that followed, a series of firsts attracted press attention to the impact of the repeal. The Marine Corps were the first branch of the armed services to recruit from the LGBTQ community. Reservist Jeremy Johnson became the first person discharged under DADT to re-enlist. Jase Daniels became the first to return to active duty, re-joining the Navy as a third class petty officer. On December 2, Air Force intelligence officer Ginger Wallace became the first open LGBT service member to have a same-sex partner participate in the \"pinning-on\" ceremony that marked her promotion to colonel. On December 23, after 80 days at sea, US Navy Petty Officer 2nd Class Marissa Gaeta won the right to the traditional \"first kiss\" upon returning to port and shared it with her same-sex partner. On January 20, 2012, U.S. service members deployed to Bagram, Afghanistan, produced a video in support of the It Gets Better Project, which aims to support LGBT at-risk youth. Widespread news coverage continued even months after the repeal date, when a photograph of Marine Sgt. Brandon Morgan kissing his partner at a February 22, 2012, homecoming celebration on Marine Corps Base Hawaii went viral. When asked for a comment, a spokesperson for the Marine Corps said: \"It's your typical homecoming photo.\"\nOn September 30, 2011, Under Secretary of Defense Clifford Stanley announced the DOD's policy that military chaplains are allowed to perform same-sex marriages \"on or off a military installation\" where local law permits them. His memo noted that \"a chaplain is not required to participate in or officiate a private ceremony if doing so would be in variance with the tenets of his or her religion\" and \"a military chaplain's participation in a private ceremony does not constitute an endorsement of the ceremony by DoD\". Some religious groups announced that their chaplains would not participate in such weddings, including an organization of evangelical Protestants, the Chaplain Alliance for Religious Liberty and Roman Catholics led by Archbishop Timothy Broglio of the Archdiocese for the Military Services, USA.\nIn late October 2011, speaking at the Air Force Academy, Colonel Gary Packard, leader of the team that drafted the DOD's repeal implementation plan, said: \"The best quote I've heard so far is, 'Well, some people's Facebook status changed, but that was about it. In late November, discussing the repeal of DADT and its implementation, Marine General James F. Amos said \"I'm very pleased with how it has gone\" and called it a \"non-event\". He said his earlier public opposition was appropriate based on ongoing combat operations and the negative assessment of the policy given by 56% of combat troops under his command in the Department of Defense's November 2010 survey. A Defense Department spokesperson said implementation of repeal occurred without incident and added: \"We attribute this success to our comprehensive pre-repeal training program, combined with the continued close monitoring and enforcement of standards by our military leaders at all levels.\"\nIn December 2011, Congress considered two DADT-related amendments in the course of work on the National Defense Authorization Act for 2012. The Senate approved 97\u20133, an amendment removing the prohibition on sodomy found in Article 125 of the Uniform Code of Military Justice as recommended by the Comprehensive Review Working Group (CRWG) a year earlier. The House approved an amendment banning same-sex marriages from being performed at military bases or by military employees, including chaplains and other employees of the military when \"acting in an official capacity\". Neither amendment appeared in the final legislation.\nIn July 2012, the Department of Defense granted permission for military personnel to wear their uniforms while participating in the San Diego Pride Parade. This was the first time that U.S. military personnel were permitted to wear their service uniforms in such a parade.\nMarking the first anniversary of the passage of the Repeal Act, television news networks reported no incidents in the three months since DADT ended. One aired video of a social gathering for gay service members at a base in Afghanistan. Another reported on the experience of lesbian and gay troops, including some rejection after coming out to colleagues.\nThe Palm Center, a think tank that studies issues of sexuality and the military, released a study in September 2012 that found no negative consequences, nor any effect on military effectiveness from DADT repeal. This study began six months following repeal and concluded at the one year mark. The study included surveys of 553 generals and admirals who had opposed repeal, experts who supported DADT, and more than 60 heterosexual, gay, lesbian and bisexual active duty service personnel.\nOn January 7, 2013, the ACLU reached a settlement with the federal government in \"Collins v. United States\". It provided for the payment of full separation pay to service members discharged under DADT since November 10, 2004, who had previously been granted only half that.\n2012 presidential campaign issue.\nSeveral candidates for the 2012 Republican presidential nomination called for the restoration of DADT, including Michele Bachmann, Rick Perry, and Rick Santorum. Newt Gingrich called for an extensive review of DADT's repeal.\nRon Paul, having voted for the Repeal Act, maintained his support for allowing military service by open homosexuals. Herman Cain called the issue \"a distraction\" and opposed reinstating DADT. Mitt Romney said that the winding down of military operations in Iraq and Afghanistan obviated his opposition to the repeal and said he was not proposing any change to policy.\nOn September 22, 2011, the audience at a Republican candidates' debate booed a U.S. soldier posted in Iraq who asked a question via video about the repeal of DADT, and none of the candidates acknowledged or responded to the crowd's behavior. Two days later, Obama commented on the incident while addressing a dinner of the Human Rights Campaign: \"You want to be commander in chief? You can start by standing up for the men and women who wear the uniform of the United States, even when it's not politically convenient\".\nIn June 2012, Rep. Howard McKeon, Republican chair of the House Armed Services Committee, said he considered the repeal of DADT a settled issue, and that if Romney became president he would not advocate its reinstatement, though others in his party might.\n2021 benefits restoration.\nIn September 2021, on the 10th anniversary of the Don't Ask, Don't Tell repeal, President Joe Biden announced that the Veterans Administration would start providing benefits for service members who received other-than-honorable discharges (before DADT was enacted and while it was in effect) because of their sexual orientation.\nIn 2024, more than 800 veterans who had been previously dishonorably discharged under DADT had their cases reviewed and discharge papers automatically updated to honorable discharge status. With this review, nearly all of the 13,500 people who were dishonorably discharged under DADT now have an honorable discharge and as a result, can access benefits for veterans.\nViews of the policy.\nPublic opinion.\nIn 1993, \"Time\" reported that 44% of those polled supported openly gay service members, and in 1994, a CNN poll indicated 53% of Americans believed gays and lesbians should be permitted to serve openly.\nAccording to a December 2010 \"Washington Post\"\u2013ABC News poll, 77% of Americans said gays and lesbians who publicly disclose their sexual orientation should be able to serve in the military. That number showed little change from polls over the previous two years, but represented the highest level of support in a Post-ABC poll. The support also cut across partisan and ideological lines, with majorities of Democrats (86%), Republicans (74%), independents (74%), liberals (92%), conservatives (67%), white evangelical Protestants (70%) and non-religious (84%) in favor of homosexuals serving openly.\nA November 2010 survey by the Pew Research Center found that 58% of the U.S. public favored allowing gays and lesbians to serve openly in the military, while less than half as many (27%) were opposed. According to a November 2010 CNN/Opinion Research Corporation poll, 72% of adult Americans favored permitting people who are openly gay or lesbian to serve in the military, while 23% opposed it. \"The main difference between the CNN poll and the Pew poll is in the number of respondents who told pollsters that they didn't have an opinion on this topic \u2013 16 percent in the Pew poll compared to only five percent in the CNN survey\", said CNN Polling Director Keating Holland. \"The two polls report virtually the same number who say they oppose gays serving openly in the military, which suggests that there are some people who favor that change in policy but for some reason were reluctant to admit that to the Pew interviewers. That happens occasionally on topics where moral issues and equal-treatment issues intersect.\"\nA February 2010 Quinnipiac University Polling Institute national poll showed 57% of American voters favored gays serving openly, compared to 36% opposed, while 66% said not allowing openly gay personnel to serve is discrimination, compared to 31% who did not see it as discrimination. A CBS News/\"New York Times\" national poll done at the same time showed 58% of Americans favored gays serving openly, compared to 28% opposed.\nChaplains and religious groups.\nChaplain groups and religious organizations took various positions on DADT. Some felt that the policy needed to be withdrawn to make the military more inclusive. The Southern Baptist Convention battled the repeal of DADT, warning that their endorsements for chaplains might be withdrawn if the repeal took place. They took the position that allowing gay men and women to serve in the military without restriction would have a negative impact on the ability of chaplains who think homosexuality is a sin to speak freely regarding their religious beliefs. The Roman Catholic Church called for the retention of the policy, but had no plans to withdraw its priests from serving as military chaplains. Sixty-five retired chaplains signed a letter opposing repeal, stating that repeal would make it impossible for chaplains whose faith teaches that same-sex behavior is immoral to minister to military service members. Other religious organizations and agencies called the repeal of the policy a \"non-event\" or \"non-issue\" for chaplains, saying that chaplains have always supported military service personnel, whether or not they agree with all their actions or beliefs.\nDischarges under DADT.\nAfter the policy was introduced in 1993, the military discharged over 13,000 troops from the military under DADT. The number of discharges per fiscal year under DADT dropped sharply after the September 11 attacks and remained comparatively low through to the repeal. Discharges exceeded 600 every year until 2009.\nState-based gay and lesbian military veteran laws.\nIn November 2019, both Rhode Island and New York State signed into law and implemented restoring military benefits to gay and lesbian military veterans. It is estimated that approximately 100,000 individuals were discharged between the beginning of World War II and the repeal of the Don't Ask Don't Tell policy in September 2011."}
{"id": "8691", "revid": "8524693", "url": "https://en.wikipedia.org/wiki?curid=8691", "title": "Divination", "text": "Divination () is the attempt to gain insight into a question or situation by way of an occultic ritual or practice. Using various methods throughout history, diviners ascertain their interpretations of how a should proceed by reading signs, events, or omens, or through alleged contact or interaction with supernatural agencies such as spirits, gods, god-like-beings or the \"will of the universe\".\nDivination can be seen as an attempt to organize what appears to be random so that it provides insight into a problem or issue at hand. Some instruments or practices of divination include Tarot-card reading, rune casting, tea-leaf reading, automatic writing, water scrying, and psychedelics like psilocybin mushrooms and DMT. If a distinction is made between divination and fortune-telling, divination has a more formal or ritualistic element and often contains a more social character, usually in a religious context, as seen in traditional African medicine. Fortune-telling, on the other hand, is a more everyday practice for personal purposes. Particular divination methods vary by culture and religion.\nIn its functional relation to magic in general, divination can have a preliminary and investigative role:\n[...] the diagnosis or prognosis achieved through divination is both temporarily and logically related to the manipulative, protective or alleviative function of magic rituals. In divination one finds the cause of an ailment or a potential danger, in magic one subsequently acts upon this knowledge.\nDivination has long attracted criticism. In the modern era, it has been dismissed by the scientific community and by skeptics as being superstitious; experiments do not support the idea that divination techniques can actually predict the future more reliably or precisely than would be possible without it. In antiquity, divination came under attack from philosophers such as the Academic skeptic Cicero in \"De Divinatione\" (1st century BCE) and the Pyrrhonist Sextus Empiricus in \"Against the Astrologers\" (2nd century CE). The satirist Lucian ( 125 \u2013 after 180) devoted an essay to Alexander the false prophet.\nHistory.\nAntiquity.\nThe eternal fire at Nymphaion in southern Illyria (present-day Albania) also functioned as an oracle. The forms of divination practiced in this natural fire sanctuary with peculiar physical properties were widely known to the ancient Greek and Roman authors. The Oracle of Amun at the Siwa Oasis was made famous when Alexander the Great visited it after conquering Egypt from Persia in 332 BC.\n or can be interpreted as categorically forbidding divination. But some biblical practices, such as Urim and Thummim, casting lots and prayer, are considered to be divination. Trevan G. Hatch disputes these comparisons because divination did not consult the \"one true God\" and manipulated the divine for the diviner's self-interest. One of the earliest known divination artifacts, a book called the Sortes Sanctorum, is believed to be of Christian roots, and utilizes dice to provide insight into the future.\nUri Gabbay states that divination was associated with sacrificial rituals in the ancient Near East, including Mesopotamia and Israel. Extispicy was a common example, where diviners would pray to their god(s) before vivisecting a sacrificial animal. Their abominal organs would reveal a divine message, which aligned with cardiocentric views of the mind.\nOracles and Greek divination.\nBoth oracles and seers in ancient Greece practiced divination. Oracles were the conduits for the gods on earth; their prophecies were understood to be the will of the gods verbatim. Because of the high demand for oracle consultations and the oracles\u2019 limited work schedule, they were not the main source of divination for the ancient Greeks. That role fell to the seers ().\nSeers were not in direct contact with the gods; instead, they were interpreters of signs provided by the gods. Seers used many methods to explicate the will of the gods including extispicy, ornithomancy, etc. They were more numerous than the oracles and did not keep a limited schedule; thus, they were highly valued by all Greeks, not just those with the capacity to travel to Delphi or other such distant sites.\nThe disadvantage of seers was that only direct yes-or-no questions could be answered. Oracles could answer more generalized questions, and seers often had to perform several sacrifices in order to get the most consistent answer. For example, if a general wanted to know if the omens were proper for him to advance on the enemy, he would ask his seer both that question and if it were better for him to remain on the defensive. If the seer gave consistent answers, the advice was considered valid.\nDuring battle, generals would frequently ask seers at both the campground (a process called the \"hiera\") and at the battlefield (called the \"sphagia\"). The hiera entailed the seer slaughtering a sheep and examining its liver for answers regarding a more generic question; the sphagia involved killing a young female goat by slitting its throat and noting the animal's last movements and blood flow. The battlefield sacrifice only occurred when two armies prepared for battle against each other. Neither force would advance until the seer revealed appropriate omens.\nBecause the seers had such power over influential individuals in ancient Greece, many were skeptical of the accuracy and honesty of the seers. The degree to which seers were honest depends entirely on the individual seers. Despite the doubt surrounding individual seers, the craft as a whole was well regarded and trusted by the Greeks, and the Stoics accounted for the validity of divination in their physics.\nMiddle Ages and Early Modern period.\nThe divination method of casting lots (Cleromancy) was used by the remaining eleven disciples of Jesus in to select a replacement for Judas Iscariot. Therefore, divination was arguably an accepted practice in the early church. However, divination became viewed as a pagan practice by Christian emperors during ancient Rome.\nIn 692 the Quinisext Council, also known as the \"Council in Trullo\" in the Eastern Orthodox Church, passed canons to eliminate pagan and divination practices. Fortune-telling and other forms of divination were widespread through the Middle Ages. In the constitution of 1572 and public regulations of 1661 of the Electorate of Saxony, capital punishment was used on those predicting the future. Laws forbidding divination practice continue to this day. The Waldensians sect were accused of practicing divination.\nSm\u00e5land is famous for \u00c5rsg\u00e5ng, a practice which occurred until the early 19th century in some parts of Sm\u00e5land. Generally occurring on Christmas and New Year's Eve, it is a practice in which one would fast and keep themselves away from light in a room until midnight to then complete a set of complex events to interpret symbols encountered throughout the journey to foresee the coming year.\nIn Islam, astrology (\"\u2018ilm ahkam al-nujum\"), the most widespread divinatory science, is the study of how celestial entities could be applied to the daily lives of people on earth. It is important to emphasize the practical nature of divinatory sciences because people from all socioeconomic levels and pedigrees sought the advice of astrologers to make important decisions in their lives. Astronomy was made a distinct science by intellectuals who did not agree with the former, although distinction may not have been made in daily practice, where astrology was technically outlawed and only tolerated if it was employed in public. Astrologers, trained as scientists and astronomers, were able to interpret the celestial forces that ruled the \"sub-lunar\" to predict a variety of information from lunar phases and drought to times of prayer and the foundation of cities. The courtly sanction and elite patronage of Muslim rulers benefited astrologers\u2019 intellectual statures.\nThe \u201cscience of the sand\u201d (\"\u2018ilm al-raml\"), otherwise translated as geomancy, is \u201cbased on the interpretation of figures traced on sand or other surface known as geomantic figures.\u201d It is a good example of Islamic divination at a popular level. The core principle that meaning derives from a unique occupied position is identical to the core principle of astrology.\nLike astronomy, geomancy used deduction and computation to uncover significant prophecies as opposed to omens (\"\u2018ilm al-fa\u2019l\"), which were process of \u201creading\u201d visible random events to decipher the invisible realities from which they originated. It was upheld by prophetic tradition and relied almost exclusively on text, specifically the Qur\u2019an (which carried a table for guidance) and poetry, as a development of bibliomancy. The practice culminated in the appearance of the illustrated \u201cBooks of Omens\u201d (\"Falnama\") in the early 16th century, an embodiment of the apocalyptic fears as the end of the millennium in the Islamic calendar approached.\nDream interpretation, or oneiromancy \"(\u2018ilm ta\u2019bir al-ru\u2019ya\"), is more specific to Islam than other divinatory science, largely because of the Qur\u2019an\u2019s emphasis on the predictive dreams of Abraham, Yusuf, and Muhammad. The important delineation within the practice lies between \u201cincoherent dreams\u201d and \u201csound dreams,\u201d which were \u201ca part of prophecy\u201d or heavenly message. Dream interpretation was always tied to Islamic religious texts, providing a moral compass to those seeking advice. The practitioner needed to be skilled enough to apply the individual dream to general precedent while appraising the singular circumstances.\nThe power of text held significant weight in the \"science of letters\" \"(\u2018ilm al-huruf\"), the foundational principle being \"God created the world through His speech.\" The science began with the concept of language, specifically Arabic, as the expression of \"the essence of what it signifies.\" Once the believer understood this, while remaining obedient to God\u2019s will, they could uncover the essence and divine truth of the objects inscribed with Arabic like amulets and talismans through the study of the letters of the Qur\u2019an with alphanumeric computations.\nIn Islamic practice in Senegal and Gambia, just like many other West African countries, diviners and religious leaders and healers were interchangeable because Islam was closely related with esoteric practices (like divination), which were responsible for the regional spread of Islam. As scholars learned esoteric sciences, they joined local non-Islamic aristocratic courts, who quickly aligned divination and amulets with the \"proof of the power of Islamic religion.\" So strong was the idea of esoteric knowledge in West African Islam, diviners and magicians uneducated in Islamic texts and Arabic bore the same titles as those who did.\nFrom the beginning of Islam, there \"was (and is) still a vigorous debate about whether or not such [divinatory] practices were actually permissible under Islam,\u201d with some scholars like Abu-Hamid al Ghazili (d. 1111) objecting to the science of divination because he believed it bore too much similarity to pagan practices of invoking spiritual entities that were not God. Other scholars justified esoteric sciences by comparing a practitioner to \"a physician trying to heal the sick with the help of the same natural principles.\"\nMesoamerica.\nDivination was a central component of ancient Mesoamerican religious life. Many Aztec gods, including central creator gods, were described as diviners and were closely associated with sorcery. Tezcatlipoca is the patron of sorcerers and practitioners of magic. His name means \"smoking mirror,\" a reference to a device used for divinatory scrying. In the Mayan \"Popol Vuh\", the creator gods Xmucane and Xpiacoc perform divinatory hand casting during the creation of people. The Aztec \"Codex Borbonicus\" shows the original human couple, Oxomoco and Cipactonal, engaged in divining with kernels of maize. This primordial pair is associated with the ritual calendar, and the Aztecs considered them to be the first diviners.\nEvery civilization that developed in pre-Columbian Mexico, from the Olmecs to the Aztecs, practiced divination in daily life, both public and private. Scrying through the use of reflective water surfaces, mirrors, or the casting of lots were among the most widespread forms of divinatory practice. Visions derived from hallucinogens were another important form of divination, and are still widely used among contemporary diviners of Mexico. Among the more common hallucinogenic plants used in divination are morning glory, jimson weed, and peyote.\nContemporary divination in Asia.\nIndia and Nepal.\nTheyyam or \"theiyam\" in Malayalam is the process by which a devotee invites a Hindu god or goddess to use his or her body as a medium or channel and answer other devotees' questions. The same is called \"arulvaakku\" or \"arulvaak\" in Tamil, another south Indian language - Adhiparasakthi Siddhar Peetam is famous for arulvakku in Tamil Nadu. The people in and around Mangalore in Karnataka call the same, Buta Kola, \"paathri\" or \"darshin\"; in other parts of Karnataka, it is known by various names such as, \"prashnaavali\", \"vaagdaana\", \"asei\", \"aashirvachana\", and so on. In Nepal it is known as, \"Devta ka dhaamee\" or \"jhaakri\".\nIn English, the closest translation for these is, \"oracle.\" The Dalai Lama, who lives in exile in northern India, still consults an oracle known as the \"Nechung Oracle\", which is considered the official state oracle of the government of Tibet. The Dalai Lama has according to centuries-old custom, consulted the Nechung Oracle during the new year festivities of Losar.\nJapan.\nAlthough Japan retains a history of traditional and local methods of divination, such as \"onmy\u014dd\u014d\", contemporary divination in Japan, called \"uranai\", derives from outside sources. Contemporary methods of divination in Japan include both Western and Chinese astrology, geomancy or feng shui, tarot cards, I Ching (Book of Changes) divination, and physiognomy (methods of reading the body to identify traits).\nIn Japan, divination methods include Futomani from the Shinto tradition.\nPersonality types.\nPersonality typing as a form of divination has been prevalent in Japan since the 1980s. Various methods exist for divining personality type. Each attempt to reveal glimpses of an individual's destiny, productive and inhibiting traits, future parenting techniques, and compatibility in marriage. Personality type is increasingly important for young Japanese, who consider personality the driving factor of compatibility, given the ongoing marriage drought and birth rate decline in Japan.\nAn import to Japan, Chinese zodiac signs based on the birth year in 12 year cycles (rat, ox, tiger, hare, dragon, snake, horse, sheep, monkey, cock, dog, and boar) are frequently combined with other forms of divination, such as so-called 'celestial types' based on the planets (Saturn, Venus, Mars, Jupiter, Mercury, or Uranus). Personality can also be divined using cardinal directions, the four elements (water, earth, fire, air), and yin-yang. Names can also lend important personality information under name classification which asserts that names bearing certain Japanese vowel sounds (a, i, u, e, o) share common characteristics. Numerology, which utilizes methods of divining 'birth numbers' from significant numbers such as birth date, may also reveal character traits of individuals.\nIndividuals can also assess their own and others' personalities according to physical characteristics. Blood type remains a popular form of divination from physiology. Stemming from Western influences, body reading or \"ninsou\", determines personality traits based on body measurements. The face is the most commonly analyzed feature, with eye size, pupil shape, mouth shape, and eyebrow shape representing the most important traits. An upturned mouth may be cheerful, and a triangle eyebrow may indicate that someone is strong-willed.\nMethods of assessment in daily life may include self-taken measurements or quizzes. As such, magazines targeted at women in their early-to-mid twenties feature the highest concentration of personality assessment guides. There are approximately 144 different women's magazines, known as \"nihon zashi koukoku kyoukai\", published in Japan aimed at this audience.\nJapanese tarot.\nThe adaptation of the Western divination method of tarot cards into Japanese culture presents a particularly unique example of contemporary divination as this adaptation mingles with Japan's robust visual culture. Japanese tarot cards are created by professional artists, advertisers, and fans of tarot. One tarot card collector claimed to have accumulated more than 1,500 Japan-made decks of tarot cards.\nJapanese tarot cards fall into diverse categories such as:\nThe images on tarot cards may come from images from Japanese popular culture, such as characters from manga and anime including Hello Kitty, or may feature cultural symbols. Tarot cards may adapt the images of Japanese historical figures, such as high priestess Himiko (170\u2013248CE) or imperial court wizard Abe no Seimei (921\u20131005CE). Still others may feature images of cultural displacement, such as English knights, pentagrams, the Jewish Torah, or invented glyphs. The introduction of such cards began by the 1930s and reached prominence 1970s. Japanese tarot cards were originally created by men, often based on the Rider-Waite-Smith tarot published by the Rider Company in London in 1909. Since, the practice of Japanese tarot has become overwhelmingly feminine and intertwined with kawaii culture. Referring to the cuteness of tarot cards, Japanese model Kuromiya Niina was quoted as saying \"because the images are cute, even holding them is enjoyable.\" While these differences exist, Japanese tarot cards function similarly to their Western counterparts. Cards are shuffled and cut into piles then used to forecast the future, for spiritual reflection, or as a tool for self-understanding.\nTaiwan.\nA common act of divination in Taiwan is called the Poe. \u201cThe Poe\u201d translated to English means \u201cmoon boards\u201d. It consists of two wood or bamboo blocks cut into the shape of a crescent moon. The one edge is rounded while the other is flat; the two are mirror images. Both crescents are held out in one's palms and while kneeling, they are raised to the forehead level. Once in this position, the blocks are dropped and the future can be understood depending on their landing. If both fall flat side up or both fall rounded side up, that can be taken as a failure of the deity to agree. If the blocks land one rounded and one flat, the deity indicates \"Yes\", or positive. \u201cLaughing poe\u201d is when rounded sides land down and they rock before coming to a standstill. \u201cNegative poe\u201d is when the flat sides fall downward and abruptly stop; this indicates \"No\". When there is a positive fall, it is called \u201cSacred poe\u201d, although the negative falls are not usually taken seriously. As the blocks are being dropped the question is said in a murmur, and if the answer is yes, the blocks are dropped again. To make sure the answer is definitely a yes, the blocks must fall in a \u201cyes\u201d position three times in a row.\nA more serious type of divination is the Ki\u014d-\u00e1. There is a small wooden chair, and around the sides of the chair are small pieces of wood that can move up and down in their sockets, this causes a clicking sounds when the chair is moved in any way. Two men hold this chair by its legs before an altar, while the incense is being burned, and the deity is invited to descend onto the chair. It is seen that it is in the chair by an onset of motion. Eventually, the chair crashes onto a table prepared with wood chips and burlap. The characters on the table are then traced and these are said to be written by the deity who possessed the chair, these characters are then interpreted for the devotees.\nContemporary divination in Africa.\nDivination is widespread throughout Africa. Among many examples it is one of the central tenets of Serer religion in Senegal. Only those who have been initiated as Saltigues (the Serer high priests and priestesses) can divine the future. These are the \"hereditary rain priests\" whose role is both religious and medicinal."}
{"id": "8693", "revid": "18707019", "url": "https://en.wikipedia.org/wiki?curid=8693", "title": "Diets of Nuremberg", "text": "The Diets of Nuremberg, also called the Imperial Diets of Nuremberg, took place at different times between the Middle Ages and the 17th century.\nThe first Diet of Nuremberg, in 1211, elected the future emperor Frederick II of Hohenstaufen as German king.\nAt the Diet of 1356 the Emperor Charles IV issued the Golden Bull of 1356, which required each Holy Roman Emperor to summon the first Imperial Diet after his election at Nuremberg. Apart from that, a number of other diets were held there.\nImportant to Protestantism were the Diets of 1522 (\"First Diet of Nuremberg\"), 1524 (\"Second Diet of Nuremberg\") and 1532 (\"Third Diet of Nuremberg\").\nThe 1522 Diet of Nuremberg.\nThis Diet has become known mostly for the reaction of the papacy to the decision made on Luther at the Diet of Worms the previous year. The new pope, Adrian VI, sent his nuncio Francesco Chieregati to the Diet, to insist both that the Edict of Worms be executed and that action be taken promptly against Luther. This demand, however, was coupled with a promise of thorough reform in the Roman hierarchy, frankly admitting the partial guilt of the Vatican in the decline of the Church.\nIn the recess drafted on 9 February 1523, however, the German princes rejected this appeal. Using Adrian's admissions, they declared that they could not have it appear 'as though they wished to oppress evangelical truth and assist unchristian and evil abuses.'\nThe 1524 Diet of Nuremberg.\nThis Diet generally took the same line as the previous one. The Estates reiterated their decision from the previous Diet. The Cardinal-legate, Campeggio, who was present, showed his disgust at the behaviour of the Estates. On 18 April, the Estates decided to call 'a general gathering of the German nation', to meet at Speyer the following year and to decide what would be done until the meeting of the general council of the Church which they demanded. This resulted in the Diet of Speyer (1526), which in turn was followed by the Diet of Speyer (1529). The latter included the Protestation at Speyer."}
{"id": "8695", "revid": "1398", "url": "https://en.wikipedia.org/wiki?curid=8695", "title": "Dr. Strangelove", "text": "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (known simply and more commonly as Dr. Strangelove) is a 1964 political satire black comedy film co-written, produced, and directed by Stanley Kubrick. It is loosely based on the thriller novel \"Red Alert\" (1958) by Peter George, who wrote the screenplay with Kubrick and Terry Southern. It stars Peter Sellers in three roles. The film, financed and released by Columbia Pictures, was a co-production between the United States and the United Kingdom.\n\"Dr. Strangelove\" parodies Cold War fears of a nuclear war between the United States and the Soviet Union and stars George C. Scott, Sterling Hayden, Keenan Wynn, Slim Pickens, and Tracy Reed. The story concerns an insane brigadier general of the United States Air Force who orders a pre-emptive nuclear attack on the Soviet Union. It follows the President of the United States (Sellers), his scientific advisor Dr. Strangelove (Sellers), the Joint Chiefs of Staff, and a Royal Air Force exchange officer (Sellers) as they attempt to stop the crew of a B-52 from bombing the Soviet Union and starting a nuclear war.\nThe film is considered one of the best comedy films and one of the greatest films of all time. In 1998, the American Film Institute ranked it 26th in its list of the best American films (in the 2007 edition, the film ranked 39th), and in 2000, it was listed as number three on its list of the funniest American films. In 1989, the United States Library of Congress included \"Dr. Strangelove\" as one of the first 25 films selected for preservation in the National Film Registry for being \"culturally, historically, or aesthetically significant\". The film received four Academy Award nominations, including Best Picture, Best Director, Best Adapted Screenplay, and Best Actor for Sellers. The film was also nominated for seven BAFTA Film Awards, winning Best Film From Any Source, Best British Film, and Best Art Direction (Black and White), and it also won the Hugo Award for Best Dramatic Presentation.\nPlot.\nUnited States Air Force Brigadier General Jack D. Ripper, the commander of Burpelson Air Force Base, orders his executive officer, Group Captain Lionel Mandrake (an exchange officer from the Royal Air Force), to put the base on alert (condition red, the most intense lockdown status), confiscate all privately owned radios from base personnel and issue \"Wing Attack Plan R\" to the planes of the 843rd Bomb Wing. At the time of issuance of said order, the planes, flying B-52 bombers armed with thermonuclear bombs, are on airborne alert two hours from their targets inside the Soviet Union. \nThe aircraft commence attack flights on the USSR and set their radios to allow communications only through their CRM 114 discriminators, which are designed to accept only communications preceded by a secret three-letter code known only to General Ripper. Happening upon a radio that had been missed earlier and hearing regular civilian broadcasting, Mandrake realizes that no attack order has been issued by the Pentagon and tries to stop Ripper, who locks them both in his office. Ripper tells Mandrake that he believes the Soviets have been fluoridating American water supplies to pollute the \"precious bodily fluids\" of Americans. Mandrake realizes Ripper has gone completely mad.\nIn the War Room at the Pentagon, General Buck Turgidson briefs President Merkin Muffley and other officers about how \"Plan R\" enables a senior officer to launch a retaliatory nuclear attack on the Soviets if all of his superior officers have been killed in a first strike on the United States. Trying every CRM code combination to issue a recall order would require two days, so Muffley orders the U.S. Army to storm the base and arrest General Ripper. Turgidson, noting the slim odds of recalling the planes in time, then proposes that Muffley not only let the attack proceed but send reinforcements. Muffley rejects Turgidson's recommendation and instead brings Soviet ambassador Alexei de Sadeski into the War Room to telephone Soviet Premier Dimitri Kissov. Muffley warns the premier of the impending attack and offers to reveal the targets, flight plans, and defensive systems of the bombers so that the Soviets can protect themselves.\nAfter a heated discussion with Kissov, the ambassador informs President Muffley that the Soviet Union created a doomsday machine as a nuclear deterrent; it consists of many buried cobalt bombs, which are set to detonate automatically should any nuclear attack strike the country. The resulting nuclear fallout would render the Earth's surface uninhabitable for 93 years. The device cannot be deactivated, as it is programmed to explode if any such attempt is made. The president's German scientific adviser, the paraplegic former Nazi Dr. Strangelove, points out that such a doomsday machine would only have been an effective deterrent if everyone knew about it; de Sadeski replies that Kissov had planned to reveal its existence to the world the following week at the Party Congress.\nWhen the U.S. Army troops gain control of Burpelson, General Ripper commits suicide. Mandrake deduces Ripper's CRM code from doodles on his desk blotter and relays it to the Pentagon. Using the code, Strategic Air Command successfully recalls all of the bombers except for one, commanded by Major T. J. \"King\" Kong. Because its radio equipment was damaged by a Soviet SAM, it is unable to receive or send communications. To conserve fuel, Kong flies below radar and switches targets, thus preventing Soviet air radar from detecting and intercepting their plane. Because the Soviet missile also damaged the bomb bay doors, Kong enters the bay and repairs the electrical wiring. When he is successful, the bomb drops with him straddling it. Kong joyously hoots and waves his cowboy hat as he rides the falling bomb to his death.\nIn the War Room, Dr. Strangelove recommends that the President gather several hundred thousand people to live in deep underground mines where the radiation will not penetrate. Worried that the Soviets will do the same, Turgidson warns about a \"mineshaft gap\" while de Sadeski secretly photographs the War Room. Dr. Strangelove prepares to announce his plan for that when he suddenly stands up out of his wheelchair and exclaims, \"Mein F\u00fchrer, I can walk!\" The movie ends with a montage of explosions set to \"We'll Meet Again\" signifying the activation of the doomsday device.\nCast.\nPeter Sellers's roles.\nColumbia Pictures agreed to finance the film if Peter Sellers played at least four major roles. The condition stemmed from the studio's opinion that much of the success of Kubrick's previous film \"Lolita\" (1962) was based on Sellers's performance, in which his single character assumes several identities. Sellers also played three roles in \"The Mouse That Roared\" (1959). Kubrick accepted the demand, later saying that \"such crass and grotesque stipulations are the \"sine qua non\" of the motion-picture business.\"\nSellers had been expected to play Air Force Major T. J. \"King\" Kong, the B-52 aircraft commander. Sellers was reluctant; he felt his workload was too heavy and worried he would not properly portray the character's Texan accent. Kubrick pleaded with him, and he asked the screenwriter Terry Southern (who had been raised in Texas) to record a tape with Kong's lines spoken in his accent, which he practiced using Southern's tapes. After the start of shooting in the aircraft, Sellers sprained his ankle and could no longer work in the cramped aircraft mockup.\nSellers improvised much of his dialogue, with Kubrick incorporating the ad-libs into the written screenplay, a practice known as retroscripting. \nGroup Captain Lionel Mandrake.\nAccording to film critic Alexander Walker, the author of biographies of both Sellers and Kubrick, the role of Group Captain Lionel Mandrake was the easiest of the three for Sellers to play, since he was aided by his experience of mimicking his superiors while serving in the RAF during World War II. There is also a heavy resemblance to Sellers's friend and occasional co-star Terry-Thomas and the prosthetic-limbed RAF flying ace Sir Douglas Bader.\nPresident Merkin Muffley.\nFor his performance as President Merkin Muffley, Sellers assumed a Midwestern American English accent. Sellers drew inspiration for the role from Adlai Stevenson, a former Illinois governor who was the Democratic candidate for the 1952 and 1956 presidential elections and the U.N. ambassador during the Cuban Missile Crisis.\nIn early takes, Sellers simulated cold symptoms to emphasize the character's apparent weakness. That caused frequent laughter among the film crew, ruining several takes. Kubrick ultimately found this comic portrayal inappropriate, feeling Muffley should be a serious character. In later takes, Sellers played the role straight, though the President's cold is still evident in several scenes.\nDr. Strangelove.\nDr. Strangelove is a scientist and former Nazi, suggesting Operation Paperclip, the US effort to recruit top German technical talent at the end of World War II. He serves as President Muffley's scientific adviser in the War Room. When General Turgidson wonders aloud to Mr. Staines (Jack Creley), what kind of name \"Strangelove\" is, possibly a \"Kraut name\", Staines responds that Strangelove's original German surname was \"Merkw\u00fcrdigliebe\" (\"strange love\" in German) and that \"he changed it when he became a citizen\". Strangelove accidentally addresses the president as \"Mein F\u00fchrer\" twice in the film. Dr. Strangelove did not appear in the book \"Red Alert\".\nThe character is an amalgamation of RAND Corporation strategist Herman Kahn, rocket scientist Wernher von Braun (a central figure in Nazi Germany's rocket development program recruited to the US after the war), and Edward Teller, the \"father of the hydrogen bomb\". Rumors claimed the character was based on Henry Kissinger, but Kubrick and Sellers denied this; Sellers said: \"Strangelove was never modeled after Kissinger\u2014that's a popular misconception. It was always Wernher von Braun.\" Furthermore, Henry Kissinger points out in his memoirs that at the time of the writing of \"Dr. Strangelove\", he was a little-known academic.\nThe wheelchair-using Strangelove furthers a Kubrick trope of the menacing, seated antagonist, first depicted in \"Lolita\" through the character Dr. Zaempf. Strangelove's accent was influenced by that of Austrian-American photographer Weegee, who worked for Kubrick as a special photographic effects consultant. Strangelove's appearance echoes the mad scientist archetype as seen in the character Rotwang in Fritz Lang's film \"Metropolis\" (1927). Sellers's Strangelove takes from Rotwang the single black gloved hand (which, in Rotwang's case, is mechanical because of a lab accident), the wild hair, and, most importantly, his ability to avoid being controlled by political power. According to Alexander Walker, Sellers improvised Dr. Strangelove's lapse into the Nazi salute, borrowing one of Kubrick's black leather gloves for the uncontrollable hand that makes the gesture. Dr. Strangelove apparently has alien hand syndrome. Kubrick wore the gloves on the set to avoid being burned when handling hot lights, and Sellers, recognizing the potential connection to Lang's work, found them to be menacing.\nSlim Pickens as Major T. J. \"King\" Kong.\nSlim Pickens, an established character actor and veteran of many Western films, was eventually chosen to replace Sellers as Major Kong after Sellers' injury. John Wayne was offered the role after Sellers was injured, but he never responded to Kubrick's offer. Dan Blocker of the \"Bonanza\" western television series was also approached to play the part, but according to Southern, Blocker's agent rejected the script as being \"too pinko\". Kubrick then recruited Pickens, whom he knew from his brief involvement in a Marlon Brando western film project that was eventually filmed as \"One-Eyed Jacks\".\nHis fellow actor James Earl Jones recalls, \"He was Major Kong on and off the set\u2014he didn't change a thing\u2014his temperament, his language, his behavior.\" Pickens was not told that the movie was a black comedy, and he was only given the script for scenes he was in to get him to play it \"straight\".\nKubrick's biographer John Baxter explained, in the documentary \"Inside the Making of Dr. Strangelove\":\nPickens, who had previously played only supporting and character roles, said that his appearance as Maj. Kong greatly improved his career. He later commented, \"After \"Dr. Strangelove\", my salary jumped five times, and assistant directors started saying 'Hey, Slim' instead of 'Hey, you'.\"\nGeorge C. Scott as General Buck Turgidson.\nGeorge C. Scott played the role of General Buck Turgidson, the Chairman of the Joint Chiefs of Staff. In this capacity General Turgidson was the nation's highest-ranking military officer and the principal military adviser to the president and the National Security Council. He is seen during most of the movie advising President Muffley on the best steps to take in order to stop the fleet of B-52 Stratofortresses that was deployed by Brigadier General Jack D. Ripper to drop nuclear bombs on Soviet soil.\nAccording to James Earl Jones, Kubrick tricked Scott into playing the role of Gen. Turgidson in a much more outlandish manner than Scott was comfortable doing. According to Jones, Kubrick talked Scott into doing absurd \"practice\" takes, which Kubrick told Scott would never be used, as a way to warm up for the \"real\" takes. According to Jones, Kubrick used these takes in the final film, rather than the more restrained ones, allegedly causing Scott to swear never to work with Kubrick again.\nDuring the filming, Kubrick and Scott had different opinions regarding certain scenes, but Kubrick obtained Scott's compliance largely by beating him at chess, which they played frequently on the set.\nProduction.\nNovel and screenplay.\nStanley Kubrick started with nothing but a vague idea to make a thriller about a nuclear accident that built on the widespread Cold War fear for survival. While doing research, Kubrick gradually became aware of the subtle and paradoxical \"balance of terror\" between nuclear powers. At Kubrick's request, Alastair Buchan (the head of the Institute for Strategic Studies) recommended the thriller novel \"Red Alert\" by Peter George. Kubrick was impressed with the book, which had also been praised by game theorist and future Nobel Prize in Economics winner Thomas Schelling in an article written for the \"Bulletin of the Atomic Scientists\" and reprinted in \"The Observer\", and immediately bought the film rights. In 2006, Schelling wrote that conversations between Kubrick, Schelling, and George in late 1960 about a treatment of \"Red Alert\" updated with intercontinental missiles eventually led to the making of the film.\nIn collaboration with George, Kubrick started writing a screenplay based on the book. While writing the screenplay, they benefited from some brief consultations with Schelling and later, Herman Kahn. In following the tone of the book, Kubrick originally intended to film the story as a serious drama. However, he began to see comedy inherent in the idea of mutual assured destruction as he wrote the first draft. He later said:\nAmong the titles that Kubrick considered for the film were \"Dr. Doomsday or: How to Start World War III Without Even Trying\", \"Dr. Strangelove's Secret Uses of Uranus\", and \"Wonderful Bomb\". After deciding to make the film a black comedy, Kubrick brought in Terry Southern as a co-writer in late 1962. The choice was influenced by reading Southern's comic novel \"The Magic Christian\", which Kubrick had received as a gift from Peter Sellers, and which itself became a Sellers film in 1969. Southern made important contributions to the film, but his role led to a rift between Kubrick and Peter George; after \"Life\" magazine published a photo-essay on Southern in August 1964 which implied that Southern had been the script's principal author\u2014a misperception neither Kubrick nor Southern did much to dispel\u2014 George wrote a letter to the magazine, published in its September 1964 issue, in which he pointed out that he had both written the film's source novel and collaborated on various incarnations of the script over a period of ten months, whereas \"Southern was briefly employed ... to do some additional rewriting for Kubrick and myself and fittingly received a screenplay credit in behind Mr. Kubrick and myself.\"\nSets and filming.\n\"Dr. Strangelove\" was filmed at Shepperton Studios, near London, as Sellers was in the middle of a divorce at the time and unable to leave England. The sets occupied three main sound stages: the Pentagon War Room, the B-52 Stratofortress bomber and the last one containing both the motel room and General Ripper's office and outside corridor. The studio's buildings were also used as the Air Force base exterior. The film's set design was done by Ken Adam, the production designer of several \"James Bond\" films (at the time he had already worked on \"Dr. No\"). The black-and-white cinematography was by Gilbert Taylor, and the film was edited by Anthony Harvey and an uncredited Kubrick. The original musical score for the film was composed by Laurie Johnson, and the special effects were done by Wally Veevers. The opening theme is an instrumental version of \"Try a Little Tenderness\". The theme of the chorus from the bomb run scene is a modification of \"When Johnny Comes Marching Home\". Sellers and Kubrick got along well during the film's production and shared a love of photography.\nFor the War Room, Ken Adam first designed a two-level set which Kubrick initially liked, only to decide later that it was not what he wanted. Adam next began work on the design that was used in the film, an expressionist set that was compared with \"The Cabinet of Dr. Caligari\" and Fritz Lang's \"Metropolis\". It was an enormous concrete room ( long and wide, with a -high ceiling) suggesting a bomb shelter, with a triangular shape (based on Kubrick's idea that this particular shape would prove the most resistant against an explosion). One side of the room was covered with gigantic strategic maps reflecting in a shiny black floor inspired by dance scenes in Fred Astaire films. In the middle of the room there was a large circular table lit from above by a circle of lamps, suggesting a poker table. Kubrick insisted that the table would be covered with green baize (although this could not be seen in the black-and-white film) to reinforce the actors' impression that they are playing 'a game of poker for the fate of the world.' Kubrick asked Adam to build the set ceiling in concrete to force the director of photography to use only the on-set lights from the circle of lamps. Moreover, each lamp in the circle of lights was carefully placed and tested until Kubrick was happy with the result.\nLacking cooperation from the Pentagon in the making of the film, the set designers reconstructed the aircraft cockpit to the best of their ability by comparing the cockpit of a B-29 Superfortress and a single photograph of the cockpit of a B-52 and relating this to the geometry of the B-52's fuselage. The B-52 was state-of-the-art in the 1960s, and its cockpit was off-limits to the film crew. When some United States Air Force personnel were invited to view the reconstructed B-52 cockpit, they said that \"it was absolutely correct, even to the little black box which was the CRM.\" It was so accurate that Kubrick was concerned about whether Adam's team had carried out all its research legally.\nIn several shots of the B-52 flying over the polar ice en route to Russia, the shadow of the actual camera plane, a Boeing B-17 Flying Fortress, is visible on the icecap below. The B-52 was a scale model composited into the Arctic footage, which was sped up to create a sense of jet speed. Home movie footage included in \"Inside the Making of Dr. Strangelove\" on the 2001 Special Edition DVD release of the film shows clips of the B-17 with a cursive \"Dr. Strangelove\" painted over the rear entry hatch on the right side of the fuselage.\nIn 1967, some of the flying footage from \"Dr. Strangelove\" was re-used in The Beatles' television film \"Magical Mystery Tour\". As told by editor Roy Benson in the BBC radio documentary \"Celluloid Beatles\", the production team of \"Magical Mystery Tour\" lacked footage to cover the sequence for the song \"Flying\". Benson had access to the aerial footage filmed for the B-52 sequences of \"Dr. Strangelove\", which was stored at Shepperton Studios. The use of the footage prompted Kubrick to call Benson to complain.\n\"Fail Safe\".\n\"Red Alert\" author Peter George collaborated on the screenplay with Kubrick and satirist Terry Southern. \"Red Alert\" was more solemn than its film version, and it did not include the character Dr. Strangelove, though the main plot and technical elements were quite similar. A novelization of the actual film, rather than a reprint of the original novel, was published by Peter George, based on an early draft in which the narrative is bookended by the account of aliens, who, having arrived at a desolated Earth, try to piece together what has happened. It was reissued in October 2015 by Candy Jar Books, featuring never-before-published material on Strangelove's early career.\nDuring the filming of \"Dr. Strangelove\", Stanley Kubrick learned that \"Fail Safe\", a film with a similar theme, was being produced. Although \"Fail Safe\" was to be an ultrarealistic thriller, Kubrick feared that its plot resemblance would damage his film's box office potential, especially if it were released first. Indeed, the novel \"Fail-Safe\" (on which the film is based) is so similar to \"Red Alert\" that Peter George sued on charges of plagiarism and settled out of court.\nWhat worried Kubrick the most was that \"Fail Safe\" boasted the acclaimed director Sidney Lumet and the first-rate dramatic actors Henry Fonda as the American president and Walter Matthau as the adviser to the Pentagon, Professor Groeteschele. Kubrick decided to throw a legal wrench into \"Fail Safe\"s production gears. Lumet recalled in the documentary \"Inside the Making of Dr. Strangelove\": \"We started casting. Fonda was already set\u00a0... which of course meant a big commitment in terms of money. I was set, Walter [Bernstein, the screenwriter] was set\u00a0... And suddenly, this lawsuit arrived, filed by Stanley Kubrick and Columbia Pictures.\"\nKubrick argued that \"Fail Safe\"s own source novel \"Fail-Safe\" (1962) had been plagiarized from Peter George's \"Red Alert\", to which Kubrick owned creative rights. He pointed out unmistakable similarities in intentions between the characters Groeteschele and Strangelove. The plan worked, and the suit was settled out of court, with the agreement that Columbia Pictures, which had financed and was distributing \"Strangelove\", also buy \"Fail Safe\", which had been an independently financed production. Kubrick insisted that the studio release his movie first, and \"Fail Safe\" opened eight months after \"Dr. Strangelove\", to critical acclaim but mediocre ticket sales.\nEnding.\nThe end of the film shows Dr. Strangelove exclaiming, \"\"Mein F\u00fchrer,\" I can walk!\" before cutting to footage of nuclear explosions, with Vera Lynn and her audience singing \"We'll Meet Again\". This footage comes from nuclear tests such as shot \"Baker\" of Operation Crossroads at Bikini Atoll, the Trinity test, a test from Operation Sandstone and the hydrogen bomb tests from Operation Redwing and Operation Ivy. In some shots, old warships (such as the German heavy cruiser \"Prinz Eugen\"), which were used as targets, are plainly visible. In others, the smoke trails of rockets used to create a calibration backdrop can be seen. \"Goon Show\" writer and friend of Sellers Spike Milligan was credited with suggesting Vera Lynn's song for the ending.\nOriginal ending.\nIt was originally planned for the film to end with a scene that depicted everyone in the War Room involved in a pie fight. Accounts vary as to why the pie fight was cut. In a 1969 interview, Kubrick said, \"I decided it was farce and not consistent with the satiric tone of the rest of the film.\" Critic Alexander Walker observed that \"the cream pies were flying around so thickly that people lost definition, and you couldn't really say whom you were looking at.\" Nile Southern, son of screenwriter Terry Southern, suggested the fight was intended to be less jovial: \"Since they were laughing, it was unusable, because instead of having that totally black, which would have been amazing, like, this blizzard, which in a sense is metaphorical for all of the missiles that are coming, as well, you just have these guys having a good old time. So, as Kubrick later said, 'it was a disaster of Homeric proportions.\nEffects of the Kennedy assassination on the film.\nA first test screening of the film was scheduled for November 22, 1963, the day of the assassination of John F. Kennedy. The film was just weeks from its scheduled premiere, but because of the assassination, the release was delayed until late January 1964, as it was felt that the public was in no mood for such a film any sooner.\nDuring post-production, one line by Slim Pickens, \"a fella could have a pretty good weekend in Dallas with all that stuff\", was dubbed to change \"Dallas\" to \"Vegas\", since Dallas was where Kennedy was killed. The original reference to Dallas survives in the English audio of the French-subtitled version of the film.\nThe assassination also serves as another possible reason that the pie-fight scene was cut. In the scene, after Muffley takes a pie in the face, General Turgidson exclaims: \"Gentlemen! Our gallant young president has been struck down in his prime!\" Editor Anthony Harvey stated that the scene \"would have stayed, except that Columbia Pictures were horrified, and thought it would offend the president's family.\" Kubrick and others have said that the scene had already been cut before preview night because it was inconsistent with the rest of the film.\nRe-release in 1994.\nIn 1994, the film was re-released. While the 1964 release used a 1.85:1 aspect ratio, the new print was in the slightly squarer 1.66:1 (5:3) ratio that Kubrick had originally intended.\nThemes.\nSatirizing the Cold War.\n\"Dr. Strangelove\" ridicules nuclear war planning. It mocks numerous contemporary Cold War attitudes such as the \"missile gap\" but it primarily directs its satire on the theory of mutually assured destruction (MAD), in which each side is supposed to be deterred from a nuclear war by the prospect of a universal cataclysm regardless of who \"won\". Military strategist and former physicist Herman Kahn, in the book \"On Thermonuclear War\" (1960), used the theoretical example of a \"doomsday machine\" to illustrate the limitations of MAD, which was developed by John von Neumann.\nThe concept of such a machine is consistent with MAD doctrine when it is logically pursued to its conclusion. It thus worried Kahn that the military might like the idea of a doomsday machine and build one. Kahn, a leading critic of MAD and the Eisenhower administration's doctrine of massive retaliation upon the slightest provocation by the USSR, considered MAD to be foolish bravado, and urged the United States to instead plan for proportionality, and thus even a limited nuclear war. With this reasoning, Kahn became one of the architects of the flexible response doctrine which, while superficially resembling MAD, allowed for the possibility of responding to a limited nuclear strike with a proportional, or calibrated, return of fire (see \"Conflict escalation\").\nKahn educated Kubrick on the concept of the semi-realistic \"cobalt-thorium G\" doomsday machine, and then Kubrick used the concept for the film. Kahn in his writings and talks would often come across as cold and calculating, for example, with his use of the term \"megadeaths\" and in his willingness to estimate how many human lives the United States could lose and still rebuild economically. Kahn's dispassionate attitude towards millions of deaths is reflected in Turgidson's remark to the president about the outcome of a preemptive nuclear war: \"Mr. President, I'm not saying we wouldn't get our hair mussed. But I do say no more than ten to twenty million killed, tops, uh, depending on the breaks.\" Turgidson has a binder that is labelled \"World Targets in Megadeaths\", a term coined in 1953 by Kahn and popularized in his 1960 book \"On Thermonuclear War\".\nThe fallout-shelter-network proposal mentioned in the film, with its inherently high radiation protection characteristics, has similarities and contrasts to that of the real Swiss civil defense network. Switzerland has an overcapacity of nuclear fallout shelters for the country's population size, and by law, new homes must still be built with a fallout shelter. If the US did that, it would violate the spirit of MAD and, according to MAD adherents, allegedly destabilize the situation because the US could launch a first strike and its population would largely survive a retaliatory second strike (see MAD \u00a7 Theory).\nTo rebut early 1960s novels and Hollywood films like \"Fail-Safe\" and \"Dr. Strangelove\", which raised questions about US control over nuclear weapons, the Air Force produced a documentary film, \"SAC Command Post\", to demonstrate its responsiveness to presidential command and its tight control over nuclear weapons. However, later academic research into declassified documents showed that U.S. military commanders had been given presidentially authorized pre-delegation for the use of nuclear weapons during the early Cold War, showing that this aspect of the film's plot was plausible.\nThe characters of Buck Turgidson and Jack D. Ripper both satirize the real-life Gen. Curtis LeMay of the Strategic Air Command.\nSexual themes.\nIn the months following the film's release, director Stanley Kubrick received a fan letter from Legrace G. Benson of the Department of History of Art at Cornell University interpreting the film as being sexually layered. The director wrote back to Benson and confirmed the interpretation, \"Seriously, you are the first one who seems to have noticed the sexual framework from intromission (the planes going in) to the last spasm (Kong's ride down and detonation at target).\"\nRelease.\nThe film was a popular success, earning US$4,420,000 in rentals in North America during its initial theatrical release.\nReception.\nCritical response.\n\"Dr. Strangelove\" is Kubrick's highest-rated film on Rotten Tomatoes, holding a 98% approval rating based on 96 reviews, with an average rating of 9.1/10. The site's summary states that \"Stanley Kubrick's brilliant Cold War satire remains as funny and razor-sharp today as it was in 1964.\" The film also holds a score of 97 out of 100 on Metacritic, based on 32 reviews, indicating \"universal acclaim\". The film is ranked number 7 in the All-Time High Scores chart of Metacritic's Video/DVD section. It was selected for preservation in the United States National Film Registry.\n\"Dr. Strangelove\" is on Roger Ebert's list of \"The Great Movies\", and he described it as \"arguably the best political satire of the century\". One of the most celebrated of all film comedies, in 1998, \"Time Out\" conducted a reader's poll and \"Dr. Strangelove\" was voted the 47th greatest film of all time. \"Entertainment Weekly\" voted it at No. 14 on their list of \"100 Greatest Movies of All Time\". in 2002, it was ranked as the 5th best film in \"Sight &amp; Sound\" poll of best films. John Patterson of \"The Guardian\" wrote, \"There had been nothing in comedy like \"Dr Strangelove\" ever before. All the gods before whom the America of the stolid, paranoid 50s had genuflected\u2014the Bomb, the Pentagon, the National Security State, the President himself, Texan masculinity and the alleged Commie menace of water-fluoridation\u2014went into the wood-chipper and never got the same respect ever again.\" It is also listed as number 26 on \"Empire's 500 Greatest Movies of All Time\", and in 2010 it was listed by \"Time\" magazine as one of the 100 best films since the publication's inception in 1923. The Writers Guild of America ranked its screenplay the 12th best ever written.\nIn 2000, readers of \"Total Film\" magazine voted it the 24th greatest comedic film of all time. The film ranked 42nd in the BBC's 2015 list of the 100 greatest American films. The film was selected as the 2nd best comedy of all time in a poll of 253 film critics from 52 countries conducted by the BBC in 2017.\nStudio response.\nColumbia Pictures' early reaction to \"Dr. Strangelove\" was anything but enthusiastic. In \"Notes From The War Room\", in the summer 1994 issue of \"Grand Street\" magazine, co-screenwriter Terry Southern recalled that, as production neared the end, \"It was about this time that word began to reach us, reflecting concern as to the nature of the film in production. Was it anti-American? Or just anti-military? And the jackpot question: Was it, in fact, anti-American to whatever extent it was anti-military?\"\nSouthern recalled how Kubrick grew concerned about seeming apathy and distancing by studio heads Abe Schneider and Mo Rothman, and by Columbia's characterization of the film as \"just a zany, novelty flick which did not reflect the views of the corporation in any way.\" Southern noted that Rothman was in \"prominent attendance\" at a ceremony in 1989 when the Library of Congress announced it as one of the first 25 films on the National Film Registry.\nAccolades.\nThe film ranked No. 32 on \"TV Guide\"s list of the 50 Greatest Movies on TV (and Video).\nAmerican Film Institute included the film as #26 in AFI's 100 Years...100 Movies, #3 in AFI's 100 Years...100 Laughs, #64 in AFI's 100 Years...100 Movie Quotes (\"Gentlemen, you can't fight in here! This is the War Room!\") and #39 in AFI's 100 Years...100 Movies (10th Anniversary Edition).\nCanceled sequel.\nIn 1995, Kubrick enlisted Terry Southern to script a sequel titled \"Son of Strangelove\". Kubrick had Terry Gilliam in mind to direct. The script was never completed, but index cards laying out the story's basic structure were found among Southern's papers after he died in October 1995. It was set largely in underground bunkers, where Dr. Strangelove had taken refuge with a group of women.\nIn 2013, Gilliam commented, \"I was told after Kubrick died\u2014by someone who had been dealing with him\u2014that he had been interested in trying to do another \"Strangelove\" with me directing. I never knew about that until after he died but I would have loved to.\"\nStage adaptation.\nOn July 14, 2023, it was announced that a stage adaptation of the film would be produced, co-adapted by Armando Iannucci and Sean Foley and starring Steve Coogan. It premiered in London's West End at the Noel Coward Theatre in October 2024. It is the first stage adaptation of Kubrick's works.\nExternal links.\nPapers\nMetadata"}
{"id": "8697", "revid": "1272233562", "url": "https://en.wikipedia.org/wiki?curid=8697", "title": "DNA ligase", "text": "DNA ligase is a type of enzyme that facilitates the joining of DNA strands together by catalyzing the formation of a phosphodiester bond. It plays a role in repairing single-strand breaks in duplex DNA in living organisms, but some forms (such as DNA ligase IV) may specifically repair double-strand breaks (i.e. a break in both complementary strands of DNA). Single-strand breaks are repaired by DNA ligase using the complementary strand of the double helix as a template, with DNA ligase creating the final phosphodiester bond to fully repair the DNA.\nDNA ligase is used in both DNA repair and DNA replication (see \"Mammalian ligases\"). In addition, DNA ligase has extensive use in molecular biology laboratories for recombinant DNA experiments (see \"Research applications\"). Purified DNA ligase is used in gene cloning to join DNA molecules together to form recombinant DNA.\nEnzymatic mechanism.\nThe mechanism of DNA ligase is to form two covalent phosphodiester bonds between 3' hydroxyl ends of one nucleotide (\"acceptor\"), with the 5' phosphate end of another (\"donor\"). Two ATP molecules are consumed for each phosphodiester bond formed. AMP is required for the ligase reaction, which proceeds in four steps:\nLigase will also work with blunt ends, although higher enzyme concentrations and different reaction conditions are required.\nTypes.\n\"E. coli\".\nThe \"E. coli\" DNA ligase is encoded by the \"lig\" gene. DNA ligase in \"E. coli\", as well as most prokaryotes, uses energy gained by cleaving nicotinamide adenine dinucleotide (NAD) to create the phosphodiester bond. It does not ligate blunt-ended DNA except under conditions of molecular crowding with polyethylene glycol, and cannot join RNA to DNA efficiently.\nThe activity of E. coli DNA ligase can be enhanced by DNA polymerase at the right concentrations. Enhancement only works when the concentrations of the DNA polymerase 1 are much lower than the DNA fragments to be ligated. When the concentrations of Pol I DNA polymerases are higher, it has an adverse effect on E. coli DNA ligase\nT4.\nThe DNA ligase from bacteriophage T4 (a bacteriophage that infects \"Escherichia coli\" bacteria). The T4 ligase is the most-commonly used in laboratory research. It can ligate either cohesive or blunt ends of DNA, oligonucleotides, as well as RNA and RNA-DNA hybrids, but not single-stranded nucleic acids. It can also ligate blunt-ended DNA with much greater efficiency than \"E. coli\" DNA ligase. Unlike \"E. coli\" DNA ligase, T4 DNA ligase cannot utilize NAD and it has an absolute requirement for ATP as a cofactor. Some engineering has been done to improve the \"in vitro\" activity of T4 DNA ligase; one successful approach, for example, tested T4 DNA ligase fused to several alternative DNA binding proteins and found that the constructs with either p50 or NF-kB as fusion partners were over 160% more active in blunt-end ligations for cloning purposes than wild type T4 DNA ligase. A typical reaction for inserting a fragment into a plasmid vector would use about 0.01 (sticky ends) to 1 (blunt ends) units of ligase. The optimal incubation temperature for T4 DNA ligase is 37\u00a0\u00b0C, a temperature at which T4 enzymes are most active. However, it is not uncommon to setup ligation reactions at 16\u00a0\u00b0C, a trade-off temperature at which the ligase is active as well as one that is suitable for base-pairing of sticky ends. \nBacteriophage T4 ligase mutants have increased sensitivity to both UV irradiation and the alkylating agent methyl methanesulfonate indicating that DNA ligase is employed in the repair of the DNA damages caused by these agents.\nMammalian.\nIn mammals, there are four specific types of ligase.\nDNA ligase from eukaryotes and some microbes uses adenosine triphosphate (ATP) rather than NAD.\nThermostable.\nDerived from a thermophilic bacterium, the enzyme is stable and active at much higher temperatures than conventional DNA ligases. Its half-life is 48 hours at 65\u00a0\u00b0C and greater than 1\u00a0hour at 95\u00a0\u00b0C. Ampligase DNA Ligase has been shown to be active for at least 500 thermal cycles (94\u00a0\u00b0C/80\u00a0\u00b0C) or 16 hours of cycling.10\u00a0This exceptional thermostability permits extremely high hybridization stringency and ligation specificity.\nMeasurement of activity.\nThere are at least three different units used to measure the activity of DNA ligase:\nResearch applications.\nDNA ligases have become indispensable tools in modern molecular biology research for generating recombinant DNA sequences. For example, DNA ligases are used with restriction enzymes to insert DNA fragments, often genes, into plasmids.\nControlling the optimal temperature is a vital aspect of performing efficient recombination experiments involving the ligation of cohesive-ended fragments. Most experiments use T4 DNA Ligase (isolated from bacteriophage T4), which is most active at 37\u00a0\u00b0C. However, for optimal ligation efficiency with cohesive-ended fragments (\"sticky ends\"), the optimal enzyme temperature needs to be balanced with the melting temperature Tm of the sticky ends being ligated, the homologous pairing of the sticky ends will not be stable because the high temperature disrupts hydrogen bonding. A ligation reaction is most efficient when the sticky ends are already stably annealed, and disruption of the annealing ends would therefore result in low ligation efficiency. The shorter the overhang, the lower the Tm.\nSince blunt-ended DNA fragments have no cohesive ends to anneal, the melting temperature is not a factor to consider within the normal temperature range of the ligation reaction. The limiting factor in blunt end ligation is not the activity of the ligase but rather the number of alignments between DNA fragment ends that occur. The most efficient ligation temperature for blunt-ended DNA would therefore be the temperature at which the greatest number of alignments can occur. The majority of blunt-ended ligations are carried out at 14-25\u00a0\u00b0C overnight. The absence of stably annealed ends also means that the ligation efficiency is lowered, requiring a higher ligase concentration to be used.\nA novel use of DNA ligase can be seen in the field of nano chemistry, specifically in DNA origami. \u00a0DNA based self-assembly principles have proven useful for organizing nanoscale objects, such as biomolecules, nanomachines, nanoelectronic and photonic component. Assembly of such nano structure requires the creation of an intricate mesh of DNA molecules. Although DNA self-assembly is possible without any outside help using different substrates such as provision of catatonic surface of Aluminium foil, DNA ligase can provide the enzymatic assistance that is required to make DNA lattice structure from DNA over hangs.\nHistory.\nThe first DNA ligase was purified and characterized in 1967 by the Gellert, Lehman, Richardson, and Hurwitz laboratories. It was first purified and characterized by Weiss and Richardson using a six-step chromatographic-fractionation process beginning with elimination of cell debris and addition of streptomycin, followed by several Diethylaminoethyl (DEAE)-cellulose column washes and a final phosphocellulose fractionation. The final extract contained 10% of the activity initially recorded in the\u00a0\"E. coli\u00a0\"media; along the process it was discovered that ATP and Mg++ were necessary to optimize the reaction. The common commercially available DNA ligases were originally discovered in bacteriophage T4, \"E. coli\" and other bacteria.\nDisorders.\nGenetic deficiencies in human DNA ligases have been associated with clinical syndromes marked by immunodeficiency, radiation sensitivity, and developmental abnormalities,\u00a0 LIG4 syndrome (Ligase IV syndrome) is a rare disease associated with mutations in DNA ligase 4 and interferes with dsDNA break-repair mechanisms. Ligase IV syndrome causes immunodeficiency in individuals and is commonly associated with microcephaly and marrow hypoplasia. A list of prevalent diseases caused by lack of or malfunctioning of DNA ligase is as follows.\nXeroderma pigmentosum.\nXeroderma pigmentosum, which is commonly known as XP, is an inherited condition characterized by an extreme sensitivity to ultraviolet (UV) rays from sunlight. This condition mostly affects the eyes and areas of skin exposed to the sun. Some affected individuals also have problems involving the nervous system.\nAtaxia-telangiectasia.\nMutations in the\u00a0ATM\u00a0gene cause\u00a0ataxia\u2013telangiectasia. The\u00a0ATM\u00a0gene provides instructions for making a protein that helps control\u00a0cell\u00a0division\u00a0and is involved in DNA repair. This protein plays an important role in the normal development and activity of several body systems, including the nervous system and immune system. The ATM protein assists cells in recognizing damaged or broken DNA strands and coordinates DNA repair by activating enzymes that fix the broken strands. Efficient repair of damaged DNA strands helps maintain the stability of the cell's genetic information. Affected children typically develop difficulty walking, problems with balance and hand coordination, involuntary jerking movements (chorea), muscle twitches (myoclonus), and disturbances in nerve function (neuropathy). The movement problems typically cause people to require wheelchair assistance by adolescence. People with this disorder also have slurred speech and trouble moving their eyes to look side-to-side (oculomotor apraxia).\nFanconi Anemia.\nFanconi anemia (FA) is a rare, inherited blood disorder that leads to bone marrow failure. FA prevents bone marrow from making enough new blood cells for the body to work normally. FA also can cause the bone marrow to make many faulty blood cells. This can lead to serious health problems, such as leukemia.\nBloom syndrome.\nBloom syndrome results in skin that is sensitive to sun exposure, and usually the development of a butterfly-shaped patch of reddened skin across the nose and cheeks. A skin rash can also appear on other areas that are typically exposed to the sun, such as the back of the hands and the forearms. Small clusters of enlarged blood vessels (telangiectases) often appear in the rash; telangiectases can also occur in the eyes. Other skin features include patches of skin that are lighter or darker than the surrounding areas (hypopigmentation or hyperpigmentation respectively). These patches appear on areas of the skin that are not exposed to the sun, and their development is not related to the rashes.\nAs a drug target.\nIn recent studies, human DNA ligase I was used in Computer-aided drug design to identify DNA ligase inhibitors as possible therapeutic agents to treat cancer. Since excessive cell growth is a hallmark of cancer development, targeted chemotherapy that disrupts the functioning of DNA ligase can impede adjuvant cancer forms. Furthermore, it has been shown that DNA ligases can be broadly divided into two categories, namely, ATP- and NAD+-dependent. Previous research has shown that although NAD+-dependent DNA ligases have been discovered in sporadic cellular or viral niches outside the bacterial domain of life, there is no instance in which a NAD+-dependent ligase is present in a eukaryotic organism. The presence solely in non-eukaryotic organisms, unique substrate specificity, and distinctive domain structure of NAD+ dependent compared with ATP-dependent human DNA ligases together make NAD+-dependent ligases ideal targets for the development of new antibacterial drugs."}
{"id": "8699", "revid": "35131960", "url": "https://en.wikipedia.org/wiki?curid=8699", "title": "Dewey Decimal Classification", "text": "The Dewey Decimal Classification (DDC), colloquially known as the Dewey Decimal System, is a proprietary library classification system which allows new books to be added to a library in their appropriate location based on subject. \nIt was first published in the United States by Melvil Dewey in 1876. Originally described in a 44-page pamphlet, it has been expanded to multiple volumes and revised through 23 major editions, the latest printed in 2011. It is also available in an abridged version suitable for smaller libraries. OCLC, a non-profit cooperative that serves libraries, currently maintains the system and licenses online access to WebDewey, a continuously updated version for catalogers.\nThe decimal number classification introduced the concepts of \"relative location\" and \"relative index\". Libraries previously had given books permanent shelf locations that were related to the order of acquisition rather than topic. The classification's notation makes use of three-digit numbers for main classes, with fractional decimals allowing expansion for further detail. Numbers are flexible to the degree that they can be expanded in linear fashion to cover special aspects of general subjects. A library assigns a classification number that unambiguously locates a particular volume in a position relative to other books in the library, on the basis of its subject. The number makes it possible to find any book and to return it to its proper place on the library shelves. The classification system is used in 200,000 libraries in at least 135 countries.\nHistory.\n1873\u20131885: early development.\nMelvil Dewey (1851\u20131931) was an American librarian and self-declared reformer. He was a founding member of the American Library Association and can be credited with the promotion of card systems in libraries and business. He developed the ideas for his library classification system in 1873 while working at the Amherst College library. He applied the classification to the books in that library, until in 1876 he had a first version of the classification. In 1876, he published the classification in pamphlet form with the title \"A Classification and Subject Index for Cataloguing and Arranging the Books and Pamphlets of a Library.\"\nHe used the pamphlet, published in more than one version during the year, to solicit comments from other librarians. It is not known who received copies or how many commented as only one copy with comments has survived, that of Ernest Cushing Richardson. His classification system was mentioned in an article in the first issue of the \"Library Journal\" and in an article by Dewey in the Department of Education publication \"Public Libraries in America\" in 1876. In March 1876, he applied for, and received, copyright on the first edition of the index. The edition was 44 pages in length, with 2,000 index entries, and was printed in 200 copies.\n1885\u20131942: period of adoption.\nThe second edition of the Dewey Decimal system, published in 1885 with the title \"\", comprised 314 pages, with 10,000 index entries. Five hundred copies were produced. Editions 3\u201314, published between 1888 and 1942, used a variant of this same title. Dewey modified and expanded his system considerably for the second edition. In an introduction to that edition Dewey states that \"nearly 100 persons hav [spelling of 'have' per English-language spelling reform, which Dewey championed] contributed criticisms and suggestions\".\nOne of the innovations of the Dewey Decimal system was that of positioning books on the shelves in relation to other books on similar topics. When the system was first introduced, most libraries in the US used fixed positioning: each book was assigned a permanent shelf position based on the book's height and date of acquisition. Library stacks were generally closed to all but the most privileged patrons, so shelf browsing was not considered of importance. The use of the Dewey Decimal system increased during the early 20th century as librarians were convinced of the advantages of relative positioning and of open shelf access for patrons.\nNew editions were readied as supplies of previously published editions were exhausted, even though some editions provided little change from the previous, as they were primarily needed to fulfill demand. In the next decade, three editions followed closely on: the 3rd (1888), 4th (1891), and 5th (1894). Editions 6 through 11 were published from 1899 to 1922. The 6th edition was published in a record 7,600 copies, although subsequent editions were much lower. During this time, the size of the volume grew, and edition 12 swelled to 1,243 pages, an increase of 25% over the previous edition.\nIn response to the needs of smaller libraries which were finding the expanded classification schedules difficult to use, in 1894, the first abridged edition of the Dewey Decimal system was produced. The abridged edition generally parallels the full edition, and has been developed for most full editions since that date. By popular request, in 1930, the Library of Congress began to print Dewey Classification numbers on nearly all of its cards, thus making the system immediately available to all libraries making use of the Library of Congress card sets.\nDewey's was not the only library classification available, although it was the most complete. Charles Ammi Cutter published the Expansive Classification in 1882, with initial encouragement from Melvil Dewey. Cutter's system was not adopted by many libraries, with one major exception: it was used as the basis for the Library of Congress Classification system.\nIn 1895, the International Institute of Bibliography, located in Belgium and led by Paul Otlet, contacted Dewey about the possibility of translating the classification into French, and using the classification system for bibliographies (as opposed to its use for books in libraries). This would have required some changes to the classification, which was under copyright. Dewey gave permission for the creation of a version intended for bibliographies, and also for its translation into French. Dewey did not agree, however, to allow the International Institute of Bibliography to later create an English version of the resulting classification, considering that a violation of their agreement, as well as a violation of Dewey's copyright. Shortly after Dewey's death in 1931, however, an agreement was reached between the committee overseeing the development of the Decimal Classification and the developers of the French \"Classification Decimal\". The English version was published as the Universal Decimal Classification and is still in use today.\nAccording to a study done in 1927, the Dewey system was used in the US in approximately 96% of responding public libraries and 89% of the college libraries. After the death of Melvil Dewey in 1931, administration of the classification was under the Decimal Classification Committee of the Lake Placid Club Education Foundation, and the editorial body was the Decimal Classification Editorial Policy Committee with participation of the American Library Association (ALA), Library of Congress, and Forest Press. By the 14th edition in 1942, the Dewey Decimal Classification index was over 1,900 pages in length and was published in two volumes.\n1942\u2013present: forging an identity.\nThe growth of the classification to date had led to significant criticism from medium and large libraries which were too large to use the abridged edition but found the full classification overwhelming. Dewey had intended issuing the classification in three editions: the library edition, which would be the fullest edition; the bibliographic edition, in English and French, which was to be used for the organization of bibliographies rather than of books on the shelf; and the abridged edition. In 1933, the bibliographic edition became the Universal Decimal Classification, which left the library and abridged versions as the formal Dewey Decimal Classification editions. The 15th edition, edited by Milton Ferguson, implemented the growing concept of the \"standard edition\", designed for the majority of general libraries but not attempting to satisfy the needs of the very largest or of special libraries. It also reduced the size of the Dewey system by over half, from 1,900 to 700 pages. This revision was so radical that an advisory committee was formed right away for the 16th and 17th editions. The 16th and 17th editions, under the editorship of the Library of Congress, grew again to two volumes. However, by then, the Dewey Decimal system had established itself as a classification for general libraries, with the Library of Congress Classification having gained acceptance for large research libraries.\nThe first electronic version of \"Dewey\" was created in 1993. Hard-copy editions continue to be issued at intervals; the online WebDewey and Abridged WebDewey are updated quarterly.\nAdministration and publication.\nDewey and a small editorial staff managed the administration of the very early editions. Beginning in 1922, the Lake Placid Club Educational Foundation, a not-for-profit organization founded by Melvil Dewey, managed administrative affairs. The ALA set up a Special Advisory Committee on the Decimal Classification as part of the Cataloging and Classification division of ALA in 1952. The previous Decimal Classification Committee was changed to the Decimal Classification Editorial Policy Committee, with participation of the ALA Division of Cataloging and Classification, and of the Library of Congress.\nMelvil Dewey edited the first three editions of the classification system and oversaw the revisions of all editions until his death in 1931. May Seymour became editor in 1891 and served until her death in 1921. She was followed by Dorcas Fellows, who was editor until her death in 1938. Constantin J. Mazney edited the 14th edition. Milton Ferguson functioned as editor from 1949 to 1951. The 16th edition in 1958 was edited under an agreement between the Library of Congress and Forest Press, with David Haykin as director. Editions 16\u201319 were edited by Benjamin A. Custer and the editor of edition 20 was John P. Comaromi. Joan Mitchell was editor until 2013, covering editions 21 to 23. In 2013 Michael Panzer of OCLC became Editor-in-Chief. The Dewey Editorial Program Manager since 2016 has been Rebecca Green.\nDewey himself held copyright in editions 1 to 6 (1876\u20131919). Copyright in editions 7\u201310 was held by the publisher, The Library Bureau. On the death of May Seymour, Dewey conveyed the \"copyrights and control of all editions\" to the Lake Placid Club Educational Foundation, a non-profit chartered in 1922. The Online Computer Library Center of Dublin, Ohio, U.S., acquired the trademark and copyrights associated with the Dewey Decimal Classification system when it bought Forest Press in 1988. In 2003 the Dewey Decimal Classification came to the attention of the U.S. press when OCLC sued the Library Hotel for trademark infringement for using the classification system as the hotel theme. The case was settled shortly thereafter.\nThe OCLC has maintained the classification since 1988, and also publishes new editions of the system. The editorial staff responsible for updates is based partly at the Library of Congress and partly at OCLC. Their work is reviewed by the Decimal Classification Editorial Policy Committee, a ten-member international board which meets twice each year. The four-volume unabridged edition was published approximately every six years, with the last edition (DDC 23) published in mid-2011. In 2017 the editorial staff announced that the English edition of DDC will no longer be printed, in favor of using the frequently updated WebDewey. An experimental version of Dewey in RDF was available at dewey.info beginning in 2009, but has not been available since 2015.\nIn addition to the full version, a single-volume abridged edition designed for libraries with 20,000 titles or fewer has been made available since 1895. The last printed English abridged edition, Abridged Edition 15, was published in early 2012.\nDesign.\nThe Dewey Decimal Classification organizes library materials by discipline or field of study. The scheme comprises ten classes, each divided into ten divisions, each having ten sections. The system's notation uses Indo-Arabic numbers, with three whole numbers making up the main classes and sub-classes and decimals designating further divisions. The classification structure is hierarchical and the notation follows the same hierarchy. Libraries not needing the full level of detail of the classification can trim right-most decimal digits from the class number to obtain more general classifications. For example:\nThe classification was originally enumerative, meaning that it listed all of the classes explicitly in the schedules. Over time it added some aspects of a faceted classification scheme, allowing classifiers to construct a number by combining a class number for a topic with an entry from a separate table. Tables cover commonly used elements such as geographical and temporal aspects, language, and bibliographic forms. For example, a class number could be constructed using 330 for economics +\u00a0.9 for geographic treatment +\u00a0.04 for Europe to create the class 330.94 European economy. Or one could combine the class 973 (for the United States) +\u00a0.05 (for periodical publications on the topic) to arrive at the number 973.05 for periodicals concerning the United States generally. The classification also makes use of mnemonics in some areas, such that the number 5 represents the country Italy in classification numbers like 945 (history of Italy), 450 (Italian language), and 195 (Italian philosophy). The combination of faceting and mnemonics makes the classification \"synthetic\" in nature, with meaning built into parts of the classification number.\nThe Dewey Decimal Classification has a number for all subjects, including fiction, although many libraries maintain a separate fiction section shelved by alphabetical order of the author's surname. Each assigned number consists of two parts: a class number (from the Dewey system) and a book number, which \"prevents confusion of different books on the same subject\". A common form of the book number is called a Cutter number, which represents the author.\nRelative Index.\nThe Relative Index (or, as Dewey spelled it, \"Relativ Index\") is an alphabetical index to the classification, for use both by classifiers and by library users when seeking books by topic. The index was \"relative\" because the index entries pointed to the class numbers, not to the page numbers of the printed classification schedule. In this way, the Dewey Decimal Classification itself had the same relative positioning as the library shelf and could be used either as an entry point to the classification, by catalogers, or as an index to a Dewey-classed library itself.\nInfluence and criticism.\nDewey Decimal Classification numbers formed the basis of the Universal Decimal Classification (UDC), which combines the basic Dewey numbers with selected punctuation marks (comma, colon, parentheses, etc.). Adaptations of the system for specific regions outside the English-speaking world include the Korean Decimal Classification, the New Classification Scheme for Chinese Libraries, and the Nippon Decimal Classification in Japan.\nDespite its widespread use, the classification has been criticized for its complexity and its limited capability for amendment. This is particularly demonstrated with the literature section (800s): literature in European languages takes the entire range from 810 through 889, while the entire rest of the world's literature is relegated to the 890s. In 2007\u201308, the Maricopa County Library District in Arizona abandoned the DDC in favor of the Book Industry Standards and Communications (BISAC) system commonly used by commercial bookstores, in an effort to make its libraries more accessible for their users. Several other libraries across the United States and other countries (including Canada and the Netherlands) followed suit. \nTreatment of homosexuality.\nIn 1932, topics relating to homosexuality were first added to the system under 132 (mental derangements) and 159.9 (abnormal psychology). In 1952, homosexuality was also included under 301.424 (the study of sexes in society). In 1989, it was added to 363.49 (social problems), a classification that continues in the current edition.\nIn 1996, homosexuality was added to 306.7 (sexual relations); this remains the preferred location in the current edition. Although books can also be found under 616.8583 (sexual practices viewed as medical disorders), the official direction states:\nTreatment of religion.\nThe top-level class for religion heavily favors Christianity, dedicating nearly all of the 200 division to it: the world's thousands of other religions were listed under the 290s. For example, Islam is under just DDC 297. The entire 200 section has remained largely unchanged since DDC 1, since restructuring would pose a significant amount of work for existing libraries. The motivation for this change is ideological rather than technical, as appending significant figures can add space as needed.\nTreatment of women.\nIt has also been argued by Hope A. Olson that the placement of topics related to women shows implicit bias, but this has been simpler to address than the religion schema. Some changes made so far have been in numerical proximity, altering the placement of topics relative to each other. For example, in older versions of the DDC, some categories regarding women were adjacent to categories on etiquette; the placement of these categories next to each other imposed an association of etiquette with women, rather than treating it as gender-neutral. This was changed in DDC version 17, in 1965."}
{"id": "8702", "revid": "16925756", "url": "https://en.wikipedia.org/wiki?curid=8702", "title": "Du\u1e25kha", "text": "Du\u1e25kha (; , ) \"suffering\", \"pain\", \"unease\", or \"unsatisfactoriness\", is an important concept in Buddhism, Jainism and Hinduism. Its meaning depends on the context, and may refer more specifically to the \"unsatisfactoriness\" or \"unease\" of craving for and grasping after transient 'things' (sense objects, including thoughts), expecting pleasure from them while ignorant of this transientness. In Buddhism, dukkha is part of the first of the Four Noble Truths and one of the three marks of existence. The term also appears in scriptures of Hinduism, such as the Upanishads, in discussions of moksha (spiritual liberation).\nWhile the term \"dukkha\" has often been derived from the prefix \"du-\" (\"bad\" or \"difficult\") and the root \"kha\" (\"empty,\" \"hole\"), meaning a badly fitting axle-hole of a cart or chariot giving \"a very bumpy ride,\" it may actually be derived from \"du\u1e25-stha\", a \"dis-/ bad- + stand-\", that is, \"standing badly, unsteady,\" \"unstable.\"\nEtymology and meaning.\n\"Du\u1e25kha\" (Sanskrit: \u0926\u0941\u0903\u0916; Pali: \"dukkha\") is a term found in the Upanishads and Buddhist texts, meaning anything that is \"uneasy, uncomfortable, unpleasant, difficult, causing pain or sadness\". It is also a concept in Indian religions about the nature of transient phenomena which are innately \"unpleasant\", \"suffering\", \"pain\", \"sorrow\", \"distress\", \"grief\" or \"misery.\" The term \"du\u1e25kha\" does not have a one-word English translation, and embodies diverse aspects of unpleasant human experiences. It is often understood as the opposite of \"sukha\", meaning lasting \"happiness,\" \"comfort\" or \"ease.\"\nEtymology.\nAxle hole.\nThe word has been explained in recent times as a derivation from Aryan terminology for an axle hole, referring to an axle hole which is not in the center and leads to a bumpy, uncomfortable ride. According to Winthrop Sargeant,\nJoseph Goldstein, American vipassana teacher and writer, explains the etymology as follows:\n'Standing unstable'.\nHowever, according to Monier Monier-Williams, the actual roots of the Pali term \"dukkha\" appear to be Sanskrit \u0926\u0941\u0938\u094d- (\"dus-\", \"bad\") + \u0938\u094d\u0925\u093e (\"sth\u0101\", \"to stand\"). Regular phonological changes in the development of Sanskrit into the various Prakrits led to a shift from \"dus-sth\u0101\" to \"du\u1e25kha\" to \"dukkha\".\nAnalayo concurs, stating that \"dukkha\" as derived from \"du\u1e25-sth\u0101\", \"standing badly,\" \"conveys nuances of \"uneasiness\" or of being \"uncomfortable.\" Silk Road philologist Christopher I. Beckwith elaborates on this derivation. According to Beckwith:\nTranslation.\nThe literal meaning of \"du\u1e25kha\", as used in a general sense is \"suffering\" or \"painful.\" Its exact translation depends on the context. Contemporary translators of Buddhist texts use a variety of English words to convey the aspects of \"dukh\". Early Western translators of Buddhist texts (before the 1970s) typically translated the Pali term \"dukkha\" as \"suffering.\" Later translators have emphasized that \"suffering\" is a too limited translation for the term du\u1e25kha, and have preferred to either leave the term untranslated, or to clarify that translation with terms such as anxiety, distress, frustration, unease, unsatisfactoriness, not having what one wants, having what one doesn't want, etc. In the sequence \"birth is painful,\" \"dukhka\" may be translated as \"painful.\" When related to vedana, \"feeling,\" \"dukkha\" (\"unpleasant,\" \"painful\") is the opposite of \"sukkha\" (\"pleasure,\" \"pleasant\"), yet all feelings are \"dukkha\" in that they are impermanent, conditioned phenomena, which are unsatisfactory, incapable of providing lasting satisfaction. The term \"unsatisfactoriness\" then is often used to emphasize the unsatisfactoriness of \"life under the influence of afflictions and polluted karma.\"\nBuddhism.\nEarly Buddhism.\n\"Du\u1e25kha\" is one of the three marks of existence, namely \"anitya\" (\"impermanent\"), \"du\u1e25kha\" (\"unsatisfactory\"), \"anatman\" (without a lasting essence).\nVarious sutras sum up how cognitive processes result in an aversion to unpleasant things and experiences (\"du\u1e25kha\"), forming a corrupted process together with the complementary process of clinging to and craving for pleasure (\"suhkha\"). This is expressed as \"sa\u1e43s\u0101ra\", an ongoing process of death and rebirth, but also more pointly and non-metaphysically in the process-formula of the five skandhas:\nEarly emphasis is on the importance of developing insight into the nature of \"du\u1e25kha\", the corrupted process of clinging and craving which starts with sense-contact, as described in the skandhas, and how this corruption can be overcome, namely by training the mind culminating in the process of the dhyanas. This is summarized in the teachings on the Four Noble Truths and other formulaic expressions of the Buddhist way to awakening.\nWithin the Buddhist sutras, du\u1e25kha has a broad meaning, and has also been specified in three categories:\nChinese Buddhism.\nChinese Buddhist tradition has been influenced by Taoism and Confucian theory that advocates that duhkha (\u53e4:\u5341Ten directions, \u53e3 hole or opening) is associated to the theory of seven emotions of endogenous disease through the formation of the spirit of the po a term that relates to the Western psychological notion of ego or the theological reference to the human soul. This theory is expounded in the application of traditional Chinese medicine for the treatment and prevention of pain and suffering from illness, disease and ignorance. \nLiteral suffering and awakening.\nAwakening, that is, awakening to one's true mind of emptiness and compassion, does not necessarily end physical suffering. In the Buddhist tradition, suffering after awakening is often explained as the working-out or untangling of karma of one's previous present life.\nHinduism.\nIn Hinduism, \"du\u1e25kha\" encompasses many meanings such as the phenomenological senses of pain and grief, a deep-seated dissatisfaction with the limitations of worldly existence, and the devastation of impermanence.\nIn Hindu scriptures, the earliest Upaniads \u2014 the and the \u2014 in all likelihood predate the advent of Buddhism. In these scriptures of Hinduism, the Sanskrit word \"dukha\" (\u0926\u0941\u0903\u0916) appears in the sense of \"suffering, sorrow, distress\", and in the context of a spiritual pursuit and liberation through the knowledge of Atman ('essence').\nThe concept of sorrow and suffering, and self-knowledge as a means to overcome it, appears extensively with other terms in the pre-Buddhist Upanishads. The term \"Duhkha\" also appears in many other middle and later post-Buddhist Upanishads such as the verse 6.20 of Shvetashvatara Upanishad, as well as in the Bhagavad Gita, all in the contexts of moksha and bhakti.\nThe term also appears in the foundational Sutras of the six schools of Hindu philosophy, such as the opening lines of \"Samkhya karika\" of the Samkhya school. The Samkhya school identifies three types of suffering. The Yoga Sutras of Patanjali state that \"for one who has discrimination, everything is suffering\" (\"du\u1e25kham eva sarva\u1e41 vivekina\u1e25\").\nSome of the Hindu scripture verses referring to \"duhkha\" are:\nJainism.\n\"Du\u1e25kha\" is explained in the Tattvartha Sutra, an authoritative Jain scripture from the 2nd century."}
{"id": "8703", "revid": "4680642", "url": "https://en.wikipedia.org/wiki?curid=8703", "title": "Darwin Awards", "text": "The Darwin Awards are a rhetorical tongue-in-cheek honor that originated in Usenet newsgroup discussions around 1985. They recognize individuals who have supposedly contributed to human evolution by selecting themselves out of the gene pool by dying or becoming sterilized by their own actions.\nThe project became more formalized with the creation of a website in 1993, followed by a series of books starting in 2000 by Wendy Northcutt. The criterion for the awards states: \"In the spirit of Charles Darwin, the Darwin Awards commemorate individuals who protect our gene pool by making the ultimate sacrifice of their own lives. Darwin Award winners eliminate themselves in an extraordinarily idiotic manner, thereby improving our species' chances of long-term survival.\"\nAccidental self-sterilization also qualifies, but the site notes: \"Of necessity, the award is usually bestowed posthumously.\" The candidate is disqualified, though, if \"innocent bystanders\" are killed in the process, as they might have contributed positively to the gene pool. The logical problem presented by award winners who may have already reproduced is not addressed in the selection process owing to the difficulty of ascertaining whether or not a person has children; the Darwin Award rules state that the presence of offspring does not disqualify a nominee.\nHistory.\nThe origin of the Darwin Awards can be traced back to posts on Usenet group discussions as early as 1985. A post on August 7, 1985, describes the awards as being \"given posthumously to people who have made the supreme sacrifice to keep their genes out of our pool. Style counts, not everyone who dies from their own stupidity can win.\" This early post cites an example of a person who tried to break into a vending machine and was crushed to death when he pulled it over himself. Another widely distributed early story mentioning the Darwin Awards is the JATO Rocket Car, which describes a man who strapped a jet-assisted take-off unit to his Chevrolet Impala in the Arizona desert and who died on the side of a cliff as his car achieved speeds of . This story was later determined to be an urban legend by the Arizona Department of Public Safety. Wendy Northcutt says the official Darwin Awards website run by Northcutt does its best to confirm all stories submitted, listing them as, \"confirmed true by Darwin\". Many of the viral emails circulating the Internet, however, are hoaxes and urban legends.\nThe website and collection of books were started in 1993 by Wendy Northcutt, who at the time was a graduate in molecular biology from the University of California, Berkeley. She went on to study neurobiology at Stanford University, doing research on cancer and telomerase. In her spare time, she organised chain letters from family members into the original Darwin Awards website hosted in her personal account space at Stanford. She eventually left the bench in 1998 and devoted herself full-time to her website and books in September 1999. By 2002, the website received 7 million page hits per month.\nNorthcutt encountered some difficulty in publishing the first book, since most publishers would only offer her a deal if she agreed to remove the stories from the Internet, but she refused: \"It was a community! I could not do that. Even though it might have cost me a lot of money, I kept saying no.\" She eventually found a publisher who agreed to print a book containing only 10% of the material gathered for the website. The first book turned out to be a success, and was listed on \"The New York Times\" best-seller list for 6 months.\nNot all of the feedback from the stories Northcutt published was positive, and she occasionally received email from people who knew the deceased. One such person advised: \"This is horrible. It has shocked our community to the core. You should remove this.\" Northcutt demurred: \"I can't. It's just too stupid.\" Northcutt kept the stories on the website and in her books, citing them as a \"funny-but-true safety guide\", and mentioning that children who read the book are going to be much more careful around explosives.\nThe website also awards Honorable Mentions to individuals who survive their misadventures with their reproductive capacity intact. One example of this is Larry Walters, who attached helium-filled weather balloons to a lawn chair and floated far above Long Beach, California, in July 1982. He reached an altitude of , but survived, to be later fined for crossing controlled airspace. (Walters later fell into depression and died by suicide.) Another notable honorable mention was given to the two men who attempted to burgle the home of footballer Duncan Ferguson (who had an infamous reputation for physical aggression on and off the pitch, including four convictions for assault and who had served six months in Glasgow's Barlinnie Prison) in 2001, with one burglar requiring three days' hospitalisation after being confronted by the player.\nA 2014 study published in the \"British Medical Journal\" found that between 1995 and 2014, males represented 88.7% of Darwin Award winners (see figure).\nThe comedy film \"The Darwin Awards\" (2006), written and directed by Finn Taylor, was based on the website and many of the Darwin Awards stories.\nRules.\nNorthcutt has stated five requirements for a Darwin Award: Two of them are that the event must be verified to have happened, and that the nominee themselves were responsible for the activity. The others are:\nNominee must be dead or rendered sterile.\nThis may be subject to dispute. Potential awardees may be out of the gene pool because of age; others have already reproduced before their deaths. To avoid debates about the possibility of \"in vitro\" fertilization, artificial insemination, or cloning, the original Darwin Awards book applied the following \"deserted island\" test to potential winners: If the person were unable to reproduce when stranded on a deserted island with a fertile member of the opposite sex, he or she would be considered sterile. Winners of the award, in general, either are dead or have become unable to use their sexual organs.\nAstoundingly stupid judgment.\nThe candidate's foolishness must be unique and sensational, likely because the award is intended to be funny. A number of foolish but common activities, such as smoking in bed, are excluded from consideration. In contrast, self-immolation caused by smoking after being administered a flammable ointment in a hospital and specifically told not to smoke is grounds for nomination. One \"Honorable Mention\" (a man who attempted suicide by swallowing nitroglycerin pills, and then tried to detonate them by running into a wall) is noted to be in this category, despite being intentional and self-inflicted (i.e. attempted suicide), which would normally disqualify the inductee.\nCapable of sound judgment.\nIn 2011, however, the awards targeted a 16-year-old boy in Leeds who died stealing copper wiring (he was underage at the time of his death; the standard minimum driving age in Great Britain being 17). In 2012, Northcutt made similar light of a 14-year-old girl in Brazil who was killed while leaning out of a school bus window, but she was \"disqualified\" for the award itself because of the likely public objection owing to the girl's age, which Northcutt asserts is based on \"magical thinking\".\nUnder this rule, and for reasons of good taste, individuals whose misfortune was caused by mental impairment or disability are not eligible for a Darwin Award, primarily to avoid mocking or making light of the disabled, and to ensure that the awards do not celebrate or trivialize tragedies involving vulnerable individuals."}
{"id": "8704", "revid": "42788136", "url": "https://en.wikipedia.org/wiki?curid=8704", "title": "Outline of dance", "text": "The following outline is provided as an overview of and topical guide to dance:\nDance \u2013 human movement either used as a form of expression or presented in a social, spiritual or performance setting. Choreography is the art of making dances, and the person who does this is called a choreographer. Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet. A great many dances and dance styles are performed to dance music.\nWhat type of thing is dance?\nDance (also called \"dancing\") can fit the following categories:\nSome other things can be named \"dance\" metaphorically; see dance (disambiguation)\nTypes of dance.\nType of dance \u2013 a particular dance or dance style. There are many varieties of dance. Dance categories are not mutually exclusive. For example, tango is traditionally a \"partner dance\". While it is mostly \"social dance\", its ballroom form may be \"competitive dance\", as in DanceSport. At the same time it is enjoyed as \"performance dance\", whereby it may well be a \"solo dance\".\nHistory of dance.\nHistory of dance\nDance science.\nDance science"}
{"id": "8706", "revid": "21436738", "url": "https://en.wikipedia.org/wiki?curid=8706", "title": "DCM", "text": "DCM may refer to:"}
{"id": "8707", "revid": "12317794", "url": "https://en.wikipedia.org/wiki?curid=8707", "title": "DKW", "text": "DKW (Dampf-Kraft-Wagen, ; the same initials later also used for Deutsche Kinder-Wagen, ; Das Kleine Wunder, ; and Des Knaben Wunsch ) was a German car- and motorcycle-marque. DKW was one of the four companies that formed Auto Union in 1932 and thus became an ancestor of the modern-day Audi company.\nIn 1916, Danish engineer J\u00f8rgen Skafte Rasmussen founded a factory in Zschopau, Saxony, Germany, to produce steam fittings. That year he attempted to produce a steam-driven car, which he called the DKW. That steam car was unsuccessful, and in 1919 he made toy two-stroke engines under the name \"Des Knaben Wunsch\" \u2013 \"the boy's wish\". He put a slightly modified version of the toy engine into a motorcycle and called it \"Das Kleine Wunder\" \u2013 \"the little wonder\", and by the late 1920s DKW had become the world's largest motorcycle manufacturer.\nIn September 1924, DKW bought , saving them from Germany's hyperinflation economic crisis. Rudolf Slaby became chief engineer at DKW.\nIn 1932, DKW merged with Audi, Horch and Wanderer to form Auto Union. After World War II, DKW moved to West Germany. The original factory became MZ. Auto Union came under Daimler-Benz ownership in 1957 and was purchased by the Volkswagen Group in 1964. The last German-built DKW car was the F102, which ceased production in 1966. Its successor, the four-stroke F103, was marketed under the Audi brand, another Auto Union marque.\nDKW-badged cars continued to be built under license in Brazil and Argentina until 1967 and 1969 respectively. The DKW trademark is currently owned by Auto Union GmbH, a wholly-owned subsidiary of Audi AG which also owns the rights to other historical trademarks and intellectual property of the Auto Union combine.\nAutomobiles made between 1928 and 1942.\nDKW cars were made from 1928 until 1966, apart from the interruption caused by the Second World War. DKWs always used two-stroke engines, reflecting the company's position by the end of the 1920s as the world's largest producer of motorcycles. The first DKW car, the small and rather crude Typ P, emerged on 7 May 1928 and the model continued to be built at the company's Spandau (Berlin) plant, first as a roadster and later as a stylish if basic sports car, until 1931.\nMore significant was a series of inexpensive cars built 300\u00a0km (185 miles) to the south in Zwickau in the plant acquired by the company's owner J\u00f8rgen Skafte Rasmussen in 1928 when he had become the majority owner in Audi Werke AG. Models F1 to F8 (F for Front) were built between 1931 and 1942, with successor models reappearing after the end of the war in 1945. They were the first volume production cars in Europe with front wheel drive, and were powered by transversely mounted two-cylinder two-stroke engines. Displacement was 584 or 692\u00a0cc: claimed maximum power was initially 15 PS, and from 1931 a choice between 18 or . These models had a generator that doubled as a starter, mounted directly on the crankshaft, known as a Dynastart. DKW in Zwickau produced approximately 218,000 units between 1931 and 1942. Most of those cars were sold on the home market and over 85% of DKWs produced in the 1930s were the little F series cars: DKW reached second place in German sales by 1934 and stayed there, accounting for 189,369 of the cars sold between 1931 and 1938, more than 16% of the market.\nBetween 1929 and 1940, DKW produced a less well remembered but technically intriguing series of rear-wheel drive cars called (among other names) \"Schwebeklasse\" and \"Sonderklasse\" with two-stroke V4 engines. Engine displacement was 1,000\u00a0cc, later 1,100\u00a0cc. The engines had two extra cylinders that acted as air compressors for forced induction, so they had the external appearance of a V6 engine but without spark plugs on the front cylinder pair.\nIn 1939, DKW made a prototype with the first three-cylinder engine, with a displacement of 900\u00a0cc and producing . With a streamlined body, the car could run at . It was put into production after World War II, first as an Industrieverband Fahrzeugbau (IFA) F9 (later Wartburg) in Zwickau, East Germany, and shortly afterwards in DKW-form from D\u00fcsseldorf as the 3=6 or F91.\nSaab used DKW engines as a model for the Saab two-stroke in their first production car, the Saab 92.\nAutomobiles made after 1945.\nAs Auto Union was based in Saxony in what became the German Democratic Republic (East Germany), it took some time for it to regroup after the war. The company was registered in West Germany as Auto Union GmbH in 1949, first as a spare-part provider, but soon to take up production of the RT 125 motorcycle and a new delivery van, called a \"Schnellaster\" F800. Their first line of production took place in D\u00fcsseldorf. This van used the same engine as the last F8 made before the war.\nTheir first car was the F89 using the body from the prototype F9 made before the war and the two-cylinder two-stroke engine from the last F8. Production went on until it was replaced by the successful three-cylinder engine that came with the F91. The F91 was in production 1953\u20131955, and was replaced by the larger F93 in 1956. The F91 and F93 had 900\u00a0cc three-cylinder two-stroke engines, the first ones delivering , the last . The ignition system comprised three independent sets of points and coils, one for each cylinder, with the points mounted in a cluster around a single lobed cam at the front end of the crankshaft. The cooling system was of the free convection type assisted by a fan driven from a pulley mounted at the front end of the crankshaft.\nThe F93 was produced until 1959, and was replaced by the Auto-Union 1000. These models were produced with a 1,000\u00a0cc two-stroke engine, with a choice between or S versions until 1963. During this transition, production was moved from D\u00fcsseldorf to Ingolstadt, where Audi still has its production. From 1957, the cars could be fitted with a saxomat, an automatic clutch, the only small car then offering this feature. The last versions of the Auto-Union 1000S had disc brakes as option, an early development for this technology. A sporting 2+2 seater version was available as the Auto-Union 1000 SP from 1957 to 1964, the first years only as a coup\u00e9 and from 1962 also as a convertible.\nIn 1956, the very rare DKW Monza was put into small-scale production on a private initiative, with a sporting two-seater body of glassfiber on a standard F93 frame. It was first called Solitude, but got its final name from the long-distance speed records it made on the Autodromo Nazionale Monza in Italy in December 1956. Running in F\u00e9d\u00e9ration Internationale de l'Automobile (FIA) class G, it set records including 48 hours at an average speed of , 10,000\u00a0km at and 72 hours at . The car was first produced by in Stuttgart, then by Massholder in Heidelberg and lastly by Robert Schenk in Stuttgart. The number produced is said to be around 75, 50 survived. Production finished by the end of 1958.\nA more successful range of cars was sold from 1959, the Junior/F12 series based on a modern concept from the late 1950s. The range consists of Junior (basic model) made from 1959 to 1961, Junior de Luxe (a little enhanced) from 1961 to 1963, F11 (a little larger) and F12 (larger and bigger engine) from 1963 to 1965, and F12 Roadster from 1964 to 1965. The Junior/F12 series became quite popular, and many cars were produced. An assembly plant was licensed in Ballincollig, County Cork, Ireland between 1952 and c.1964 and roughly 4,000 vehicles were assembled, ranging from saloons, vans and motorbikes to commercial combine harvesters. This was the only DKW factory outside Germany in Europe and for many years after its closure its large DKW sign could be visible on the wall of the factory. The building was demolished in the late 2000s and was redeveloped into a German Aldi store and a McDonald's drive-thru.\nAll the three-cylinder two-stroke post-war cars had some sporting potential and formed the basis for many rally victories in the 1950s and early 1960s. This made DKW the most winning car brand in the European rally league for several years during the fifties.\nIn 1960, DKW developed a V6 engine by combining two three-cylinder two-stroke engines, with a capacity of 1,000\u00a0cc. The capacity was increased and the final V6 in 1966 had a capacity of 1,300\u00a0cc, which developed at 5,000\u00a0rpm using the standard configuration with two carburettors. A four-carburettor version produced , a six-carburettor one . It weighed only . The V6 was planned to be used in the DKW Munga and the F102. About 100 engines were built for testing purposes and 13 DKW F102 and some Mungas were fitted with the V6 engine in the 1960s.\nThe last DKW was the F102, coming into production in 1964 as a replacement for the old-looking AU1000. However, the F102 sold poorly, largely due to its two-stroke engine technology which was at the limit of its development. Auto Union's parent, Daimler-Benz, decided to offload the company to Volkswagen. The car was re-engineered with a four-stroke engine and relaunched as the Audi F103. This marked the end of the DKW marque for cars, and the rebirth of the Audi name.\nFrom 1956 to 1961, Dutch importer Hart, Nibbrig &amp; Greve assembled cars in an abandoned asphalt factory in Sassenheim, where they employed about 120 workers, two transporter, that collected SKD kits from Duesseldorf and built about 13.500 cars. When the DKW plant moved the import of SKD kits stopped, as it became too expensive.\nDKW in South America.\nFrom 1956 to 1967, DKW cars were made in Brazil by the local company Vemag (\"Ve\u00edculos e M\u00e1quinas Agr\u00edcolas S.A.\", \"Vehicles and Agricultural Machinery Inc.\"). Vemag was assembling Scania-Vabis trucks, but Scania Vabis became an independent company in July 1960. The original plans were to build the Candango off-roader (Munga), a utility vehicle and a four-door sedan, called Vemaguet and Belcar respectively. The first model built was the 900\u00a0cc F91 Universal but the Belcar and Vemaguet names were applied later.\nIn 1958, the F94 four-door sedan and station wagon were launched, in the early 1960s renamed Belcar and Vemaguet. The company also produced a luxury coupe (the DKW Fissore) and the off-road Munga (locally called Candango). In 1960 Vemag cars received the larger one-litre, engine from the Auto Union 1000.\nVemag had a successful official racing team, with the coupe GT Malzoni, with fiberglass body. This project was the foundation of the long-lasting Brazilian sports car brand Puma. The Brazilian F94 line has been improved with several cosmetic changes and became more and more different from the German and Argentine models. Vemag had no capital to invest in new products and came under governmental pressure to merge. In 1964\u20131965 Volkswagen gradually took over Auto Union, a minority holder in Vemag, and in 1967 Volkswagen bought the remainder of the stock. VW quickly began phasing out DKW-Vemag production and introduced the Volkswagen 1600 sedan to the old Vemag plant, after a total of 109,343 DKW-Vemag cars had been built.\nDKW vehicles were made in Argentina from 1960 to 1969 by IASF S.A. (Industria Automotriz Santa Fe Sociedad An\u00f3nima) in Sauce Viejo, Santa Fe. The most beautiful were the Cup\u00e9 Fissore, which had many famous owners (Julio Sosa, C\u00e9sar Luis Menotti, and others). Other models are the Auto Union 1000 S Sed\u00e1n (21,797 made until 1969) and the Auto Union 1000 Universal S (6,396 made until 1969). and the Auto Union Combi/Pick-up.\nThe last version of the Auto Union Combi/Pick-up (DKW F1000 L), launched in 1969, survived a few months and was bought out by IME, which continued production until 1979.\nVans and utility vehicles.\nThe DKW Munga was built by Auto Union in Ingolstadt. Production began in October 1956 and ended in December 1968, with 46,750 cars built.\nFrom 1949 to 1962, DKW produced the \"Schnellaster\" with a trailing-arm rear suspension system with springs in the cross bar assembly. Spanish subsidiary IMOSA produced a modern successor introduced in 1963, the DKW F 1000\u00a0L. This van started with the three-cylinder 1,000\u00a0cc engine, but later received a Mercedes-Benz Diesel engine and was renamed a Mercedes-Benz in 1975.\nMotorcycles.\nDuring the late 1920s and until WWII broke out, DKW was both the world's largest motorcycle manufacturer, as well as Europe's pioneer of front-wheel drive automobiles with their successful 1931 and later DKW Front models, before the 1932 Adler Trumpf and the 1934 Citroen Traction Avant. In 1931, Arnold Zoller started building split-singles and this concept made DKW the dominant racing motorcycle in the Lightweight and Junior classes between the wars. This included off-road events like the International Six Days Trial where the marque scored some considerable inter-war year successes alongside Bavarian Motor Works At the same time, the company also had some success with super-charged racing motorcycles which because of their light weight were particularly successful in the ISDT\nThe motorcycle branch produced famous models such as the RT 125 pre- and post-World War II, and after the war with production at the original factory in GDR becoming MZ it made 175, 250 and 350 (cc) models. As war reparations, the design drawings of the RT125 were given to Harley-Davidson in the US and BSA in the UK. The Harley-Davidson version was known loosely as the Hummer (Hummer is really just a few specific years, but generally people call the Harley lightweights Hummers), while BSA used them for the Bantam. IFA and later MZ models continued in production until the 1990s, when economics brought production of the two stroke to an end. Other manufacturers copied the DKW design, officially or otherwise. This can be seen in the similarity of many small two-stroke motorcycles from the 1950s, including from Yamaha, Voskhod, Maserati and Polish WSK.\nCars.\nPre-war production.\nPre-war and war-years production of civilian models totalled almost 250,000 units, of which some 218,000 were front-wheel driven."}
{"id": "8708", "revid": "38540013", "url": "https://en.wikipedia.org/wiki?curid=8708", "title": "Doctor Syn", "text": "The Reverend Doctor Christopher Syn is the smuggler hero of a series of novels by Russell Thorndike. The first book, \"Doctor Syn: A Tale of the Romney Marsh\" was published in 1915. The story idea came from legendary coastal smuggling in the 18th century around well-known Romney Marsh, where brandy and tobacco were brought in at night by boat from France to avoid the tax. Minor battles were fought, sometimes at night, between gangs of smugglers, such as the Hawkhurst Gang, and His Majesty's Customs and Excise / Revenue, supported by the British Army, Royal Navy and local militias in the counties of the South Kent and Sussex.\nCharacter biography.\nChristopher Syn, born 1729, is portrayed as a brilliant scholar from Queen's College, Oxford, possessing swashbuckling skills such as riding, fencing, and seamanship. He was content to live the quiet life of a Church of England / Anglican country priest / vicar in Dymchurch-under-the-Wall under the patronage of Sir Charles Cobtree, the father of his best friend Anthony Cobtree, until his beautiful young Spanish wife Imogene was seduced by and eloped with Nicholas Tappitt, whom Dr Syn had considered a close friend.\nChristopher Syn set out on a quest for revenge, always managing to reach the eloped pair's destinations ahead of them just in time to terrify them against landing and facing him in a deliberate campaign of terror. While sailing from Spain to the Americas in pursuit, his ship was captured by the pirate ship \"The Sulphur Pit\", commanded by Captain Satan. In a one-on-one fight, Syn defeated and killed Captain Satan to take command of his ship and crew; among them was Mr. Mipps, a former British Royal Navy carpenter with whom Syn had become friends in England after rescuing him from the His Majesty's Customs and Excise / Revenue men. Mipps swore loyalty to Syn from that time onward.\nWith Mipps at his side, Syn turned to piracy and became a great success. Later, when his crew refused to let Syn leave, Syn and Mipps slipped away in one of the ship's boats; unknown to Syn, Mipps had arranged a convenient \"accident\" in the ship's powder magazine with an exploding barrel of gunpowder, eliminating witnesses of Syn's piratical acts.\nMipps then joined Syn in his quest for revenge, pursuing Tappitt and Imogene throughout the thirteen American colonies of British America (supposedly preaching the gospel to the Indians) and around the world (as part of a whaling voyage) afterwards. Mipps was with him in the Caribbean when Dr. Syn turned again to piracy, assuming the name of Captain Clegg (taking the name \"Clegg\" from a certain vicious biting fly he had encountered in America). \"Clegg\" hijacked his enemy Tappitt's own ship and crew and sailed off with them (renaming the ship the \"Imogene\") to become the most infamous pirate of the day.\nHowever, a mulatto who escaped the destruction of Syn's previous ship stowed away in Clegg's ship and accused him before the crew; Clegg quelled the potential mutiny by having the mulatto's tongue cut out, marooning him on a coral reef and violently killing Yellow Pete, the ship's Chinese cook, who represented the crew in their wish to rescue the mulatto. Afterwards, realizing that Clegg had become too notorious, Syn decided to abandon his quest and return to England, and Mipps set up a second \"accidental\" explosion to destroy the \"Imogene\" and her crew.\nSyn returned to England on the night of a storm (13 November 1775) that wrecked his brig off the English coast in sight of Dymchurch. That night he went to the house of his old friend (and now squire) Anthony Cobtree. When news came that the local vicar had drowned while trying to save victims of the shipwreck, Squire Cobtree offered the post to Christopher Syn. Syn accepted and settled down to a more respectable life as the vicar of Dymchurch and Dean of Peculiars in Romney Marsh, Kent, resuming his original name.\nMipps arrived in Dymchurch with the intent of settling down. Syn made him the village sexton upon condition that Mipps \"remember to forget\" (that Syn had been Clegg and that they had known each other before), and that Mipps never get involved with the local smugglers.\nSyn soon became aware that his parishioners were smuggling goods from France to avoid the excessive customs duties the government charged. Learning from Mipps (who, contrary to Syn's orders, had become a leader of the smugglers) that certain townsfolk had been ambushed and captured during a smuggling run, Syn purchased the great black stallion Gehenna from gypsy horse-traders and raced to their rescue. A suit of clothing borrowed from a scarecrow made an improvised disguise, and Syn and Mipps were able to rescue the townsfolk from the Dragoons.\nAfter this, Syn decided that he could only protect his people by becoming their leader. He created a more elaborate scarecrow costume, with eerie luminous paint. Riding Gehenna at night, the respectable Dr. Syn became \"The Scarecrow\", the feared head of the smugglers. Together with Mipps, he organized the smugglers into a well-organized band of \"Night Riders\", also called \"The Devil Riders\", with macabre disguises and code-names.\nSyn's cunning was so great that the smugglers outwitted the government forces for many years. A hidden stable watched over by Mother Handaway, the local \"witch\" (who believed the Scarecrow to be The Devil in living form), was the hiding place for the horses of the Scarecrow and his lieutenants, Mipps and the local highwayman Jimmie Bone (who, being as good a horseman as Syn and of similar build, was sometimes called upon to impersonate the Scarecrow when Syn either had to be elsewhere or seen in the same place).\nShortly after the first appearances of the Scarecrow, Nicholas Tappitt (using the name \"Colonel Delacourt\") and the ailing Imogene returned to England, ending up in Dymchurch. Recognizing Syn as Clegg, Tappitt realized that Syn and the Scarecrow were one and the same person, and helped the authorities set a trap for him, hoping to both rid himself of his enemy and claim the reward for his capture. The trap was sprung, but Squire Cobtree's daughter Charlotte, who had fallen in love with Syn and also learned his secret identities as both Clegg and the Scarecrow, was the tragic victim when she dressed in the Scarecrow's disguise and was fatally wounded as a result. Tappitt was then suspected of being the Scarecrow, and a Customs officer and three constables came to arrest him. In the ensuing fight, Tappitt killed the Customs man and the constables subdued and arrested Tappitt for murdering the Customs officer.\nAfter Imogene's death in Syn's arms (during which she revealed to him that he had a son by her who was missing somewhere in North America), Syn fought a final duel with Tappitt in his jail cell, defeating him. Syn then struck a bargain with Tappitt: if Tappitt confessed to being the notorious pirate Clegg, then Syn would look after and care for Tappitt and Imogene's new-born infant daughter (also named Imogene). Tappitt agreed, and \"Captain Clegg\" was hanged and later \"buried without benefit of clergy at a cross-roads hard by the Kent Ditch\".\nMany years later, Captain Collyer, a Royal Navy officer assigned to smash the local smuggling ring, uncovered the deception and Dr. Syn's true identity, thanks in part to the tongueless mulatto (who had been rescued by Collyer years before and who had been serving Collyer as a \"ferret\" seeking out hidden contraband) who recognized Syn as Clegg. Syn evaded capture while at the same time making sure that Imogene and Squire Cobtree's son Denis (who had fallen in love with Imogene) would have a happy life together (they were eventually married), but was murdered in revenge by the mulatto, who then mysteriously managed to escape, leaving Syn harpooned through the neck. As a last mark of respect, Collyer ordered that Syn be buried at sea, rather than have his body hung in chains.\nMipps escaped in the confusion of Syn's death and disappeared from England, but it is said that a little man very much like him is living out his days in a Buddhist Monastery somewhere in the Malay Peninsula / Malaya, delighting the monks with recounting the adventures of Doctor Syn and the eerie stories of the Romney Marsh and the mysterious Scarecrow and his Night Riders.\nPublication history.\nThe Dr. Syn books detail his adventures and attempts to help the people of Dymchurch and the surrounding area evade the Excise tax. There are seven novels in the series:\nNote: the \"first\" book, \"Doctor Syn\", is actually the final story chronologically; the others proceed in published sequence.\nAn expanded version of \"Doctor Syn Returns\" titled \"The Scarecrow Rides\" was published for the U.S. market by The Dial Press in 1935; years later in 2013 it was re-printed in paperback by Black Curtain Press. ().\nIn 1960, American author William Buchanan reworked Thorndike's \"Further Adventures of Doctor Syn\" under the title \"Christopher Syn\" (New York, Abelard Schuman), giving Thorndike co-authorship credit; this version provides a different conclusion and some conflation, renaming and even removal of the supporting characters. \"Christopher Syn\" became the basis for the 1962 Disney production; there was also a novelization of the Disney theatrical version titled \"Doctor Syn, Alias the Scarecrow\" written by Vic Crume.\nIn other media.\nFilms.\nThree film adaptations have been made of Dr. Syn's exploits.\n\"Doctor Syn\" (1937).\nThe first, \"Doctor Syn\" (1937), starred the actor George Arliss in the title role and was his last film.\n\"Captain Clegg\" (1962).\n\"Captain Clegg\" (1962), (known as \"Night Creatures\" when released in the United States), was produced by Hammer Film Productions with actor Peter Cushing in the lead role, directed by Peter Graham Scott. In the screenplay by Anthony Hinds, the main character's name was changed from \"Doctor Syn\" to \"Parson Blyss\" to avoid copyrights problems with Disney's forthcoming version, and \"Captain Clegg\"'s screenplay follows the novel \"Doctor Syn\" and the screenplay of the 1937 film closely with the exception of a tightening of the plot. In the Arliss film \"Doctor Syn\", Syn escapes to sea with Mipps and the rest of the Dymchurch smugglers, whereas \"Captain Clegg\" ends more faithfully to the novel, with Parson Blyss being killed by the mulatto (who is then killed by Mipps) and then being carried to and buried in Captain Clegg's empty grave by Mipps. \"Captain Clegg\" was released in the United Kingdom / Great Britain on DVD and Blu-ray in 2014; \"Night Creatures\" was never released on videotape in the United States, but is included in the 2014 two-disc DVD collection \"The Hammer Horror Series\". In North America, the film was released on 6 September 2005 along with seven other Hammer horror films on the 4-DVD set \"The Hammer Horror Series\" , part of MCA-Universal's \"Franchise Collection\". This set was re-released on Blu-ray 13 September 2016. A Blu-ray was released in the UK on 23 June 2014 by Final Cut Entertainment. In 2021, Powerhouse Films re-released the film on Blu-Ray, along with \"The Shadow of the Cat\", \"The Phantom of the Opera\", and \"Nightmare\", as part of \"Hammer Volume Six: Night Shadows\" boxset.\n\"The Scarecrow of Romney Marsh\" (1963).\n\"The Scarecrow of Romney Marsh\" (1963) was produced for the \"Walt Disney's Wonderful World of Color\" weekly TV series on Sunday evenings on the National Broadcasting Company (NBC-TV network). It was shot on location in Kent, England (United Kingdom / Great Britain) and was directed by James Neilson. It stars British actor Patrick McGoohan in the title role of the meek, mild, intellectual country priest / vicar of \"Doctor Syn\", with George Cole as Mipps and Australian actor Sean Scully, as young John Banks, the younger son of nearby country estate nobleman, Squire Thomas Banks (Michael Hordern). \nIn the 18th century, the local Church of England / Anglican parish of St Clement's Church in Old Romney in Kent doubled as priest / vicar plus an additional secret life and career. Dr Syn's fictional Dymchurch parish church in the production, and the Disney studio funded the additional repair of the building to use it as a filming location.\nPart One dealt with the arrival of General Pugh (Geoffrey Keen), who had been ordered by the British Royal government's War Office in London, to smash the smuggling ring along that part of the marshy coast of the North Sea and English Channel, of Kent and Sussex in southeastern England, and prevent the Scarecrow from rescuing a Dymchurch man captured by a Royal Navy press gang (drafting / shanghaiing unlucky merchant ships seamen out on the town as forced recruits/sailors on British warships in port), as bait to trap the infamous smuggler gang leader Scarecrow. Part Two of the series depicted The Scarecrow dealing with the traitorous Joe Ransley (Patrick Wymark. Part Three showed how the Scarecrow rescued Harry Banks (David Buck) and American colonialist Simon Bates (Tony Britton) from British Army General Pugh's (Geoffrey Keen), clutches in Dover Castle.\nWhile originally conceived and edited for American television (and announced in an advertisement by the NBC network in the Tuesday, 9 July 1963, issue of the noted entertainment industry newspaper \"The Hollywood Reporter\", published in Los Angeles, California), \"The Scarecrow of Romney Marsh\" was re-edited for first, a 1963 British theatrical release several months before the American prime-time television debut. Retitled as \"Dr Syn, Alias the Scarecrow\", the British theatrical version was released on a double bill with the animated \"The Sword in the Stone\" a children's historical film about legendary young King Arthur. They both ran during the December 1963 Christmas season (advertised in the January 1964 issue of \"Photoplay\" magazine). This version was shown in continental Europe as well, plus also later in Latin America of Central and South America's television networks through three years later in 1966.\nIn the following decade of the 1970s, the \"Scarecrow of Romney Marsh\" production was re-edited again for its first American theatrical release by the Disney studios, on double bills with both the four-decade old classic of 1937 of the color animated \"Snow White and the Seven Dwarfs\" old feature film and also \"Treasure Island\" (1950, starring Robert Newton, of author Robert Louis Stevenson's classic sea-faring treasure hunt saga to a distant island, in the original novel published 1883. Both involving British ships and seamen.\nThe VHS format of video cassette tape release of the 1980s, sharing the removal of the emeScarecrow's laugh from Terry Gilkyson's memorable title theme song, was expanded to include the story material from all three original broadcast NBC television episodes, while retaining the feature film structure and credits; it was available for a relatively short amount of time. Shortly after the theatrical run in the United States, it was re-edited once more for a two-part presentation and rebroadcast on Disney's television series a decade later in the 1970s, simply omitting the middle episode. The original three-part series was first shown as part of \"Walt Disney's Wonderful World of Color\" on three successive Sunday evenings on 9, 16 and 23 February 1964. Twenty years later, it was included in another broadcast late 1980s \"Wonderful World of Disney\" syndication rerun package, and cablecast in 1990s on the cable television Disney Channel. This version generally followed the storyline of \"The Further Adventures of Dr. Syn\" and made it clear that the good Doctor Syn did not die or stage his own death: at the film's end, he is seen having a cup of tea with the local Squire, (still totally unaware of his true identity and political opinions!!) who admits to now owing a debt of gratitude to the criminal smuggler, outlaw and rebel \"Scarecrow\".\nOn 11 November 2008 The Walt Disney Company released a limited pressing of 39,500 copies of \"The Scarecrow of Romney Marsh\" on the DVD format of video disc for the first time, in a collector's metal tin case. This was a part of the newly issued \"\" collection and was now titled \"Dr. Syn: The Scarecrow of Romney Marsh\". This release sold out in only three weeks. The DVD was made available again for the members of the Disney Movie Club on 17 February 2009. This two-disc set includes the American television version and the original British theatrical release version \"Dr Syn, Alias the Scarecrow\" in a widescreen format. It also includes the original televised introductions by famed artist / cartoonist and studio founder/owner Walt Disney himself, as was then traditional for the regular Sunday night program of 1963\u20131964 (in which he erroneously states that Dr. Syn was an actual historical figure, although English smuggling rings along the coasts were a historical fact in those earlier centuries) and also contains a documentary feature on Disney's interest in filming the historical fiction story. \nIn October 2019, the recently established Disney Movie Club released it on the superior Blu-ray format, this time entitling it as \"The Scarecrow of Romney Marsh\". Its single disc of advanced technology contains all three episodes originally broadcast on television in 1964. It also includes Walt Disney's taped introductions, but unfortunately, none of the supplemental features that appeared on the earlier 2009 released copy.\nOther adaptations.\nMade in 1974, \"Carry On Dick\", of the \"Carry On\" series of films, followed the same premise of a country vicar (Sid James) who is secretly an outlaw, in this case the highwayman Dick Turpin.\nTheatre.\nIn 2001 a stage adaptation titled \"Doctor Syn\" was performed at churches throughout the Romney Marsh, the final night being performed in Dymchurch. The cast featured Daniel Thorndike (the author's son), Michael Fields, Steven Povey and Ben Barton, along with various amateurs from the area.\nBritish composer Adam Pounds has written an opera, \"Syn\", based on the character of Doctor Syn, which was performed at the Mumford Theatre in Cambridge.\nAudio adaptations.\nRufus Sewell read a 10-part audio adaptation combining and abridging \"Doctor Syn on the High Seas\" and \"Doctor Syn Returns\" for BBC Radio, broadcast on BBC Radio 7 in December 2006 and repeated in June 2007.\nA 10-part audio adaptation of \"The Further Adventures of Doctor Syn\" (combining and abridging \"The Further Adventures of Doctor Syn\" and \"The Shadow of Doctor Syn\") read by Rufus Sewell was performed on BBC Radio 7 in December 2007.\nBBC Radio 7 broadcast the six-part series, an abridged reading by Rufus Sewell of the original \"Doctor Syn\" novel, from 4\u201311 January 2010.\nJohn Paul Jones of Led Zeppelin reinterpreted elements of the Doctor Syn story as his \"No Quarter\" fantasy sequence in Led Zeppelin's concert film \"The Song Remains the Same\".\nComic books.\nA three-issue adaptation of the Disney production was published by Gold Key Comics under the \"Scarecrow of Romney Marsh\" title, spanning April 1964 through October 1965.\nA much abridged revision of the adventures of Dr. Syn appeared as a short comic serialized in the monthly publication \"Disney Adventures\". The new story features the heroic Doctor and his young sidekick protecting innocent villagers from corrupt government officials and soldiers. \"Disney Adventures\" would also produce a crossover story with the \"Pirates of the Caribbean\" film franchise, where coincidentally the meek, mild intellectual country priest / vicar \"Dr. Syn\" (secretly infamous English smuggler gang leader and rebel in the \"Scarecrow of Romney Marsh\"), meets up with Captain Jack Sparrow.\nDoctor Syn appears in the \"League of Extraordinary Gentlemen\" series as a member of the league gathered by Lemuel Gulliver. His alter ego, Captain Clegg, also makes appearances, where he is mentioned to have had a brief romantic liaison with future teammate Fanny Hill. In the 2003 film adaptation of \"League\", Dr. Syn can be spotted in one of the portraits hanging on the wall in M's library.\nCultural legacy.\nA \"Days of Syn\" festival is held even-numbered years by Dymchurch residents for fund-raising. The 2006 \"Days of Syn\" was on 26\u201328 August (UK August Bank Holiday weekend) and featured a talk on Dr. Syn at the Anglican church at 6:30\u00a0pm. On Sunday at 3 p.m. there was a church service where Dr. Syn and the cast appeared in period costume. On Monday, starting at the Bowery Hall, scenes were reenacted from \"Doctor Syn\", and again during the day along the Dymchurch shoreline and in the Ocean pub.\nIn 2009, discussions took place to build a 100\u00a0ft high statue of \"The Scarecrow\" on a site in the centre of Romney Marsh. This had not been done by 2016.\nDoctor Syn is also the name given to one of the locomotives on the Romney, Hythe and Dymchurch Railway.\nDoctor Syn also inspired novelist George Chittenden, who captured smuggling on the Kent coast in his debut novel \"The Boy Who Led Them\", which follows the rise and fall of a smuggling gang leader further down the coast in the notorious town of Deal.\nIn 2009, an playfully erotic Afrikaans-language novel, \"Dagtaak\", was published pseudonymously by D R Syn. The author's name and some of the traits of the main character in this novel, allude to the Dr Syn series. Initial advances to produce an arthouse circuit movie from the novel did not come to fruition.\nTwo rooms in the Mermaid Inn, Rye (Dr. Syn's Bed Chamber and Dr. Syn's Lounge) are named after the character."}
{"id": "8709", "revid": "290314", "url": "https://en.wikipedia.org/wiki?curid=8709", "title": "Dhrystone", "text": "Dhrystone is a synthetic computing benchmark program developed in 1984 by Reinhold P. Weicker intended to be representative of system (integer) programming. The Dhrystone grew to become representative of general processor (CPU) performance. The name \"Dhrystone\" is a pun on a different benchmark algorithm called Whetstone, which emphasizes floating point performance.\nWith Dhrystone, Weicker gathered meta-data from a broad range of software, including programs written in FORTRAN, PL/1, SAL, ALGOL 68, and Pascal. He then characterized these programs in terms of various common constructs: procedure calls, pointer indirections, assignments, etc. From this he wrote the Dhrystone benchmark to correspond to a representative mix. Dhrystone was published in Ada, with the C version for Unix developed by Rick Richardson (\"version 1.1\") greatly contributing to its popularity.\nDhrystone vs. Whetstone.\nThe Dhrystone benchmark contains no floating point operations, thus the name is a pun on the then-popular Whetstone benchmark for floating point operations. The output from the benchmark is the number of Dhrystones per second (the number of iterations of the main code loop per second).\nBoth Whetstone and Dhrystone are \"synthetic\" benchmarks, meaning that they are simple programs that are carefully designed to statistically mimic the processor usage of some common set of programs. Whetstone, developed in 1972, originally strove to mimic typical Algol 60 programs based on measurements from 1970, but eventually became most popular in its Fortran version, reflecting the highly numerical orientation of computing in the 1960s.\nIssues addressed by Dhrystone.\nDhrystone's eventual importance as an indicator of general-purpose (\"integer\") performance of new computers made it a target for commercial compiler writers. Various modern compiler static code analysis techniques (such as elimination of dead code: for example, code which uses the processor but produces internal results which are not used or output) make the use and design of synthetic benchmarks more difficult. Version 2.0 of the benchmark, released by Weicker and Richardson in March 1988, had a number of changes intended to foil a range of compiler techniques. Yet it was carefully crafted so as not to change the underlying benchmark. This effort to foil compilers was only partly successful. Dhrystone 2.1, released in May of the same year, had some minor changes and remains the current definition of Dhrystone.\nOther than issues related to compiler optimization, various other issues have been cited with the Dhrystone. Most of these, including the small code size and small data set size, were understood at the time of its publication in 1984. More subtle is the slight over-representation of string operations, which is largely language-related: both Ada and Pascal have strings as normal variables in the language, whereas C does not, so what was simple variable assignment in reference benchmarks became buffer copy operations in the C library. Another issue is that the score reported does not include information which is critical when comparing systems such as which compiler was used, and what optimizations.\nDhrystone remains remarkably resilient as a simple benchmark, but its continuing value in establishing true performance is questionable. It is easy to use, well documented, fully self-contained, well understood, and can be made to work on almost any system. In particular, it has remained in broad use in the embedded computing world, though the recently developed EEMBC benchmark suite, the CoreMark standalone benchmark, HINT, Stream, and even Bytemark are widely quoted and used, as well as more specific benchmarks for the memory subsystem (Cachebench), TCP/IP (TTCP), and many others.\nResults.\nDhrystone may represent a result more meaningfully than MIPS (million instructions per second) because instruction count comparisons between different instruction sets (e.g. RISC vs. CISC) can confound simple comparisons. For example, the same high-level task may require many more instructions on a RISC machine, but might execute faster than a single CISC instruction. Thus, the Dhrystone score counts only the number of program iteration completions per second, allowing individual machines to perform this calculation in a machine-specific way. Another common representation of the Dhrystone benchmark is the DMIPS (Dhrystone MIPS) obtained when the Dhrystone score is divided by 1757 (the number of Dhrystones per second obtained on the VAX 11/780, nominally a 1 MIPS machine).\nAnother way to represent results is in DMIPS/MHz, where DMIPS result is further divided by CPU frequency, to allow for easier comparison of CPUs running at different clock rates.\nShortcomings.\nUsing Dhrystone as a benchmark has pitfalls: "}
{"id": "8710", "revid": "15934865", "url": "https://en.wikipedia.org/wiki?curid=8710", "title": "Durham University (England)", "text": ""}
{"id": "8711", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=8711", "title": "Distilling", "text": ""}
{"id": "8713", "revid": "13667518", "url": "https://en.wikipedia.org/wiki?curid=8713", "title": "Dave Winer", "text": "Dave Winer (born May 2, 1955, in Queens, New York City) is an American software developer, entrepreneur, and writer who resides in New York City. Winer is noted for his contributions to outliners, scripting, content management, and web services, as well as blogging and podcasting. He is the founder of the software companies Living Videotext, Userland Software and Small Picture Inc., a former contributing editor for the Web magazine HotWired, the author of the \"Scripting News\" weblog, a former research fellow at Harvard Law School, and current visiting scholar at New York University's Arthur L. Carter Journalism Institute.\nEarly life and education.\nWiner was born on May 2, 1955, in Queens, New York City, the son of Eve Winer, PhD, a school psychologist, and Leon Winer, PhD, a former professor of the Columbia University Graduate School of Business. Winer is also the grandnephew of German novelist Arno Schmidt and a relative of Hedy Lamarr. He graduated from the Bronx High School of Science in 1972. Winer received a BA in Mathematics from Tulane University in New Orleans in 1976. In 1978 he received an MS in Computer Science from the University of Wisconsin\u2013Madison.\nCareer.\nEarly work in outliners.\nIn 1979 Dave Winer became an employee of Personal Software, where he worked on his own product idea named VisiText, which was his first attempt to build a commercial product around an \"expand and collapse\" outline display and which ultimately established outliners as a software product. In 1981 he left the company and founded Living Videotext to develop this still-unfinished product. The company was based in Mountain View, CA, and grew to more than 50 employees.\nThinkTank, which was based on VisiText, was released in 1983 for Apple II and was promoted as an \"idea processor.\" It became the \"first popular outline processor, the one that made the term generic.\" A ThinkTank release for the IBM PC followed in 1984, as well as releases for the Macintosh 128K and 512K. Ready, a RAM resident outliner for the IBM PC released in 1985, was commercially successful but soon succumbed to the competing Sidekick product by Borland. MORE, released for Apple's Macintosh in 1986, combined an outliner and a presentation program. It became \"uncontested in the marketplace\" and won the MacUser's Editor's Choice Award for \"Best Product\" in 1986.\nIn 1987, at the height of the company's success, Winer sold Living Videotext to Symantec for an undisclosed but substantial transfer of stock that \"made his fortune.\" Winer continued to work at Symantec's Living Videotext division, but after six months he left the company in pursuit of other challenges.\nYears at UserLand.\nWiner founded UserLand Software in 1988 and served as the company's CEO until 2002.\nUserLand's original flagship product, Frontier, was a system-level scripting environment for the Mac. Winer's pioneering weblog, \"Scripting News\", takes its name from this early interest. Frontier was an outliner-based scripting language, echoing Winer's longstanding interest in outliners and anticipating code-folding editors of the late 1990s.\nWiner became interested in web publishing while helping automate the production process of the strikers' online newspaper during San Francisco's newspaper strike of November 1994, According to Newsweek, through this experience, he \"revolutionized Net publishing.\" Winer subsequently shifted the company's focus to online publishing products, enthusiastically promoting and experimenting with these products while building his websites and developing new features. One of these products was Frontier's NewsPage Suite of 1997, which supported the publication of Winer's \"Scripting News\" and was adopted by a handful of users who \"began playing around with their own sites in the Scripting News vein.\" These users included notably Chris Gulker and Jorn Barger, who envisaged blogging as a networked practice among users of the software.\nWiner was named a Seybold Fellow in 1997, to assist the executives and editors that comprised the Seybold Institute in ensuring \"the highest quality and topicality\" in their educational program, the Seybold Seminars; the honor was bestowed for his \"pioneering work in web-based publishing systems.\" Keen to enter the \"competitive arena of high-end Web development,\" Winer then came to collaborate with Microsoft and jointly developed the XML-RPC protocol. This led to the creation of SOAP, which he co-authored with Microsoft's Don Box, Bob Atkinson, and Mohsen Al-Ghosein.\nIn December 1997, acting on the desire to \"offer much more timely information,\" Winer designed and implemented an XML syndication format for use on his \"Scripting News\" weblog, thus making an early contribution to the history of web syndication technology. By December 2000, competing dialects of RSS included several varieties of Netscape's RSS, Winer's RSS 0.92, and an RDF-based RSS 1.0. Winer continued to develop the branch of the RSS fork originating from RSS 0.92, releasing in 2002 a version called RSS 2.0. Winer's advocacy of web syndication in general and RSS 2.0 in particular convinced many news organizations to syndicate their news content in that format. For example, in early 2002 \"The New York Times\" entered an agreement with UserLand to syndicate many of their articles in RSS 2.0 format. Winer resisted calls by technologists to have the shortcomings of RSS 2.0 improved. Instead, he froze the format and turned its ownership over to Harvard University.\nWith products and services based on UserLand's Frontier system, Winer became a leader in blogging tools from 1999 onward, as well as a \"leading evangelist of weblogs.\" In 2000 Winer developed the Outline Processor Markup Language OPML, an XML format for outlines, which originally served as the native file format for Radio UserLand's outliner application and has since been adopted for other uses, the most common being to exchange lists of web feeds between web feed aggregators. UserLand was the first to add an \"enclosure\" tag in its RSS, modifying its blog software and its aggregator so that bloggers could easily link to an audio file (see podcasting and history of podcasting).\nIn February 2002 Winer was named one of the \"Top Ten Technology Innovators\" by InfoWorld.\nIn June 2002 Winer underwent life-saving bypass surgery to prevent a heart attack and as a consequence stepped down as CEO of UserLand shortly after. He remained the firm's majority shareholder, however, and claimed personal ownership of Weblogs.com.\nWriter.\nAs \"one of the most prolific content generators in Web history,\" Winer has enjoyed a long career as a writer and has come to be counted among Silicon Valley's \"most influential web voices.\"\nWiner started \"DaveNet\", \"a stream-of-consciousness newsletter distributed by e-mail\" in November 1994 and maintained Web archives of the \"goofy and informative\" 800-word essays since January 1995, which earned him a Cool Site of the Day award in March 1995. From the start, the \"Internet newsletter\" \"DaveNet\" was widely read among industry leaders and analysts, who experienced it as a \"real community.\" Dissatisfied with the quality of the coverage that the Mac and, especially, his own Frontier software received in the trade press, Winer saw \"DaveNet\" as an opportunity to \"bypass\" the conventional news channels of the software business. Satisfied with his success, he \"reveled in the new direct email line he had established with his colleagues and peers, and in his ability to circumvent the media.\" In the early years, Winer often used \"DaveNet\" to vent his grievances against Apple's management, and as a consequence of his strident criticism came to be seen as \"the most notorious of the disgruntled Apple developers.\" Redacted \"DaveNet\" columns were published weekly by the web magazine \"HotWired\" between June 1995 and May 1996. \"DaveNet\" was discontinued in 2004.\nWiner's \"Scripting News\", described as \"one of the [web's] oldest blogs,\" launched in February 1997 and earned him titles such as \"protoblogger\" and \"forefather of blogging.\" \"Scripting News\" started as \"a home for links, offhand observations, and ephemera\" and allowed Winer to mix \"his roles as a widely read pundit and an ambitious entrepreneur.\" Offering an \"as-it-happened portrait of the work of writing software for the Web in the 1990s,\" the site became an \"established must-read for industry insiders.\" \"Scripting News\" continues to be updated regularly.\nVisiting scholar positions.\nWiner spent one year as a resident fellow at the Harvard Law School's Berkman Center for Internet &amp; Society, where he worked on using weblogs in education. While there, he launched \"Weblogs at Harvard Law School\" using UserLand software, and held the first BloggerCon conferences. Winer's fellowship ended in June 2004.\nIn 2010 Winer was appointed visiting scholar at New York University's Arthur L. Carter Journalism Institute.\nReturn to outliners.\nOn December 19, 2012, Winer co-founded Small Picture, Inc. with Kyle Shank; Small Picture is a corporation that builds two outlining products, Little Outliner and Fargo. Little Outliner, an entry-level outliner designed to teach new users about outliners, which launched on March 25, 2013. Fargo, the company's \"primary product\", launched less than a month later, on April 17, 2013. Fargo is a free browser-based outliner which syncs with a user's Dropbox account. Small Picture has stated that in future it may offer paid-for services to Fargo users. Fargo was retired at the end of September 2017.\nProjects and activities.\n24 Hours of Democracy.\nIn February 1996, while working as a columnist for HotWired, Winer organized 24 Hours of Democracy, an online protest against the recently passed Communications Decency Act. As part of the protest, over 1,000 people, among them Microsoft chairman Bill Gates, posted essays to the Web on the subject of democracy, civil liberty and freedom of speech.\nEdit This Page.\nIn December 1999, Winer became the \"proprietor of a growing free blog service\" at EditThisPage.com, hosting \"approximately 20,000 sites\" in February 2001. The service closed in December 2005.\nPodcasting.\nWiner has been given \"credit for the invention of the podcasting model.\" Having received user requests for audioblogging features since October 2000, especially from Adam Curry, Winer decided to include new functionality in RSS 0.92 by defining a new element called \"enclosure,\" which would pass the address of a media file to the RSS aggregator. He demonstrated the RSS enclosure feature on January 11, 2001, by enclosing a Grateful Dead song in his \"Scripting News\" weblog.\nWiner's weblogging product, Radio Userland, the program favored by Curry, had a built-in aggregator and thus provided both the \"send\" and \"receive\" components of what was then called audioblogging. In July 2003 Winer challenged other aggregator developers to provide support for enclosures. In October 2003, Kevin Marks demonstrated a script to download RSS enclosures and pass them to iTunes for transfer to an iPod. Curry then offered an RSS-to-iPod script that moved MP3 files from Radio UserLand to iTunes. The term \"podcasting\" was suggested by Ben Hammersley in February 2004.\nWiner also has an occasional podcast, Morning Coffee Notes, which has featured guests such as Doc Searls, Mike Kowalchik, Jason Calacanis, Steve Gillmor, Peter Rojas, Cecile Andrews, Adam Curry, Betsy Devine and others.\nBloggerCon.\nBloggerCon is a user-focused conference for the blogger community. BloggerCon I (October 2003) and II (April 2004), were organized by Dave Winer and friends at Harvard Law School's Berkman Center for the Internet and Society in Cambridge, Mass. BloggerCon III met at Stanford Law School on November 6, 2004.\nWeblogs.com.\nWeblogs.com provided a free ping-server used by many blogging applications, as well as free hosting to many bloggers. After leaving Userland, Winer claimed personal ownership of the site, and in mid-June 2004 he shut down its free blog-hosting service, citing lack of resources and personal problems. A swift and orderly migration off Winer's server was facilitated by Rogers Cadenhead, whom Winer then hired to port the server to a more stable platform. In October 2005, VeriSign bought the Weblogs.com ping-server from Winer and promised that its free services would remain free. The podcasting-related web site audio.weblogs.com was also included in the $2.3\u00a0million deal.\nShare your OPML.\nWiner opened his self-described \"commons for sharing outlines, feeds, and taxonomy\" in May 2006. The site allowed users to publish and syndicate blogrolls and aggregator subscriptions using OPML. Winer suspended its service in January 2008.\nRebooting the News.\nSince 2009, Winer has collaborated with New York University's associate professor of journalism Jay Rosen on \"Rebooting the News\", a weekly podcast on technology and innovation in journalism. It was announced on July 1, 2011, that the show would be on break, as NYU itself was, from June to September. However, no new episodes have been released since, making show #94 released on May 23, 2011, the last."}
{"id": "8714", "revid": "754658", "url": "https://en.wikipedia.org/wiki?curid=8714", "title": "December 10", "text": ""}
{"id": "8715", "revid": "11555324", "url": "https://en.wikipedia.org/wiki?curid=8715", "title": "Taiko", "text": " are a broad range of Japanese percussion instruments. In Japanese, the term refers to any kind of drum, but outside Japan, it is used specifically to refer to any of the various Japanese drums called and to the form of ensemble drumming more specifically called . The process of constructing varies between manufacturers, and the preparation of both the drum body and skin can take several years depending on the method.\n have a mythological origin in Japanese folklore, but historical records suggest that were introduced to Japan through Chinese and Korean cultural influence as early as the 6th century CE; pottery from the Haniwa period depicting drums has also been found. Some are similar to instruments originating from India. Archaeological evidence also supports the view that were present in Japan during the 6th century in the Kofun period. Their function has varied throughout history, ranging from communication, military action, theatrical accompaniment, religious ceremony and concert performances. In modern times, have also played a central role in social movements for minorities both within and outside Japan.\n performance, characterized by an ensemble playing on different drums, was developed in 1951 through the work of Daihachi Oguchi and later in 1961 by the Ondekoza, and was made later popular with many other groups copying the format of Ondekoza such as Kodo, Yamato, Tao, Taikoza, Fuun No Kai, Sukeroku Taiko, etc. Other performance styles, such as , have also emerged from specific communities in Japan. performance groups are active not only in Japan, but also in the United States, Australia, Canada, Europe, Taiwan, and Brazil. Taiko performance consists of many components in technical rhythm, form, stick grip, clothing, and the particular instrumentation. Ensembles typically use different types of barrel-shaped as well as smaller . Many groups accompany the drums with vocals, strings, and woodwind instruments.\nHistory.\nOrigin.\nThe origin of the and its variants is unclear, though there have been many suggestions. Historical accounts, of which the earliest date from 588\u00a0CE, note that young Japanese men traveled to Korea to study the , a drum that originated in South China. This study and appropriation of Chinese instruments may have influenced the emergence of . Certain court music styles, especially and , arrived in Japan through both China and Korea. In both traditions, dancers were accompanied by several instruments that included drums similar to . Certain percussive patterns and terminology in , an early dance and music style in Japan, in addition to physical features of the , also reflect influence from both China and India on drum use in performance.\nArchaeological evidence shows that were used in Japan as early as the 6th century CE, during the latter part of the Kofun period, and were likely used for communication, in festivals, and in other rituals. This evidence was substantiated by the discovery of haniwa statues in the Sawa District of Gunma Prefecture. Two of these figures are depicted playing drums; one of them, wearing skins, is equipped with a barrel-shaped drum hung from his shoulder and uses a stick to play the drum at hip height. This statue is titled \"Man Beating the \" and is considered the oldest evidence of performance in Japan. Similarities between the playing style demonstrated by this and known music traditions in China and Korea further suggest influences from these regions.\nThe , the second-oldest book of Japanese classical history, contains a mythological story describing the origin of . The myth tells how Amaterasu, who had sealed herself inside a cave in anger, was beckoned out by an elder goddess Ame-no-Uzume when others had failed. Ame-no-Uzume accomplished this by emptying out a barrel of sake and dancing furiously on top of it. Historians regard her performance as the mythological creation of music.\nUse in warfare.\nIn feudal Japan, were often used to motivate troops, call out orders or announcements, and set a marching pace; marches were usually set to six paces per beat of the drum. During the 16th-century Warring States period, specific drum calls were used to communicate orders for retreating and advancing. Other rhythms and techniques were detailed in period texts. According to the war chronicle , nine sets of five beats would summon an ally to battle, while nine sets of three beats, sped up three or four times, was the call to advance and pursue an enemy. Folklore from the 16th century on the legendary 6th-century Emperor Keitai offers a story that he obtained a large drum from China, which he named . The Emperor was thought to have used it to both encourage his own army and intimidate his enemies.\nIn traditional settings.\n have been incorporated in Japanese theatre for rhythmic needs, general atmosphere, and in certain settings decoration. In the kabuki play \"The Tale of Shiroishi and the Taihei Chronicles\", scenes in the pleasure quarters are accompanied by to create dramatic tension. Noh theatre also features music, where performance consists of highly specific rhythmic patterns. The school of drumming, for example, contains 65 basic patterns in addition to 25 special patterns; these patterns are categorized in several classes. Differences between these patterns include changes in tempo, accent, dynamics, pitch, and function in the theatrical performance. Patterns are also often connected together in progressions.\n continue to be used in , a classical music tradition typically performed at the Tokyo Imperial Palace in addition to local temples and shrines. In , one component of the art form is traditional dance, which is guided in part by the rhythm set by the .\n have played an important role in many local festivals across Japan. They are also used to accompany religious ritual music. In , a category of music and dances stemming from Shinto practices, frequently appear alongside other performers during local festivals. In Buddhist traditions, are used for ritual dances as part of the Bon Festival. , along with other instruments, are featured atop towers that are adorned with red-and-white cloth and serve to provide rhythms for the dancers who are encircled around the performers.\nIn addition to the instruments, the term also refers to the performance itself, and commonly to one style called , or ensemble-style playing (as opposed to festival performances, rituals, or theatrical use of the drums). was developed by Daihachi Oguchi in 1951. He is considered a master performer and helped transform performance from its roots in traditional settings in festivals and shrines. Oguchi was trained as a jazz musician in Nagano, and at one point, a relative gave him an old piece of written music. Unable to read the traditional and esoteric notation, Oguchi found help to transcribe the piece, and on his own added rhythms and transformed the work to accommodate multiple taiko players on different-sized instruments. Each instrument served a specific purpose that established present-day conventions in performance.\nOguchi's ensemble, Osuwa Daiko, incorporated these alterations and other drums into their performances. They also devised novel pieces that were intended for non-religious performances. Several other groups emerged in Japan through the 1950s and 1960s. Oedo Sukeroku Daiko was formed in Tokyo in 1959 under Seid\u014d Kobayashi, and has been referred to as the first group who toured professionally. Globally, performance became more visible during the 1964 Summer Olympics in Tokyo, when it was featured during the Festival of Arts event.\n was also developed through the leadership of , who gathered young men who were willing to devote their entire lifestyle to playing and took them to Sado Island for training where Den and his family had settled in 1968. Den chose the island based on a desire to reinvigorate the folk arts in Japan, particularly ; he became inspired by a drumming tradition unique to Sado called that required considerable strength to play well. Den called the group \"Za Ondekoza\" or Ondekoza for short, and implemented a rigorous set of exercises for its members including long-distance running. In 1975, Ondekoza was the first group to tour in the United States. Their first performance occurred just after the group finished running the Boston Marathon while wearing their traditional uniforms. In 1981, some members of Ondekoza split from Den and formed another group called Kodo under the leadership of Eitetsu Hayashi. Kodo continued to use Sado Island for rigorous training and communal living, and went on to popularize through frequent touring and collaborations with other musical performers. Kodo is one of the most recognized groups both in Japan and worldwide.\nEstimates of the number of groups in Japan vary to up to 5,000 active groups in Japan, but more conservative assessments place the number closer to 800 based on membership in the Nippon Taiko Foundation, the largest national organization of groups. Some pieces that have emerged from early groups that continue to be performed include Yatai-bayashi from Ondekoza, from Osuwa Daiko, and from Kodo.\nCategorization.\nTaiko have been developed into a broad range of percussion instruments that are used in both Japanese folk and classical musical traditions. An early classification system based on shape and tension was advanced by Francis Taylor Piggott in 1909. Taiko are generally classified based on the construction process, or the specific context in which the drum is used, but some are not classified, such as the toy den-den daiko.\nWith few exceptions, taiko have a drum shell with heads on both sides of the body, and a sealed resonating cavity. The head may be fastened to the shell using a number of different systems, such as using ropes. Taiko may be either tunable or non-tunable depending on the system used.\nTaiko are categorized into three types based on construction process. \"By\u014d-uchi-daiko\" are constructed with the drumhead nailed to the body. \"Shime-daiko\" are classically constructed with the skin placed over iron or steel rings, which are then tightened with ropes. Contemporary \"shime-daiko\" are tensioned using bolts or turnbuckles systems attached to the drum body. \"Tsuzumi\" are also rope-tensioned drums, but have a distinct hourglass shape and their skins are made using deerskin.\n\"By\u014d-uchi-daiko\" were historically made only using a single piece of wood; they continue to be made in this manner, but are also constructed from staves of wood. Larger drums can be made using a single piece of wood, but at a much greater cost due to the difficulty in finding appropriate trees. The preferred wood is the Japanese zelkova or \"keyaki\", but a number of other woods, and even wine barrels, have been used to create taiko. \"By\u014d-uchi-daiko\" cannot be tuned.\nThe typical \"by\u014d-uchi-daiko\" is the \"nagad\u014d-daiko\", an elongated drum that is roughly shaped like a wine barrel. \"Nagad\u014d-daiko\" are available in a variety of sizes, and their head diameter is traditionally measured in shaku (units of roughly 30\u00a0cm). Head diameters range from . are the smallest of these drums and are usually about in diameter. The is a medium-sized \"nagad\u014d-daiko\" ranging from , and weighing about . vary in size, and are often as large as in diameter. Some \"\u014d-daiko\" are difficult to move due to their size, and therefore permanently remain inside the performance space, such as temple or shrine. \"\u014c-daiko\" means \"large drum\" and for a given ensemble, the term refers to their largest drum. The other type of \"by\u014d-uchi-daiko\" is called a and can be any drum constructed such that the head diameter is greater than the length of the body.\n\"Shime-daiko\" are a set of smaller, roughly snare drum-sized instrument that are tunable. The tensioning system usually consists of hemp cords or rope, but bolt or turnbuckle systems have been used as well. , sometimes referred to as \"taiko\" in the context of theater, have thinner heads than other kinds of shime-daiko. The head includes a patch of deerskin placed in the center, and in performance, drum strokes are generally restricted to this area. The is a heavier type of \"shime-daiko\". They are available in sizes 1\u20135, and are named according to their number: \"namitsuke\"\u00a0(1), \"nich\u014d-gakke\"\u00a0(2), \"sanch\u014d-gakke\"\u00a0(3), \"yonch\u014d-gakke\"\u00a0(4), and \"goch\u014d-gakke\"\u00a0(5). The \"namitsuke\" has the thinnest skins and the shortest body in terms of height; thickness and tension of skins, as well as body height, increase toward the \"goch\u014d-gakke\". The head diameters of all \"shime-daiko\" sizes are around .\n is a type of racket-shaped Japanese drum. It is the only Japanese traditional drum without a sound box and only one skin. It is played with a drumstick while hanging it with the other hand.\n\"Oked\u014d-daiko\" or simply \"oked\u014d\", are a type of \"shime-daiko\" that are stave-constructed using narrower strips of wood, have a tube-shaped frame. Like other \"shime-daiko\", drum heads are attached by metal hoops and fastened by rope or cords. \"Oked\u014d\" can be played using the same drumsticks (called \"bachi\") as \"shime-daiko\", but can also be hand-played. \"Oked\u014d\" come in short- and long-bodied types.\n\"Tsuzumi\" are a class of hourglass-shaped drums. The drum body is shaped on a spool and the inner body carved by hand. Their skins can be made from cowhide, horsehide, or deerskin. While the \"\u014d-tsuzumi\" skins are made from cowhide, \"ko-tsuzumi\" are made from horsehide. While some classify \"tsuzumi\" as a type of taiko, others have described them as a drum entirely separate from taiko.\nTaiko can also be categorized by the context in which they are used. The \"miya-daiko\", for instance, is constructed in the same manner as other \"by\u014d-uchi-daiko\", but is distinguished by an ornamental stand and is used for ceremonial purposes at Buddhist temples. The (a \"ko-daiko\") and (a \"nagad\u014d-daiko\" with a cigar-shaped body) are used in sumo and festivals respectively.\nSeveral drums, categorized as \"gagakki\", are used in the Japanese theatrical form, gagaku. The lead instrument of the ensemble is the kakko, which is a smaller \"shime-daiko\" with heads made of deerskin, and is placed horizontally on a stand during performance. A \"tsuzumi\", called the \"san-no-tsuzumi\" is another small drum in gagaku that is placed horizontally and struck with a thin stick. are the largest drums of the ensemble, and have heads that are about in diameter. During performance, the drum is placed on a tall pedestals and surrounded by a rim decoratively painted with flames and adorned with mystical figures such as wyverns. \"Dadaiko\" are played while standing, and are usually only played on the downbeat of the music. The is a smaller drum that produces a lower sound, its head measuring about in diameter. It is used in ensembles that accompany bugaku, a traditional dance performed at the Tokyo Imperial Palace and in religious contexts. \"Tsuri-daiko\" are suspended on a small stand, and are played sitting down. \"Tsuri-daiko\" performers typically use shorter mallets covered in leather knobs instead of bachi. They can be played simultaneously by two performers; while one performer plays on the head, another performer uses bachi on the body of the drum.\nThe larger \"\u014d-tsuzumi\" and smaller \"ko-tsuzumi\" are used in the opening and dances of Noh theater. Both drums are struck using the fingers; players can also adjust pitch by manually applying pressure to the ropes on the drum. The color of the cords of these drums also indicates the skill of the musician: Orange and red for amateur players, light blue for performers with expertise, and lilac for masters of the instrument. \"Nagauta-shime daiko\" or \"uta daiko\" are also featured in Noh performance.\nMany taiko in Noh are also featured in kabuki performance and are used in a similar manner. In addition to the \"\u014d-tsuzumi\", \"ko-tsuzumi\", and \"nagauta-shime daiko\", Kabuki performances make use of the larger \"\u014d-daiko\" offstage to help set the atmosphere for different scenes.\nConstruction.\nProcess.\nTaiko construction has several stages, including making and shaping of the drum body (or shell), preparing the drum skin, and tuning the skin to the drumhead. Variations in the construction process often occur in the latter two parts of this process. Historically, \"by\u014d-uchi-daiko\" were crafted from trunks of the Japanese zelkova tree that were dried out over years, using techniques to prevent splitting. A master carpenter then carved out the rough shape of the drum body with a chisel; the texture of the wood after carving softened the tone of the drum. In contemporary times, taiko are carved out on a large lathe using wood staves or logs that can be shaped to fit drum bodies of various sizes. Drumheads can be left to air-dry over a period of years, but some companies use large, smoke-filled warehouses to hasten the drying process. After drying is complete, the inside of the drum is worked with a deep-grooved chisel and sanded. Lastly, handles are placed onto the drum. These are used to carry smaller drums and they serve an ornamental purpose for larger drums.\nThe skins or heads of taiko are generally made from cowhide from Holstein cows aged about three or four years. Skins also come from horses, and bull skin is preferred for larger drums. Thinner skins are preferred for smaller taiko, and thicker skins are used for larger ones. On some drumheads, a patch of deer skin placed in the center serves as the target for many strokes during performance. Before fitting it to the drum body the hair is removed from the hide by soaking it in a river or stream for about a month; winter months are preferred as colder temperatures better facilitate hair removal. To stretch the skin over the drum properly, one process requires the body to be held on a platform with several hydraulic jacks underneath it. The edges of the cowhide are secured to an apparatus below the jacks, and the jacks stretch the skin incrementally to precisely apply tension across the drumhead. Other forms of stretching use rope or cords with wooden dowels or an iron wheel to create appropriate tension. Small tension adjustments can be made during this process using small pieces of bamboo that twist around the ropes. Particularly large drumheads are sometimes stretched by having several workers, clad in stockings, hop rhythmically atop it, forming a circle along the edge. After the skin has dried, tacks, called \"by\u014d\", are added to the appropriate drums to secure it; \"ch\u016b-daiko\" require about 300 of them for each side. After the body and skin have been finished, excess hide is cut off and the drum can be stained as needed.\nDrum makers.\nSeveral companies specialize in the production of taiko. One such company that created drums exclusively for the Emperor of Japan, Miyamoto Unosuke Shoten in Tokyo, has been making taiko since 1861. The Asano Taiko Corporation is another major taiko-producing organization, and has been producing taiko for over 400 years. The family-owned business started in Matt\u014d, Ishikawa, and, aside from military equipment, made taiko for Noh theater and later expanded to creating instruments for festivals during the Meiji period. Asano currently maintains an entire complex of large buildings referred to as Asano Taiko Village, and the company reports producing up to 8000 drums each year. As of 2012, there is approximately one major taiko production company in each prefecture of Japan, with some regions having several companies. Of the manufacturers in Naniwa, Taikoya Matab\u0113 is one of the most successful and is thought to have brought considerable recognition to the community and attracted many drum makers there. Umetsu Daiko, a company that operates in Hakata, has been producing taiko since 1821.\nPerformance.\nTaiko performance styles vary widely across groups in terms of the number of performers, repertoire, instrument choices, and stage techniques. Nevertheless, a number of early groups have had broad influence on the tradition. For instance, many pieces developed by Ondekoza and Kodo are considered standard in many taiko groups.\nForm.\nKata is the posture and movement associated with taiko performance. The notion is similar to that of kata in martial arts: for example, both traditions include the idea that the hara is the center of being. Author Shawn Bender argues that kata is the primary feature that distinguishes different taiko groups from one another and is a key factor in judging the quality of performance. For this reason, many practice rooms intended for taiko contain mirrors to provide visual feedback to players. An important part of kata in taiko is keeping the body stabilized while performing and can be accomplished by keeping a wide, low stance with the legs, with the left knee bent over the toes and keeping the right leg straight. It is important that the hips face the drum and the shoulders are relaxed. Some teachers note a tendency to rely on the upper body while playing and emphasize the importance of the holistic use of the body during performance.\nSome groups in Japan, particularly those active in Tokyo, also emphasize the importance of the lively and spirited \"iki\" aesthetic. In taiko, it refers to very specific kinds of movement while performing that evoke the sophistication stemming from the mercantile and artisan classes active during the Edo period (1603\u20131868).\nThe sticks for playing taiko are called \"bachi\", and are made in various sizes and from different kinds of wood such as white oak, bamboo, and Japanese magnolia. \"Bachi\" are also held in a number of different styles. In \"kumi-daiko\", it is common for a player to hold their sticks in a relaxed manner between the V-shape of the index finger and thumb, which points to the player. There are other grips that allow performers to play much more technically difficult rhythms, such as the \"shime\" grip, which is similar to a matched grip: the \"bachi\" are gripped at the back end, and the fulcrum rests between the performer's index finger and thumb, while the other fingers remain relaxed and slightly curled around the stick.\nPerformance in some groups is also guided by principles based on Zen Buddhism. For instance, among other concepts, the San Francisco Taiko Dojo is guided by emphasizing communication, respect, and harmony. The way the \"bachi\" are held can also be significant; for some groups, \"bachi\" represent a spiritual link between the body and the sky. Some physical parts of taiko, like the drum body, its skin, and the tacks also hold symbolic significance in Buddhism.\nInstrumentation.\n\"Kumi-daiko\" groups consist primarily of percussive instruments where each of the drums plays a specific role. Of the different kinds of taiko, the most common in groups is the \"nagad\u014d-daiko\". \"Ch\u016b-daiko\" are common in taiko groups and represent the main rhythm of the group, whereas \"shime-daiko\" set and change tempo. A \"shime-daiko\" often plays the Jiuchi, a base rhythm holding together the ensemble. \"\u014c-daiko\" provide a steady, underlying pulse and serve as a counter-rhythm to the other parts. It is common for performances to begin with a single stroke roll called an \"\". The player starts slowly, leaving considerable space between strikes, gradually shortening the interval between hits, until the drummer is playing a rapid roll of hits. Oroshi are also played as a part of theatrical performance, such as in Noh theater.\nDrums are not the only instruments played in the ensemble; other Japanese instruments are also used. Other kinds of percussion instruments include the , a hand-sized gong played with a small mallet. In kabuki, the shamisen, a plucked string instrument, often accompanies taiko during the theatrical performance. \"Kumi-daiko\" performances can also feature woodwinds such as the shakuhachi and the shinobue.\nVoiced calls or shouts called kakegoe and kiai are also common in taiko performance. They are used as encouragement to other players or cues for transition or change in dynamics such as an increase in tempo. In contrast, the philosophical concept of ma, or the space between drum strikes, is also important in shaping rhythmic phrases and creating appropriate contrast.\nClothing.\nThere is a wide variety of traditional clothing that players wear during taiko performance. Common in many \"kumi-daiko\" groups is the use of the happi, a decorative, thin-fabric coat, and traditional headbands called hachimaki. Tabi, , and are also typical. During his time with the group Ondekoza, Eitetsu Hayashi suggested that a loincloth called a fundoshi be worn when performing for French fashion designer Pierre Cardin, who saw Ondekoza perform for him in 1975. The Japanese group Kodo has sometimes worn fundoshi for its performances.\nEducation.\nTaiko performance is generally taught orally and through demonstration. Historically, general patterns for taiko were written down, such as in the 1512 encyclopedia called the \"Taigensho\", but written scores for taiko pieces are generally unavailable. One reason for the adherence to an oral tradition is that, from group to group, the rhythmic patterns in a given piece are often performed differently. Furthermore, ethnomusicologist William\u00a0P. Malm observed that Japanese players within a group could not usefully predict one another using written notation, and instead did so through listening. In Japan, printed parts are not used during lessons.\nOrally, patterns of onomatopoeia called kuchi sh\u014dga are taught from teacher to student that convey the rhythm and timbre of drum strikes for a particular piece. For example, represents a single strike to the center of the drum, where as represents two successive strikes, first by the right and then the left, and lasts the same amount of time as one \"don\" strike. Some taiko pieces, such as \"Yatai-bayashi\", include patterns that are difficult to represent in Western musical notation. The exact words used can also differ from region to region.\nMore recently, Japanese publications have emerged in an attempt to standardize taiko performance. The Nippon Taiko Foundation was formed in 1979; its primary goals were to foster good relations among taiko groups in Japan and to both publicize and teach how to perform taiko. Daihachi Oguchi, the leader of the Foundation, wrote \"Japan Taiko\" with other teachers in 1994 out of concern that correct form in performance would degrade over time. The instructional publication described the different drums used in \"kumi-daiko\" performance, methods of gripping, correct form, and suggestions on instrumentation. The book also contains practice exercises and transcribed pieces from Oguchi's group, Osuwa Daiko. While there were similar textbooks published before 1994, this publication had much more visibility due to the Foundation's scope.\nThe system of fundamentals \"Japan Taiko\" put forward was not widely adopted because taiko performance varied substantially across Japan. An updated 2001 publication from the Foundation, called the , describes regional variations that depart from the main techniques taught in the textbook. The creators of the text maintained that mastering a set of prescribed basics should be compatible with learning local traditions.\nRegional styles.\nAside from \"kumi-daiko\" performance, a number of folk traditions that use taiko have been recognized in different regions in Japan. Some of these include from Sado Island, ' from the town of Kokura, and ' from Iwate Prefecture.\nEisa.\nA variety of folk dances originating from Okinawa, known collectively as eisa, often make use of the taiko. Some performers use drums while dancing, and generally speaking, perform in one of two styles: groups on the Yokatsu Peninsula and on Hamahiga Island use small, single-sided drums called whereas groups near the city of Okinawa generally use \"shime-daiko\". Use of \"shime-daiko\" over \"p\u0101ranku\" has spread throughout the island, and is considered the dominant style. Small \"nagad\u014d-daiko\", referred to as \"\u014d-daiko\" within the tradition, are also used and are worn in front of the performer. These drum dances are not limited to Okinawa and have appeared in places containing Okinawan communities such as in S\u00e3o Paulo, Hawaii, and large cities on the Japanese mainland.\nHachij\u014d-daiko.\n is a taiko tradition originating on the island of Hachij\u014d-jima. Two styles of \"Hachij\u014d-daiko\" emerged and have been popularized among residents: an older tradition based on a historical account, and a newer tradition influenced by mainland groups and practiced by the majority of the islanders.\nThe \"Hachij\u014d-daiko\" tradition was documented as early as 1849 based on a journal kept by an exile named Kakuso Kizan. He mentioned some of its unique features, such as \"a taiko is suspended from a tree while women and children gathered around\", and observed that a player used either side of the drum while performing. Illustrations from Kizan's journal show features of \"Hachij\u014d-daiko\". These illustrations also featured women performing, which is unusual as taiko performance elsewhere during this period was typically reserved for men. Teachers of the tradition have noted that the majority of its performers were women; one estimate asserts that female performers outnumbered males by three to one.\nThe first style of Hachij\u014d-daiko is thought to descend directly from the style reported by Kizan. This style is called \"Kumaoji-daiko\", named after its creator Okuyama Kumaoji, a central performer of the style. \"Kumaoji-daiko\" has two players on a single drum, one of whom, called the , provides the underlying beat. The other player, called the , builds on this rhythmical foundation with unique and typically improvised rhythms. While there are specific types of underlying rhythms, the accompanying player is free to express an original musical beat. \"Kumaoji-daiko\" also features an unusual positioning for taiko: the drums are sometimes suspended from ropes, and historically, sometimes drums were suspended from trees.\nThe contemporary style of \"Hachij\u014d-daiko\" is called , which differs from \"Kumaoji-daiko\" in multiple ways. For instance, while the lead and accompanying roles are still present, \"shin-daiko\" performances use larger drums exclusively on stands. \"Shin-daiko\" emphasizes a more powerful sound, and consequently, performers use larger bachi made out of stronger wood. Looser clothing is worn by \"shin-daiko\" performers compared to kimono worn by \"Kumaoji-daiko\" performers; the looser clothing in \"shin-daiko\" allow performers to adopt more open stances and larger movements with the legs and arms. Rhythms used for the accompanying \"shita-by\u014dshi\" role can also differ. One type of rhythm, called \"y\u016bkichi\", consists of the following: \nThis rhythm is found in both styles, but is always played faster in \"shin-daiko\". Another type of rhythm, called \"honbadaki\", is unique to \"shin-daiko\" and also contains a song which is performed in standard Japanese.\nMiyake-daiko.\n is a style that has spread amongst groups through Kodo, and is formally known as . The word \"miyake\" comes from Miyake-jima, part of the Izu Islands, and the word \"Kamitsuki\" refers to the village where the tradition came from. Miyake-style taiko came out of performances for \u2014 a traditional festival held annually in July on Miyake Island since 1820 honoring the deity Gozu Tenn\u014d. In this festival, players perform on taiko while portable shrines are carried around town. The style itself is characterized in a number of ways. A \"nagad\u014d-daiko\" is typically set low to the ground and played by two performers, one on each side; instead of sitting, performers stand and hold a stance that is also very low to the ground, almost to the point of kneeling.\nOutside Japan.\nAustralia.\nTaiko groups in Australia began forming in the 1990s. The first group, called Ataru Taru Taiko, was formed in 1995 by Paulene Thomas, Harold Gent, and Kaomori Kamei. TaikOz was later formed by percussionist Ian Cleworth and Riley Lee, a former Ondekoza member, and has been performing in Australia since 1997. They are known for their work in generating interest in performing taiko among Australian audiences, such as by developing a complete education program with both formal and informal classes, and have a strong fan base. Cleworth and other members of the group have developed several original pieces.\nBrazil.\nThe introduction of \"kumi-daiko\" performance in Brazil can be traced back to the 1970s and 1980s in S\u00e3o Paulo. Tangue Setsuko founded an eponymous taiko dojo and was Brazil's first taiko group; Setsuo Kinoshita later formed the group Wadaiko Sho. Brazilian groups have combined native and African drumming techniques with taiko performance. One such piece developed by Kinoshita is called \"Taiko de Samba\", which emphasizes both Brazilian and Japanese aesthetics in percussion traditions. Taiko was also popularized in Brazil from 2002 through the work of Yukihisa Oda, a Japanese native who visited Brazil several times through the Japan International Cooperation Agency.\nThe Brazilian Association of Taiko (ABT) suggests that there are about 150 taiko groups in Brazil and that about 10\u201315% of players are non-Japanese; Izumo Honda, coordinator of a large annual festival in S\u00e3o Paulo, estimated that about 60% of all taiko performers in Brazil are women.\nNorth America.\nTaiko emerged in the United States in the late 1960s. The first group, San Francisco Taiko Dojo, was formed in 1968 by Seiichi Tanaka, a postwar immigrant who studied taiko in Japan and brought the styles and teachings to the US. A year later, a few members of Senshin Buddhist Temple in Los Angeles led by its minister Masao Kodani initiated another group called Kinnara Taiko. San Jose Taiko later formed in 1973 in Japantown, San Jose, under Roy and PJ Hirabayashi. Taiko started to branch out to the eastern US in the late 1970s. This included formation of Denver Taiko in 1976, and Soh Daiko in New York City in 1979. Many of these early groups lacked the resources to equip each member with a drum and resorted to makeshift percussion materials such as rubber tires or creating taiko out of wine barrels.\nJapanese-Canadian taiko began in 1979 with Katari Taiko, and was inspired by the San Jose Taiko group. Its early membership was predominantly female. Katari Taiko and future groups were thought to represent an opportunity for younger, third-generation Japanese Canadians to explore their roots, redevelop a sense of ethnic community, and expand taiko into other musical traditions.\nThere are no official counts or estimates of the number of active taiko groups in the United States or Canada, as there is no governing body for taiko groups in either country. Unofficial estimates have been made. In 1989, there were as many as 30 groups in the US and Canada, seven of which were in California. One estimate suggested that around 120 groups were active in the US and Canada as of 2001, many of which could be traced to the San Francisco Taiko Dojo; later estimates in 2005 and 2006 suggested there were about 200 groups in the United States alone.\nThe Cirque du Soleil shows \"Myst\u00e8re\" in Las Vegas and \"Dralion\" have featured taiko performance. Taiko performance has also been featured in commercial productions such as the 2005 Mitsubishi Eclipse ad campaign, and in events such as the 2009 Academy Awards and 2011 Grammy Awards.\nFrom 2005 to 2006, the Japanese American National Museum held an exhibition called \"Big Drum: Taiko in the United States\". The exhibition covered several topics related to taiko in the United States, such as the formation of performance groups, their construction using available materials, and social movements. Visitors were able to play smaller drums.\nNorth America hosts the North American Taiko Conference (NATC) which has been ongoing since its inaugural conference in Los Angeles in 1997.\nIn 2013, the Taiko Community Alliance (TCA) formed as virtual nonprofit 501(c)3 organization with a mission to empower the people and advance the art of taiko. The Taiko Community Alliance has been responsible for helping organize the NATC conferences to help further its mission of educating and raising awareness of taiko through the taiko community.\nItaly.\nThe first group, called Quelli del Taiko, was formed in 2000 by Pietro Notarnicola. They played in World Premiere - 2017 - \"On Western Terror 8\" - Concerto for Taiko Ensemble and Orchestra of the Italian composed Luigi Morleo\nUnited Kingdom.\nKagemusha Taiko based in the south-west were formed in 1999 by Jonathan Kirby and perform original pieces of their own creation. They are known for their work in schools and have performed in several UK venues as well as the USA and Japan.\nRelated cultural and social movements.\nCertain peoples have used taiko to advance social or cultural movements, both within Japan and elsewhere in the world.\nGender conventions.\nTaiko performance has frequently been viewed as an art form dominated by men. Historians of taiko argue that its performance comes from masculine traditions. Those who developed ensemble-style taiko in Japan were men, and through the influence of Ondekoza, the ideal taiko player was epitomized in images of the masculine peasant class, particularly through the character Muh\u014dmatsu in the 1958 film \"Rickshaw Man\". Masculine roots have also been attributed to perceived capacity for \"spectacular bodily performance\" where women's bodies are sometimes judged as unable to meet the physical demands of playing.\nBefore the 1980s, it was uncommon for Japanese women to perform on traditional instruments, including taiko, as their participation had been systematically restricted; an exception was the San Francisco Taiko Dojo under the guidance of Grandmaster Seiichi Tanaka, who was the first to admit women to the art form. In Ondekoza and in the early performances of Kodo, women performed only dance routines either during or between taiko performances. Thereafter, female participation in \"kumi-daiko\" started to rise dramatically, and by the 1990s, women equaled and possibly exceeded representation by men. While the proportion of women in taiko has become substantial, some have expressed concern that women still do not perform in the same roles as their male counterparts and that taiko performance continues to be a male-dominated profession. For instance, a member of Kodo was informed by the director of the group's apprentice program that women were permitted to play, but could only play \"as women\". Other women in the apprentice program recognized a gender disparity in performance roles, such as what pieces they were allowed to perform, or in physical terms based on a male standard.\nFemale taiko performance has also served as a response to gendered stereotypes of Japanese women as being quiet, subservient, or a femme fatale. Through performance, some groups believe they are helping to redefine not only the role of women in taiko, but how women are perceived more generally.\nBurakumin.\nThose involved in the construction of taiko are usually considered part of the burakumin, a marginalized minority class in Japanese society, particularly those working with leather or animal skins. Prejudice against this class dates back to the Tokugawa period in terms of legal discrimination and treatment as social outcasts. Although official discrimination ended with the Tokugawa era, the burakumin have continued to face social discrimination, such as scrutiny by employers or in marriage arrangements. Drum makers have used their trade and success as a means to advocate for an end to discriminatory practices against their class.\nThe , representing the contributions of burakumin, is found in Naniwa Ward in Osaka, home to a large proportion of burakumin. Among other features, the road contains taiko-shaped benches representing their traditions in taiko manufacturing and leatherworking, and their influence on national culture. The road ends at the Osaka Human Rights Museum, which exhibits the history of systematic discrimination against the burakumin. The road and museum were developed in part due an advocacy campaign led by the Buraku Liberation League and a taiko group of younger performers called .\nNorth American \"sansei\".\nTaiko performance was an important part of cultural development by third-generation Japanese residents in North America, who are called \"sansei\". During World War II, second-generation Japanese residents, called \"nisei\" faced internment in the United States and in Canada on the basis of their race. During and after the war, Japanese residents were discouraged from activities such as speaking Japanese or forming ethnic communities. Subsequently, sansei could not engage in Japanese culture and instead were raised to assimilate into more normative activities. There were also prevailing stereotypes of Japanese people, which sansei sought to escape or subvert. During the 1960s in the United States, the civil rights movement influenced sansei to reexamine their heritage by engaging in Japanese culture in their communities; one such approach was through taiko performance. Groups such as San Jose Taiko were organized to fulfill a need for solidarity and to have a medium to express their experiences as Japanese-Americans. Later generations have adopted taiko in programs or workshops established by sansei; social scientist Hideyo Konagaya remarks that this attraction to taiko among other Japanese art forms may be due to its accessibility and energetic nature. Konagaya has also argued that the resurgence of taiko in the United States and Japan are differently motivated: in Japan, performance was meant to represent the need to recapture sacred traditions, while in the United States it was meant to be an explicit representation of masculinity and power in Japanese-American men.\nNotable performers and groups.\nA number of performers and groups, including several early leaders, have been recognized for their contributions to taiko performance. Daihachi Oguchi was best known for developing \"kumi-daiko\" performance. Oguchi founded the first \"kumi-daiko\" group called Osuwa Daiko in 1951, and facilitated the popularization of taiko performance groups in Japan.\nSeid\u014d Kobayashi is the leader of the Tokyo-based taiko group Oedo Sukeroku Taiko as of December 2014. Kobayashi founded the group in 1959 and was the first group to tour professionally. Kobayashi is considered a master performer of taiko. He is also known for asserting intellectual control of the group's performance style, which has influenced performance for many groups, particularly in North America.\nIn 1968, Seiichi Tanaka founded the San Francisco Taiko Dojo and is regarded as the Grandfather of Taiko and primary developer of taiko performance in the United States. He was a recipient of a 2001 National Heritage Fellowship awarded by the National Endowment for the Arts and since 2013 is the only taiko professional presented with the Order of the Rising Sun 5th Order: Gold and Silver Rays by Emperor Akihito of Japan, in recognition of Grandmaster Seiichi Tanaka's contributions to the fostering of US-Japan relations as well as the promotion of Japanese cultural understanding in the United States.\nIn 1969, founded Ondekoza, a group well known for making taiko performance internationally visible and for its artistic contributions to the tradition. Den was also known for developing a communal living and training facility for Ondekoza on Sado Island in Japan, which had a reputation for its intensity and broad education programs in folklore and music.\nPerformers and groups beyond the early practitioners have also been noted. Eitetsu Hayashi is best known for his solo performance work. When he was 19, Hayashi joined Ondekoza, a group later expanded and re-founded as Kodo, one of the best known and most influential taiko performance groups in the world. Hayashi soon left the group to begin a solo career and has performed in venues such as Carnegie Hall in 1984, the first featured taiko performer there. He was awarded the 47th Education Minister's Art Encouragement Prize, a national award, in 1997 as well as the 8th Award for the Promotion of Traditional Japanese Culture from the Japan Arts Foundation in 2001."}
{"id": "8716", "revid": "38587278", "url": "https://en.wikipedia.org/wiki?curid=8716", "title": "Dolly Parton", "text": "Dolly Rebecca Parton (born January 19, 1946) is an American singer, songwriter, actress, and philanthropist, known primarily for her decades-long career in country music. After achieving success as a songwriter for others, Parton made her album debut in 1967 with \"Hello, I'm Dolly\", which led to success during the remainder of the 1960s (both as a solo artist and with a series of duet albums with Porter Wagoner), before her sales and chart peak arrived during the 1970s and continued into the 1980s. Some of Parton's albums in the 1990s did not sell as well, but she achieved commercial success again in the new millennium and has released albums on various independent labels since 2000, including her own label, Dolly Records.\nWith a career spanning 60 years, Parton has been described as a \"country legend\" and has sold more than 100 million records worldwide, making her one of the best-selling music artists of all time. Parton's music includes Recording Industry Association of America (RIAA)-certified gold, platinum and multi-platinum awards. She has had 25 singles reach No.1 on the \"Billboard\" country music charts, a record for a female artist (tied with Reba McEntire). She has 44 career Top10 country albums, a record for any artist, and she has 110 career-charted singles over the past 40 years. She has composed over 3,000 songs, including \"I Will Always Love You\" (a two-time U.S. country chart-topper, and an international hit for Whitney Houston), \"Jolene\", \"Coat of Many Colors\", and \"9to5\". As an actress, she has starred in the films \"9to5\" in 1980 and \"The Best Little Whorehouse in Texas\" in 1982 (for each of which she earned Best Actress Golden Globe nominations) as well as \"Rhinestone\" in 1984, \"Steel Magnolias\" in 1989, \"Straight Talk\" in 1992, and \"Joyful Noise\" in 2012.\nParton has received various accolades, including 11 Grammy Awards from 50 nominations. She has won ten Country Music Association Awards, including Entertainer of the Year. She is one of seven female artists to win the Country Music Association's Entertainer of the Year Award. Parton has five Academy of Country Music Awards (including Entertainer of the Year), four People's Choice Awards, and three American Music Awards. She is also in a select group to have received at least one nomination from the Academy Awards, Grammy Awards, Tony Awards, and Emmy Awards. In 1999, Parton was inducted into the Country Music Hall of Fame. In 2005, she received the National Medal of Arts, and in 2022, she was nominated for and inducted into the Rock and Roll Hall of Fame, a nomination she had initially declined but ultimately accepted.\nOutside of her work in the music industry, she also co-owns The Dollywood Company, which manages a number of entertainment venues including the Dollywood theme park, the Splash Country water park, and a number of dinner theatre venues such as The Dolly Parton Stampede and Pirates Voyage. She has founded a number of charitable and philanthropic organizations, chief among them being the Dollywood Foundation, who manage a number of projects to bring education and poverty relief to East Tennessee, where she was raised.\nEarly life and career.\nDolly Rebecca Parton was born on January 19, 1946, in a one-room cabin on the banks of the Little Pigeon River in Pittman Center, Tennessee. \nShe is the fourth of 12 children born to Avie Lee Caroline (n\u00e9e Owens; 1923\u20132003) and Robert Lee Parton Sr. (1921\u20132000). Parton's middle name comes from her maternal great-great-grandmother Rebecca (n\u00e9e Dunn) Whitted. Parton's father, known as \"Lee\", worked in the mountains of East Tennessee, first as a sharecropper and later tending his own small tobacco farm and acreage. He also worked construction jobs to supplement the farm's small income. Despite her father's illiteracy, Parton has often commented that he was one of the smartest people she has ever known with regard to business and making a profit.\nParton's mother cared for their large family. Her 11 pregnancies (the tenth being twins) in 20 years made her a mother of 12 by age 35. Parton attributes her musical abilities to the influence of her mother; often in poor health, she still managed to keep house and entertain her children with Smoky Mountain folklore and ancient ballads. Having Welsh ancestors, Avie Lee knew many old ballads that immigrants from the British Isles brought to southern Appalachia in the 18th and 19th century. Avie Lee's father, Jake Owens, was a Pentecostal preacher, and Parton and her siblings all attended church regularly. Parton has long credited her father for her business savvy, and her mother's family for her musical abilities. When Parton was a young girl, her family moved from the Pittman Center area to a farm up on nearby Locust Ridge. Most of her cherished memories of youth happened there. Today, a replica of the Locust Ridge cabin resides at Parton's namesake theme park Dollywood. The farm acreage and surrounding woodland inspired her to write the song \"My Tennessee Mountain Home\" in the 1970s. Years after the farm was sold, Parton bought it back in the late 1980s. Her brother Bobby helped with building restoration and new construction.\nParton has described her family as being \"dirt poor\". Parton's father paid missionary Dr. Robert F. Thomas with a sack of cornmeal for delivering her. Parton would write a song about Dr. Thomas when she was grown. She also outlined her family's poverty in her early songs \"Coat of Many Colors\" and \"In the Good Old Days (When Times Were Bad)\". For six or seven years, Parton and her family lived in their rustic, one-bedroom cabin on their small subsistence farm on Locust Ridge. This was a predominantly Pentecostal area located north of the Greenbrier Valley of the Great Smoky Mountains.\nMusic played an important role in her early life. She was brought up in the Church of God (Cleveland, Tennessee), in a congregation her grandfather, Jake Robert Owens, pastored. Her earliest public performances were in the church, beginning at age six. At seven, she started playing a homemade guitar. When she was eight, her uncle bought her first real guitar. The Parton family was well-fed despite their poverty, and the 2024 cookbook \"Good Lookin' Cookin\"' (co-written by her with her sister Rachel) recalls numerous family meals.\nParton began performing as a child, singing on local radio and television programs in the East Tennessee area. By ten, she was appearing on \"The Cas Walker Show\" on both WIVK Radio and WBIR-TV in Knoxville, Tennessee. At 13, she was recording (the single \"Puppy Love\") on a small Louisiana label, Goldband Records, and appeared at the Grand Ole Opry, where she first met Johnny Cash, who encouraged her to follow her own instincts regarding her career.\nAfter graduating from Sevier County High School in 1964, Parton moved to Nashville the next day. Her initial success came as a songwriter, having signed with Combine Publishing shortly after her arrival; with her frequent songwriting partner, her uncle Bill Owens, she wrote several charting singles during this time, including two Top10 hits for Bill Phillips: \"Put It Off Until Tomorrow,\" and \"The Company You Keep\" (1966), and Skeeter Davis's number 11 hit \"Fuel to the Flame\" (1967). Her songs were recorded by many other artists during this period, including Kitty Wells and Hank Williams Jr. She signed with Monument Records in 1965, at age 19; she initially was pitched as a bubblegum pop singer. She released a string of singles, but the only one that charted, \"Happy, Happy Birthday Baby\", did not crack the \"Billboard\" Hot 100. Although she expressed a desire to record country material, Monument resisted, thinking her unique, high soprano voice was not suited to the genre.\nAfter her composition \"Put It Off Until Tomorrow\", as recorded by Bill Phillips (with Parton, uncredited, on harmony), went to number six on the country chart in 1966, the label relented and allowed her to record country. Her first country single, \"Dumb Blonde\" (composed by Curly Putman, one of the few songs during this era that she recorded but did not write), reached number 24 on the country chart in 1967, followed by \"Something Fishy\", which went to number 17. The two songs appeared on her first full-length album, \"Hello, I'm Dolly\".\nMusic career.\n1967\u20131975: Country music success.\nIn 1967, musician and country music entertainer Porter Wagoner invited Parton to join his organization, offering her a regular spot on his weekly syndicated television program \"The Porter Wagoner Show\", and in his road show. As documented in her 1994 autobiography, initially, much of Wagoner's audience was unhappy that Norma Jean, the performer whom Parton had replaced, had left the show, and was reluctant to accept Parton (sometimes chanting loudly for Norma Jean from the audience). With Wagoner's assistance, however, Parton was eventually accepted. Wagoner convinced his label, RCA Victor, to sign her. RCA decided to protect their investment by releasing her first single as a duet with Wagoner. That song, a remake of Tom Paxton's \"The Last Thing on My Mind\", released in late 1967, reached the country Top10 in January 1968, launching a six-year streak of virtually uninterrupted Top10 singles for the pair.\nParton's first solo single for RCA Victor, \"Just Because I'm a Woman\", was released in the summer of 1968 and was a moderate chart hit, reaching number 17. For the next two years, none of her solo effortseven \"In the Good Old Days (When Times Were Bad)\", which later became a standardwere as successful as her duets with Wagoner. The duo was named Vocal Group of the Year in 1968 by the Country Music Association, but Parton's solo records were continually ignored. Wagoner had a significant financial stake in her future; as of 1969, he was her co-producer and owned nearly half of Owe-Par, the publishing company Parton had founded with Bill Owens.\nBy 1970, both Parton and Wagoner had grown frustrated by her lack of solo chart success. Wagoner persuaded Parton to record Jimmie Rodgers' \"Mule Skinner Blues\", a gimmick that worked. The record shot to number three, followed closely, in February 1971, by her first number-one single, \"Joshua\". For the next two years, she had numerous solo hitsincluding her signature song \"Coat of Many Colors\" (number four, 1971)in addition to her duets. Top20 singles included \"The Right Combination\" and \"Burning the Midnight Oil\" (both duets with Wagoner, 1971); \"Lost Forever in Your Kiss\" (with Wagoner), \"Touch Your Woman\" (1972), \"My Tennessee Mountain Home\" and \"Travelin' Man\" (1973).\nAlthough her solo singles and the Wagoner duets were successful, her biggest hit of this period was \"Jolene\". Released in late 1973, the song topped the country chart in February 1974 and reached the lower regions of the Hot 100 (it eventually also charted in the U.K., reaching number seven in 1976, representing Parton's first U.K. success). Parton, who had always envisioned a solo career, made the decision to leave Wagoner's organization; the pair performed their last duet concert in April 1974, and she stopped appearing on his TV show in mid-1974, although they remained affiliated. He helped produce her records through 1975. The pair continued to release duet albums, their final release being 1975's \"Say Forever You'll Be Mine\".\nIn 1974, her song, \"I Will Always Love You\", written about her professional break from Wagoner, went to number one on the country chart. Around the same time, Elvis Presley indicated that he wanted to record the song. Parton was interested until Presley's manager, Colonel Tom Parker, told her that it was standard procedure for the songwriter to sign over half of the publishing rights to any song recorded by Presley. Parton refused. That decision has been credited with helping to make her many millions of dollars in royalties from the song over the years. Parton had three solo singles reach number one on the country chart in 1974 (\"Jolene\", \"I Will Always Love You\" and \"Love Is Like a Butterfly\"), as well as the duet with Porter Wagoner, \"Please Don't Stop Loving Me\". In a 2019 episode of the Sky Arts music series \"Brian Johnson: A Life on the Road\", Parton described finding old cassette tapes and realizing that she had composed both \"Jolene\" and \"I Will Always Love You\" in the same songwriting session, telling Johnson \"Buddy, that was a good night.\" Parton again topped the singles chart in 1975 with \"The Bargain Store\".\n1976\u20131986: Pop transition.\nBetween 1974 and 1980, Parton had a series of country hits, with eight singles reaching number one. Her influence on pop culture is reflected by the many performers covering her songs, including mainstream and crossover artists such as Olivia Newton-John, Emmylou Harris, and Linda Ronstadt.\nParton began to embark on a high-profile crossover campaign, attempting to aim her music in a more mainstream direction and increase her visibility outside of the confines of country music. In 1976, she began working closely with Sandy Gallin, who served as her personal manager for the next 25 years. With her 1976 album \"All I Can Do\", which she co-produced with Porter Wagoner, Parton began taking more of an active role in production, and began specifically aiming her music in a more mainstream, pop direction. Her first entirely self-produced effort, \"New Harvest...First Gathering\" (1977), highlighted her pop sensibilities, both in terms of choice of songs \u2013 the album contained covers of the pop and R&amp;B classics \"My Girl\" and \"Higher and Higher\" \u2013 and production. Though the album was well received and topped the U.S. country albums chart, neither it nor its single \"Light of a Clear Blue Morning\" made much of an impression on the pop charts.\nAfter \"New Harvest\" disappointing crossover performance, Parton turned to high-profile pop producer Gary Klein for her next album. The result, 1977's \"Here You Come Again\", became her first million-seller, topping the country album chart and reaching number 20 on the pop chart. The Barry Mann-Cynthia Weil-penned title track topped the country singles chart, and became Parton's first Top10 single on the pop chart (no.3). A second single, the double A-sided \"Two Doors Down\"/\"It's All Wrong, But It's All Right\" topped the country chart and crossed over to the pop Top20. For the remainder of the 1970s and into the early 1980s, many of her subsequent singles moved up on both charts simultaneously. Her albums during this period were developed specifically for pop-crossover success.\nIn 1978, Parton won a Grammy Award for Best Female Country Vocal Performance for her \"Here You Come Again\" album. She continued to have hits with \"Heartbreaker\" (1978), \"Baby I'm Burning\" (1979) and \"You're the Only One\" (1979)all of which charted in the pop Top\u00a040 and topped the country chart. \"Sweet Summer Lovin'\" (1979) became the first Parton single in two years to not top the country chart (though it did reach the Top10). During this period, her visibility continued to increase, with multiple television appearances. A highly publicized candid interview on a \"Barbara Walters Special\" in 1977 (timed to coincide with \"Here You Come Again\" release) was followed by appearances in 1978 on Cher's ABC television special, and her own joint special with Carol Burnett on CBS, \"Dolly &amp; Carol in Nashville\".\nParton served as one of three co-hosts (along with Roy Clark and Glen Campbell) on the CBS special \"Fifty Years of Country Music\". In 1979, Parton hosted the NBC special \"The Seventies: An Explosion of Country Music\", performed live at the Ford Theatre in Washington, D.C., and whose audience included President Jimmy Carter.\nHer commercial success grew in 1980, with three consecutive country chart number-one hits: the Donna Summer-written \"Starting Over Again\", \"Old Flames Can't Hold a Candle to You\", and \"9to5\", which topped the country and pop charts in early 1981. She had another Top10 single that year with \"Making Plans\", a single released from a 1980 album with Porter Wagoner, released as part of a lawsuit settlement between the pair.\nThe theme song to the 1980 feature film \"9to5\", in which she starred along with Jane Fonda and Lily Tomlin, not only reached number one on the country chartin February 1981 it reached number one on the pop and the adult-contemporary charts, giving her a triple number-one hit. Parton became one of the few female country singers to have a number-one single on the country and pop charts simultaneously. It also received a nomination for an Academy Award for Best Original Song. Her singles continued to appear consistently in the country Top10. Between 1981 and 1985, she had twelve Top10 hits; half of them hit number one. She continued to make inroads on the pop chart as well. A re-recorded version of \"I Will Always Love You\", from the feature film \"The Best Little Whorehouse in Texas\" (1982) scraped the Top50 that year and her duet with Kenny Rogers, \"Islands in the Stream\" (written by the Bee Gees and produced by Barry Gibb), spent two weeks at number one in 1983.\nIn the mid-1980s, her record sales were still relatively strong, with \"Save the Last Dance for Me\", \"Tennessee Homesick Blues\", \"God Won't Get You\" (1984), \"Real Love\" (another duet with Kenny Rogers), \"Don't Call It Love\" (1985) and \"Think About Love\" (1986) all reaching the country Top10 (\"Tennessee Homesick Blues\" and \"Think About Love\" reached number one; \"Real Love\" also reached number one on the country chart and became a modest crossover hit). However, RCA Records did not renew her contract after it expired in 1986, and she signed with Columbia Records in 1987.\n1987\u20132005: Country and bluegrass period.\nAlong with Emmylou Harris and Linda Ronstadt, she released \"Trio\" (1987) to critical acclaim. The album revitalized Parton's music career, spending five weeks at number one on \"Billboard's\" Country Albums chart, and also reached the Top10 on \"Billboard\" Top200 Albums chart. It sold several million copies and produced four Top10 country hits, including Phil Spector's \"To Know Him Is to Love Him\", which went to number one. \"Trio\" won the Grammy Award for Best Country Performance by a Duo or Group with Vocal and was nominated for a Grammy Award for Album of the Year. After a further attempt at pop success with \"Rainbow\" (1987), including the single \"The River Unbroken\", it ended up a commercial let-down, causing Parton to focus on recording country material. \"White Limozeen\" (1989) produced two number one hits in \"Why'd You Come in Here Lookin' Like That\" and \"Yellow Roses\". Although Parton's career appeared to be revived, it was actually just a brief revival before contemporary country music came in the early 1990s and moved most veteran artists off the charts.\nA duet with Ricky Van Shelton, \"Rockin' Years\" (1991) reached number one, though Parton's greatest commercial fortune of the decade came when Whitney Houston recorded \"I Will Always Love You\" for the soundtrack of the feature film \"The Bodyguard\" (1992). Both the single and the album were massively successful. Parton's soundtrack album from the 1992 film, \"Straight Talk\", however, was less successful. But her 1993 album \"Slow Dancing with the Moon\" won critical acclaim and did well on the charts, reaching number four on the country albums chart, and number 16 on the \"Billboard\" 200 album chart. It would also become Platinum certified. She recorded \"The Day I Fall in Love\" as a duet with James Ingram for the feature film \"Beethoven's 2nd\" (1993). The songwriters (Ingram, Carole Bayer Sager, and Clif Magness) were nominated for an Academy Award for Best Original Song, and Parton and Ingram performed the song at the awards telecast. Similar to her earlier collaborative album with Harris and Ronstadt, Parton released \"Honky Tonk Angels\" in the fall of 1993 with Loretta Lynn and Tammy Wynette. It was certified as a gold album by the Recording Industry Association of America and helped revive both Wynette and Lynn's careers. Also in 1994, Parton contributed the song \"You Gotta Be My Baby\" to the AIDS benefit album \"Red Hot + Country\" produced by the Red Hot Organization. A live acoustic album, \"\", featuring stripped-down versions of some of her hits, as well as some traditional songs, was released in late 1994.\nParton's recorded music during the mid-to-late-1990s remained steady and somewhat eclectic. Her 1995 re-recording of \"I Will Always Love You\" (performed as a duet with Vince Gill), from her album \"Something Special\" won the Country Music Association's Vocal Event of the Year Award. The following year, \"Treasures\", an album of covers of 1960s/70s hits was released, and featured a diverse collection of material, including songs by Mac Davis, Pete Seeger, Kris Kristofferson, Cat Stevens, and Neil Young. Her recording of Stevens' \"Peace Train\" was later re-mixed and released as a dance single, reaching \"Billboard's \"dance singles chart. Her 1998 country-rock album \"Hungry Again\" was made up entirely of her own compositions. Although neither of the album's two singles, \"(Why Don't More Women Sing) Honky Tonk Songs\" and \"Salt in my Tears\", charted, videos for both songs received significant airplay on CMT. A second and more contemporary collaboration with Harris and Ronstadt, \"Trio II\", was released in early 1999. Its cover of Neil Young's song \"After the Gold Rush\" won a Grammy Award for Best Country Collaboration with Vocals. Parton also was inducted into the Country Music Hall of Fame in 1999.\nParton recorded a series of bluegrass-inspired albums, beginning with \"The Grass Is Blue\" (1999), winning a Grammy Award for Best Bluegrass Album; and \"Little Sparrow\" (2001), with its cover of Collective Soul's \"Shine\" winning a Grammy Award for Best Female Country Vocal Performance. The third, \"Halos &amp; Horns\" (2002) included a bluegrass version of the Led Zeppelin song \"Stairway to Heaven\". In 2005, she released \"Those Were The Days\" consisting of her interpretations of hits from the folk-rock era of the late 1960s and early 1970s, including \"Imagine\", \"Where Do the Children Play?\", \"Crimson and Clover\", and \"Where Have All the Flowers Gone?\"\n2005\u20132020: Touring and holiday album.\nParton earned her second Academy Award nomination for Best Original Song for \"Travelin' Thru\", which she wrote specifically for the feature film \"Transamerica\". (2005) Due to the song's (and film's) acceptance of a transgender woman, Parton received death threats. She returned to number one on the country chart later in 2005 by lending her distinctive harmonies to the Brad Paisley ballad, \"When I Get Where I'm Going\". In September 2007, Parton released her first single from her own record company, Dolly Records, titled, \"Better Get to Livin'\", which eventually peaked at number 48 on \"Billboard\" Hot Country Songs chart. It was followed by the studio album \"Backwoods Barbie\", which was released on February 26, 2008, and reached number two on the country chart. The album's debut at number 17 on the all-genre \"Billboard\" 200 albums chart was the highest in her career. \"Backwoods Barbie\" produced four additional singles, including the title track, written as part of her score for \"9to5: The Musical\", an adaptation of her feature film. After the death of Michael Jackson, whom Parton knew personally, she released a video in which she somberly told of her feelings on Jackson and his death.\nOn October 27, 2009, Parton released a four-CD box set, \"Dolly\", which featured 99 songs and spanned most of her career. She released her second live DVD and album, \"Live From London\" in October 2009, which was filmed during her sold-out 2008 concerts at London's The O2 Arena. On August 10, 2010, with longtime friend Billy Ray Cyrus, Parton released the album \"Brother Clyde\". Parton is featured on \"The Right Time\", which she co-wrote with Cyrus and Morris Joseph Tancredi. On January 6, 2011, Parton announced that her new album would be titled \"Better Day\". In February 2011, she announced that she would embark on the Better Day World Tour on July 17, 2011, with shows in northern Europe and the U.S. The album's lead-off single, \"Together You and I\", was released on May 23, 2011, and \"Better Day\" was released on June 28, 2011. In 2011, Parton voiced the character Dolly Gnome in the animated film \"Gnomeo &amp; Juliet\". On February 11, 2012, after the sudden death of Whitney Houston, Parton stated, \"Mine is only one of the millions of hearts broken over the death of Whitney Houston. I will always be grateful and in awe of the wonderful performance she did on my song, and I can truly say from the bottom of my heart, 'Whitney, I will always love you. You will be missed.\nIn 2013, Parton joined Lulu Roman for a re-recording of \"I Will Always Love You\" for Roman's album, \"At Last\". In 2013, Parton and Kenny Rogers reunited for the title song of his album \"You Can't Make Old Friends\". For their performance, they were nominated at the 2014 Grammy Awards for Grammy Award for Best Country Duo/Group Performance. In 2014, Parton embarked on the Blue Smoke World Tour in support of her 42nd studio album, \"Blue Smoke\". The album was first released in Australia and New Zealand on January 31 to coincide with tour dates there in February, and reached the Top10 in both countries. It was released in the United States on May 13, and debuted at number six on the \"Billboard\" 200 chart, making it her first Top10 album and her highest-charting solo album ever; it also reached the number two on the U.S. country chart. The album was released in Europe on June 9, and reached number two on the UK album chart. On June 29, 2014, Parton performed for the first time at the UK Glastonbury Festival, singing songs such as \"Jolene\", \"9to5\" and \"Coat of Many Colors\" to a crowd of more than 180,000. On March 6, 2016, Parton announced that she would be embarking on a tour in support of her new album, \"Pure &amp; Simple\". The tour was one of Parton's biggest tours within the United States in more than 25 years. 64 dates were planned in the United States and Canada, visiting the most requested markets missed on previous tours.\nIn the fall of 2016 she released \"Jolene\" as a single with the \"a cappella\" group Pentatonix and performed on \"\" with Pentatonix and Miley Cyrus in November 2016. Also in 2016, Parton was one of 30 artists to perform on \"Forever Country\", a mash-up of the songs, \"Take Me Home, Country Roads\", \"On the Road Again\" and her own \"I Will Always Love You\". The song celebrates fifty years of the CMA Awards. At the ceremony itself, Parton was honored with the Willie Nelson Lifetime Achievement Award, which was presented by Lily Tomlin and preceded by a tribute featuring Jennifer Nettles, Pentatonix, Reba McEntire, Kacey Musgraves, Carrie Underwood and Martina McBride. In 2017, Parton appeared on \"Rainbow\", the third studio album by Kesha performing a duet of \"Old Flames Can't Hold a Candle to You\". The track had been co-written by Kesha's mother Pebe Sebert. It was previously a hit for Parton and was included on her 1980 album \"Dolly, Dolly, Dolly\". She also co-wrote and provided featuring vocals on the song \"Rainbowland\" on \"Younger Now\", the sixth album by her goddaughter Miley Cyrus.\nIn July 2019, Parton made an unannounced appearance at the Newport Folk Festival in Rhode Island, and performed several songs accompanied by the Highwomen and Linda Perry. In 2020, Parton received worldwide attention after posting four pictures, in which she showed how she would present herself on social media platforms LinkedIn, Facebook, Instagram and Twitter. The original post on Instagram went viral after celebrities posted their own versions of the so-called Dolly Parton challenge on social media. On April 10, 2020, Parton re-released 93 songs from six of her classic albums: \"Little Sparrow\", \"Halos &amp; Horns\", \"For God and Country\", \"Better Day\", \"Those Were The Days\", and \"Live and Well\". On May 27, 2020, Parton released a brand new song called \"When Life Is Good Again\". This song was released to help keep the spirits up of those affected by the 2020 COVID-19 pandemic. She also released a music video for \"When Life Is Good Again\", which premiered on \"Time\" 100 talks on May 28, 2020.\nIn 2019, Parton collaborated with Christian alternative rock duet For King and Country and released a version of their hit \"God Only Knows\". She followed this by recording a duet with Christian music artist Zach Williams in the song \"There Was Jesus\". In October 2020, Parton was featured on the single \"Pink\" alongside Monica, Jordin Sparks, Sara Evans and Rita Wilson. The single was released in aid of Breast Cancer Research. Parton released \"A Holly Dolly Christmas\" in October 2020. On December 6, CBS aired a Christmas special, \"A Holly Dolly Christmas\", where Parton performed songs from her album.\nSince 2022: Rock album.\nIn early 2022, Parton was nominated for induction into the Rock and Roll Hall of Fame. Parton initially declined the nomination believing that the Rock and Roll Hall of Fame was \"for the people in rock music\", but after learning that this was not the case Parton said she would accept her induction if she were chosen for the honor. In May her induction was announced, and finally on November 5, 2022, she was inducted into the Rock and Roll Hall of Fame. In October 2022 Parton stated in an interview that she would no longer tour, but would continue to play live shows occasionally. On December 31, 2022, Parton co-hosted NBC's New Year's special \"Miley's New Year's Eve Party.\"\nOn January 17, 2023, Parton announced she would release her first rock album, titled \"Rockstar\", later that year, during an interview on \"The View\". Lead single \"World on Fire\" was released on May 11, 2023. It went on to peak at number\u00a01 a week later. The album was released on November 17, 2023, and features collaborations with Paul McCartney, Ringo Starr, Sting, Elton John, Sheryl Crow, Miley Cyrus, and Lizzo, amongst others. The album received generally positive reviews from critics and debuted at number three on the \"Billboard\" 200, becoming Parton's highest-charting solo studio album as well as topping the Country and Rock Albums charts.\nThe soundtrack single \"Gonna Be You\" from the movie \"80 for Brady\" was released January 20, 2023. The song was written by Diane Warren, and performed by Dolly Parton, Belinda Carlisle, Cyndi Lauper, Debbie Harry and Gloria Estefan. The official music video shows Parton, Carlisle, Lauper, and Estefan performing while wearing football jerseys similar to the ones worn by the women in the film, interspersed with clips from the film.\nPublic image.\nParton had turned down several offers to pose nude for \"Playboy\" magazine, but did appear on the cover of the October 1978 issue wearing a Playboy bunny outfit, complete with ears (the issue featured Lawrence Grobel's extensive and candid interview with Parton, representing one of her earliest high-profile interviews with the mainstream press). The association of breasts with Parton's public image is illustrated in the naming of Dolly the sheep after her, since the sheep was cloned from a cell taken from an adult ewe's mammary gland. In Mobile, Alabama, the General W.K. Wilson Jr. Bridge is commonly called \"the Dolly Parton Bridge\" due to its arches resembling her bust. The thickened appearance of the turret frontal armor of the T-72A main battle tank led to the unofficial Army nickname \"Dolly Parton\" - and later the T-72BIs got the \"Super Dolly Parton\" nickname.\nParton is known for having undergone considerable plastic surgery. On a 2003 episode of \"The Oprah Winfrey Show\", Winfrey asked what kind of cosmetic surgery Parton had undergone. Parton replied that cosmetic surgery was imperative in keeping with her famous image.\nParton has repeatedly joked about her physical image and surgeries, saying, \"It takes a lot of money to look this cheap.\" Her breasts have garnered her mentions in several songs, including \"Dolly Parton's Hits\" by Bobby Braddock, \"Marty Feldman Eyes\" by Bruce Baum (a parody of \"Bette Davis Eyes\"), \"No Show Jones\" by George Jones and Merle Haggard, and \"Make Me Proud\" by Drake, featuring Nicki Minaj. When asked about future plastic surgeries, she famously said, \"If I see something sagging, bagging or dragging, I'll get it nipped, tucked or sucked.\" Parton's feminine escapism is acknowledged in her words, \"Womanhood was a difficult thing to get a grip on in those hills, unless you were a man.\" Parton said in 2012 that she had entered a Dolly Parton drag queen lookalike contest and lost.\nArtistry.\nInfluences.\nParton, though influenced by big name stars, often credits much of her inspiration to her family and community. On her own mother Parton, in her 2020 book \"Songteller: My Life in Lyrics\", wrote \"So it was just natural for my mom to always be singing. My mother had that old-timey voice, and she used to sing all these songs that were brought over from the Old World. They were English, Irish, Welsh, folk songs where people tell stories.\" Parton calls her mother's voice \"haunting\". \"Lord you would feel it\", she wrote. Her biggest influence however was her Aunt Dorothy Jo: \"People often ask me who my influences were, they think I'm going to say some big names, and there were a few 'stars' I was impressed with. But my hero was my aunt Dorothy Jo. Mama's baby sister. She was not only an evangelist, she played banjo, she played guitar, and she wrote some great songs.\" Fellow singers also had an impact on Parton, describing George Jones as her \"all time favorite singer\", and recognizing her love for other artists such as Kitty Wells, Roy Acuff, and Rose Maddox.\nMusicianship.\nThough unable to read sheet music, Parton can play many instruments, including: the dulcimer, autoharp, banjo, guitar, electric guitar, fiddle, piano, recorder, and the saxophone. Reflecting on her multi-instrumental abilities, Parton said, \"I play some of everything. I ain't that good at none of it, but I try to sell it. I really try to lay into it.\" Parton has also used her fingernails as an instrument, most evident on her 1980 song \"9 to 5\", which she derived the beat from clacking her nails together while backstage on the set of the film \"9 to 5\".\nOther ventures.\nIn 1998, \"Nashville Business\" ranked her the wealthiest country music star. , her net worth is estimated at $500million.\nSongwriting.\nParton is a prolific songwriter, having begun by writing country music songs with strong elements of folk music, based on her upbringing in humble mountain surroundings and reflecting her family's Christian background. Her songs \"Coat of Many Colors\", \"I Will Always Love You\", and \"Jolene\", among others, have become classics. On November 4, 2003, Parton was honored as a BMI Icon at the 2003 BMI Country Awards.\nParton has earned over 35 BMI Pop and Country Awards. In 2001, she was inducted into the Songwriters Hall of Fame. In a 2009 interview on CNN's \"Larry King Live\", she said she had written \"at least 3,000\" songs, having written seriously since the age of seven. Parton also said she writes something every day, be it a song or an idea.\nParton's songwriting has been featured prominently in several films. In addition to the title song for \"9to5\", she also recorded a second version of \"I Will Always Love You\" for \"The Best Little Whorehouse in Texas\" (1982). The second version was a number one country hit and also reached number 53 on the pop charts.\n\"I Will Always Love You\" has been covered by many country artists, including Ronstadt on \"Prisoner In Disguise\" (1975), Kenny Rogers on \"Vote for Love\" (1996), and LeAnn Rimes on \"\" (1997). Whitney Houston performed it on \"The Bodyguard\" soundtrack and her version became the best-selling hit both written and performed by a female vocalist, with worldwide sales of over twelve million copies. In addition, the song has been translated into Italian and performed by the Welsh opera singer Katherine Jenkins.\nAs a songwriter, Parton has twice been nominated for an Academy Award for Best Original Song, for \"9 to 5\" and \"Travelin' Thru\" (2005) from the film \"Transamerica\". \"Travelin' Thru\" won Best Original Song at the 2005 Phoenix Film Critics Society Awards. It was also nominated for both the 2005 Golden Globe Award for Best Original Song and the 2005 Broadcast Film Critics Association Award (also known as the Critics' Choice Awards) for Best Song. A cover of \"Love Is Like A Butterfly\" by Clare Torry was used as the theme music for the British TV show \"Butterflies\".\nStage musicals.\n\"9 to 5: The Musical\".\nParton wrote the score (and Patricia Resnick the book) for \"\", a musical-theater adaptation of Parton's feature film \"9 to 5\" (1980). The musical ran at the Ahmanson Theatre, Los Angeles, in late 2008. It opened on Broadway at the Marquis Theatre in New York on April 30, 2009, to mixed reviews.\nThe title track of her 2008 album \"Backwoods Barbie\" was written for the musical's character Doralee. Although her score (as well as the musical debut of actress Allison Janney) was praised, the show struggled, closing on September 6, 2009, after 24 previews and 148 performances. Parton received nominations for Drama Desk Award for Outstanding Music and Drama Desk Award for Outstanding Lyrics as well as a nomination for Tony Award for Best Original Score.\nDeveloping the musical was not a quick process. According to the public-radio program \"Studio 360\" (October 29, 2005), in October 2005 Parton was in the midst of composing songs for a Broadway musical theater adaptation of the film. In late June 2007, \"9 to 5: The Musical\" was read for industry presentations. The readings starred Megan Hilty, Allison Janney, Stephanie J. Block, Bebe Neuwirth, and Marc Kudisch. Ambassador Theatre Group announced a 2012 UK tour for \"Dolly Parton's 9to5: The Musical\", commencing at Manchester Opera House, on October 12, 2012.\n\"Dolly: An Original Musical\".\nIn June 2024, Parton announced an autobiographical musical about her life and career initially titled \"Hello, I'm Dolly\" (named after her debut album and also a play on \"Hello, Dolly!\"), with a goal of opening on Broadway in 2026 with direction by Bartlett Sher. The musical, with a co-written book by Parton and Maria S. Schlatter which would feature original songs as well as her more well-known hits. Parton also revealed that she has been working on the musical for the last decade. On December 6 of the same year, a new title, \"Dolly: An Original Musical\", was revealed. On the same day, Parton launched a nationwide casting call for an actress to play her through different stages of her life by posting videos on social media with the hashtag #SearchForDolly.\nThe Dollywood Company.\nParton invested much of her earnings into business ventures in her native East Tennessee, notably Pigeon Forge. She is a co-owner of The Dollywood Company, which operates the theme park Dollywood (a former Silver Dollar City), a dinner theater, Dolly Parton's Stampede, the waterpark Dollywood's Splash Country, and the Dream More Resort and Spa, all in Pigeon Forge. Dollywood is the 24th-most-popular theme park in the United States, with three million visitors per year.\nThe Dolly Parton's Stampede business has venues in Branson, Missouri, and Myrtle Beach, South Carolina. A former location in Orlando, Florida, closed in January 2008 after the land and building were sold to a developer. Starting in June 2011, the Myrtle Beach location became Pirates Voyage Fun, Feast and Adventure; Parton appeared for the opening, and the South Carolina General Assembly declared June 3, 2011, as Dolly Parton Day.\nOn January 19, 2012, Parton's 66th birthday, Gaylord Opryland and Dollywood announced plans to open a $50million water and snow park, a family-friendly destination in Nashville that is open all year. On September 29, 2012, Parton officially withdrew her support for the Nashville park due to the restructuring of Gaylord Entertainment Company after its merger with Marriott International.\nOn June 12, 2015, it was announced that the Dollywood Company had purchased the Lumberjack Feud Dinner Show in Pigeon Forge. The show, which opened in June 2011, was owned and operated by Rob Scheer until the close of the 2015 season. The new, renovated show by the Dollywood Company opened in 2016.\nProduction work.\nParton was a co-owner of Sandollar Productions, with Sandy Gallin, her former manager. A film and television production company, it produced the documentary \"\" (1989), which won an Academy Award for Best Documentary Feature; the television series \"Babes\" (1990\u201391) and \"Buffy the Vampire Slayer\" (1997\u20132003); and the feature films \"Father of the Bride\" (1991), \"Father of the Bride: Part II\" (1995) \"Straight Talk\" (1992) (in which Parton starred), and \"Sabrina\" (1995), among other shows. In a 2009 interview, singer Connie Francis revealed that Parton had been contacting her for years in an attempt to film the singer's life story. Francis turned down Parton's offers, as she was already in negotiations with singer Gloria Estefan to produce the film, a collaboration now ended. After the retirement of her partner, Sandy Gallin, Parton briefly operated Dolly Parton's Southern Light Productions and in 2015 she announced her new production company would be called Dixie Pixie Productions and produce the movies-of-week in development with NBC Television and Magnolia Hill Productions.\nActing career.\nActing breakthrough.\nIn addition to her performing appearances on \"The Porter Wagoner Show\" in the 1960s and into the 1970s, her two self-titled television variety shows in the 1970s and 1980s, and on \"American Idol\" in 2008 and other guest appearances, Parton has had television roles. In 1979, she received an Emmy award nomination as \"Outstanding Supporting Actress in a Variety Program\" for her guest appearance in a Cher special. During the mid-1970s, Parton wanted to expand her audience base. Although her first attempt, the television variety show \"Dolly!\" (1976\u201377), had high ratings, it lasted only one season, with Parton requesting to be released from her contract because of the stress it was causing on her vocal cords. (She later tried a second television variety show, also titled \"Dolly\" (1987\u201388); it too lasted only one season).\nIn her first feature film, Parton portrayed a secretary in a leading role with Jane Fonda and Lily Tomlin in the comedy film \"9to5\" (1980). The movie highlights discrimination against women in the workplace and created awareness of the National Association of Working Women (9\u20135). She received nominations for a Golden Globe Award for Best Actress\u00a0\u2013 Musical or Comedy and a Golden Globe Award for New Star of the Year\u00a0\u2013 Actress. Parton wrote and recorded the film's title song. It received nominations for an Academy Award for Best Song and a Golden Globe Award for Best Original Song. Released as a single, the song won both the Grammy Award for Best Female Country Vocal Performance and the Grammy Award for Best Country Song. It also reached no.1 on the Hot 100 chart and it was no.78 on the \"AFI's 100 Years...100 Songs\" list released by the American Film Institute in 2004. \"9 to 5\" became a major box office success, grossing over $3.9million its opening weekend, and over $103million worldwide. Parton was named Top Female Box Office Star by the \"Motion Picture Herald\" in both 1981 and 1982 due to the film's success.\nIn late 1981, Parton began filming her second film, the musical film \"The Best Little Whorehouse in Texas\" (1982). The film earned her a second nomination for a Golden Globe Award for Best ActressMotion Picture Musical or Comedy. The film was greeted with positive critical reviews and became a commercial success, earning over $69million worldwide. After a two-year hiatus from films, Parton was teamed with Sylvester Stallone for \"Rhinestone\" (1984). A comedy film about a country music star's efforts to mould an unknown into a music sensation, the film was a critical and financial failure, making just over $21million on a $28million budget.\nContinued roles.\nIn 1989, Parton returned to film acting in \"Steel Magnolias\" (1989), based on the play \"Steel Magnolias\" by Robert Harling. The film was popular with critics and audiences, grossing over $95million in the U.S. Parton starred in the television movies \"A Smoky Mountain Christmas\" (1986), \"Wild Texas Wind\" (1991), \"Unlikely Angel\" (1996), portraying an angel sent back to earth after a deadly car crash, and \"Blue Valley Songbird\" (1999), where her character lives through her music. She starred with James Woods in \"Straight Talk\" (1992), which received mixed reviews, and grossed a mild $21million at the box office.\nParton's 1987 variety show \"Dolly\" lasted only one season. She made a cameo appearance as herself in \"The Beverly Hillbillies\" (1993), an adaptation of the long-running TV sitcom \"The Beverly Hillbillies\" (1962\u20131971). Parton has done voice work for animation for television series, playing herself in \"Alvin and the Chipmunks\" (episode \"Urban Chipmunk\", 1983) and the character Katrina Eloise \"Murph\" Murphy (Ms. Frizzle's first cousin) in \"The Magic School Bus\" (episode \"The Family Holiday Special\", 1994). She also has guest-starred in several sitcoms, including a 1990 episode of \"Designing Women\" (episode \"The First Day of the Last Decade of the Entire Twentieth Century\") as herself, the guardian movie star of Charlene's baby. She made a guest appearance on \"Reba\" (episode \"Reba's Rules of Real Estate\") portraying a real-estate agency owner and on \"The Simpsons\" (episode \"Sunday, Cruddy Sunday\", 1999). She appeared as herself in 2000 on the Halloween episode of Bette Midler's short-lived sitcom \"Bette\", and on episode 14 of \"Babes\" (produced by Sandollar Productions, Parton and Sandy Gallin's joint production company). She made cameo appearances on the Disney Channel as \"Aunt Dolly\", visiting Hannah and her family in fellow Tennessean and real-life goddaughter Miley Cyrus's series \"Hannah Montana\" (episodes \"Good Golly, Miss Dolly\", 2006, \"I Will Always Loathe You\", 2007, and \"Kiss It All Goodbye\", 2010). She was nominated for Outstanding Guest Actress in a Comedy Series.\nParton appeared as an overprotective mother in the comedy \"Frank McKlusky, C.I.\". (2002) She made a cameo appearance in the comedy film ', starring Sandra Bullock. She was featured in \"The Book Lady\" (2008), a documentary about her campaign for children's literacy. Parton expected to reprise her television role as Hannah's godmother in the musical comedy film ' (2009), but the character was omitted from the screenplay.\nSince 2010.\nParton had a voice role in the comedy family film \"Gnomeo &amp; Juliet\" (2011), an animated film with garden gnomes about William Shakespeare's \"Romeo and Juliet\". She co-starred with Queen Latifah in the musical film \"Joyful Noise\" (2012), playing a choir director's widow who joins forces with Latifah's character, a mother of two teens, to save a small Georgia town's gospel choir. \"Dolly Parton's Coat of Many Colors\", a made-for-TV film based on Parton's song \"Coat of Many Colors\", and featuring narration by Parton, aired on NBC in December 2015, with child actress Alyvia Alyn Lind portraying the young Parton. Parton also had a cameo in , which aired in November 2016.\nIn June 2018, Parton announced an eight-part Netflix series, featuring her music career. She is its executive producer and co-star. The series, called \"Dolly Parton's Heartstrings\", aired in November 2019. Parton is the subject of the NPR podcast \"Dolly Parton's America\". It is hosted by Jad Abumrad, who also hosts Radiolab. In December 2019, the biographical documentary \"\" was added to the catalog of the Netflix streaming service. The documentary, a co-production of Netflix and the BBC, takes its name from Parton's 1971 song.\nIn November 2020, Parton produced and starred in the Netflix musical film \"Dolly Parton's Christmas on the Square\", which won her a Primetime Emmy Award for Outstanding Television Movie. In November 2021, Parton was confirmed to be appearing in the final season of \"Grace and Frankie\" in a guest-starring role, reuniting with her \"9 to 5\" co-stars Lily Tomlin and Jane Fonda. In July 2022, Parton appeared as a simulation of herself on sci-fi show \"The Orville\" in the episode \"Midnight Blue\". In December 2022, Parton appeared in an NBC special titled \"Dolly Parton's Mountain Magic Christmas\". On Thanksgiving 2023, Parton performed songs during halftime at the Washington Commanders and Dallas Cowboys NFL football game.\nPersonal life.\nFamily.\nParton is the fourth of 12 children. Her siblings are Willadeene, David Wilburn (1942\u20132024), Coy Denver, Robert Lee (Bobby), Stella Mae, Cassie Nan, Randle Huston (Randy; 1953\u20132021), Larry Gerald (born and died July 6, 1955), twins Floyd Estel (1957\u20132018) and Frieda Estelle, and Rachel Ann.\nOn May 30, 1966, Parton and Carl Thomas Dean (born July 20, 1942, in Nashville, Tennessee) were married in Ringgold, Georgia. Although Parton does not use Dean's surname professionally, she has stated that her passport reads \"Dolly Parton Dean\", and she sometimes uses Dean when signing contracts.\nDean, who is retired from running an asphalt road-paving business in Nashville, has always shunned publicity, and rarely accompanies his wife to any events. Parton has jokingly said that he has only seen her perform once. She also has said in interviews that even though it appears they spend little time together, it is simply that nobody sees him publicly. She has commented on Dean's romantic side, saying that he does spontaneous things to surprise her, and sometimes even writes poems for her. In 2011, Parton said, \"We're really very proud of our marriage. It's the first for both of us. And the last.\"\nOn May 6, 2016, Parton announced that she and Dean would renew their vows in honor of their 50th wedding anniversary later in the month.\nWhile Parton has never had children, she and Dean helped raise several of her younger siblings in Nashville, leading her nieces and nephews to refer to them as \"Uncle Peepaw\" and \"Aunt Granny\"; the latter a moniker that later lent its name to one of Parton's Dollywood restaurants. \nParton is also the godmother of singer-songwriter and actress Miley Cyrus.\nFaith.\nParton says that she is a committed Christian, which has influenced many of her musical releases.\nShe talked about her liberal approach to faith in the January 2024 issue of \"New Humanist\" magazine. \"I wouldn't even say I'm religious, though I grew up with that background. But I have a lot of faith in myself and I've been so blessed to have been around great people my whole life, my Uncle Bill and my family being supportive, and all the people I met along the way.\"\nPhilanthropy.\nSince the mid-1980s, Parton has supported many charitable efforts, particularly in the area of literacy, primarily through her Dollywood Foundation.\nHer literacy program, Dolly Parton's Imagination Library, which is a part of the Dollywood Foundation, was founded in honor of her father, who never learned to read or write. It mails one book per month to each enrolled child from the time of their birth until they enter kindergarten. Currently, over 1600 local communities provide the Imagination Library to almost 850,000 children each month across the U.S., Canada, the UK, Australia, and the Republic of Ireland. In February 2018, she donated her 100 millionth free book, a copy of Parton's children's picture book \"Coat of Many Colors\", to the Library of Congress in Washington, D.C. and was honored by the Library of Congress on account of the \"charity sending out its 100 millionth book\".\nFor her work in literacy, Parton has received various awards, including Association of American Publishers Honors Award (2000), Good Housekeeping Seal of Approval (2001) (the first time the seal had been awarded to a person), American Association of School AdministratorsGalaxy Award (2002), National State Teachers of the YearChasing Rainbows Award (2002), and Parents as Teachers National CenterChild and Family Advocacy Award (2003).\nOn May 8, 2009, Parton gave the commencement speech at the graduation ceremony for the University of Tennessee, Knoxville's College of Arts and Sciences. During the ceremony, she received an honorary Doctor of Humane Letters from the university. It was only the second honorary degree given by the university, and in presenting the degree, the university's Chancellor, Jimmy Cheek, said, \"Because of her career not just as a musician and entertainer, but for her role as a cultural ambassador, philanthropist and lifelong advocate for education, it is fitting that she be honored with an honorary degree from the flagship educational institution of her home state.\"\nIn 2006, Parton published a cookbook, \"Dolly's Dixie Fixin's: Love, Laughter and Lots of Good Food\".\nThe Dollywood Foundation, funded from Parton's profits, has been noted for bringing jobs and tax revenues to a previously depressed region. Parton also has worked to raise money for several other causes, including the American Red Cross and HIV/AIDS-related charities.\nIn December 2006, Parton pledged $500,000 toward a proposed $90million hospital and cancer center to be constructed in Sevierville in the name of Robert F. Thomas, the physician who delivered her. She announced a benefit concert to raise additional funds for the project. The concert played to about 8,000 people. That same year, Parton and Emmylou Harris allowed use of their music in a PETA ad campaign that encouraged pet owners to keep their dogs indoors rather than chained outside.\nIn 2003, her efforts to preserve the bald eagle through the American Eagle Foundation's sanctuary at Dollywood earned her the Partnership Award from the U.S. Fish and Wildlife Service. Parton received the Woodrow Wilson Award for Public Service from the Woodrow Wilson International Center for Scholars of the Smithsonian Institution at a ceremony in Nashville on November 8, 2007.\nIn response to the 2016 Great Smoky Mountains wildfires, Parton was one of a number of country music artists who participated in a telethon to raise money for victims of the fires. This was held in Nashville on December 9. In addition, Parton hosted her own telethon for the victims on December 13 and reportedly raised around $9million. Her fund, the \"My People Fund\", provided $1,000 a month for six months to over 900 families affected by the wildfires, finally culminating with $5,000 to each home in the final month due to increased fundraising, for a total of $10,000 per family. In 2018, the FBI honored Parton for her wildfire aid work, awarding her the 2018 Director's Community Leadership Award at a ceremony at FBI Headquarters in Washington. The honor was bestowed by Director Christopher Wray and was accepted on Parton's behalf by David Dotson, the CEO of the Dollywood Foundation.\nThe impact of the fund's financial relief for the 2016 wildfire victims was studied by University of Tennessee College of Social Work professor Stacia West, who examined the impact of cash transfers in poverty alleviation. West surveyed 100 recipients of the emergency relief funds in April 2017 on topics including questions on housing, financial impact, physical and emotional health, and sources of support, with a follow-up survey conducted in December 2017. West found that the \"My People Fund\", in tandem with traditional disaster response, gave families the ability to make decisions that were most beneficial to them, and concluded that unconditional cash support may be more beneficial for disaster relief than conditional financial support. The report cited the impact of the monthly financial disbursements from the \"My People Fund\" on residents' emergency savings: \"Following the monthly disbursements of unconditional cash assistance, participants were able to return to baseline financial stability reported prior to the wildfire, and improve their ability to set aside savings for hypothetical future emergencies.\"\nParton has been a generous donor to Vanderbilt University Medical Center (VUMC). Among her gifts was a contribution to the Monroe Carell Jr. Children's Hospital at Vanderbilt Pediatric Cancer Program in honor of a friend, Naji Abumrad, and her niece, Hannah Dennison, who was successfully treated for leukemia as a child at the Children's Hospital.\nIn the aftermath of 2024's Hurricane Helene, Parton announced a donation of $2\u00a0million to relief efforts, $1\u00a0million personally and another $1\u00a0million through her various businesses and the Dollywood Foundation.\nLGBTQ+ rights.\nThough often politically neutral, Parton is known for her long history of openly supporting LGBTQ rights. LGBTQ+ magazines \"LGBTQ Nation\" and \"The Advocate\" have described her as an \"LGBTQ+ icon,\" and it was noted that she first publicly showed support for LGBTQ families in her 1991 song \"Family\". She also publicly came out in support of same-sex marriage in 2009.\nModerna COVID-19 vaccine.\nIn response to the COVID-19 pandemic, Parton donated $1million towards research at Vanderbilt University Medical Center and encouraged those who can afford it to make similar donations. She said \"I'm a very proud girl today to know I had anything at all to do with something that's going to help us through this crazy pandemic.\" Her donation funded the critical early stages of development of the Moderna vaccine. In March 2021, Parton was vaccinated against COVID-19 at Vanderbilt University. She labeled social media accounts of the occasion \"Dolly gets a dose of her own medicine.\" Parton strongly encouraged everyone to get vaccinated when eligible and performed a song celebrating her vaccination, set to the tune of her song \"Jolene\".\nAwards and honors.\nDolly Parton is one of the most-honored female country performers of all time. The Record Industry Association of America has certified 25 of her single or album releases as either Gold Record, Platinum Record or Multi-Platinum Record. She has had 26 songs reach no.1 on the Billboard country charts, a record for a female artist. She has 42 career Top10 country albums, a record for any artist, and 110 career-charted singles over the past 40 years. As of 2012 she had written more than 3,000 songs and sold more than 100 million records, making her one of the best-selling female artists of all time. As of 2021, she had appeared on the country music charts in each of seven decades, the most of any artist.\nDolly Parton has earned 11 Grammy Awards (including her 2011 Lifetime Achievement Grammy) and a total of 55 Grammy Award nominations, the third-most nominations of any female artist in the history of the prestigious awards. \nAt the American Music Awards, she has won three awards out of 18 nominations. At the Country Music Association, she has won ten awards out of 42 nominations. At the Academy of Country Music, she has won seven awards and 39 nominations. She is one of only six female artists (including Reba McEntire, Barbara Mandrell, Shania Twain, Loretta Lynn, and Taylor Swift), to win the Country Music Association's highest honor, Entertainer of the Year (1978). She also has been nominated for two Academy Awards and a Tony Award. She was nominated for an Emmy Award for her appearance in a 1978 Cher television special. She was awarded a star on the Hollywood Walk of Fame for her music in 1984, located at 6712 Hollywood Boulevard in Hollywood, California; a star on the Nashville StarWalk for Grammy winners; and a bronze sculpture on the courthouse lawn in Sevierville. She has called that statue of herself in her hometown \"the greatest honor\", because it came from the people who knew her. Parton was inducted into the Grand Ole Opry in 1969, and in 1986 was named one of \"Ms. Magazine\" Women of the Year. In 1986, she was inducted into the Nashville Songwriters Hall of Fame.\nIn 1999, Parton received country music's highest honor, an induction into the Country Music Hall of Fame. She received an honorary doctorate degree from Carson-Newman College (Jefferson City, Tennessee) in 1990. This was followed by induction into the National Academy of Popular Music/Songwriters Hall of Fame in 2001. In 2002, she ranked no.4 in CMT's 40 Greatest Women of Country Music.\nParton was honored in 2003 with a tribute album called \"Just Because I'm a Woman: Songs of Dolly Parton\". The artists who recorded versions of Parton's songs included Melissa Etheridge (\"I Will Always Love You\"), Alison Krauss (\"9 to 5\"), Shania Twain (\"Coat of Many Colors\"), Meshell Ndegeocello (\"Two Doors Down\"), Norah Jones (\"The Grass is Blue\"), and Sin\u00e9ad O'Connor (\"Dagger Through the Heart\"). Parton herself contributed a re-recording of the title song, originally the title song for her first RCA album in 1968. Parton was awarded the Living Legend Medal by the U.S. Library of Congress on April 14, 2004, for her contributions to the cultural heritage of the United States. She is also the focus of a Library of Congress collection exploring the influences of country music on her life and career. The collection contains images, articles, sheet music, and more.\nIn 2005, she was honored with the National Medal of Arts, the highest honor given by the U.S. government for excellence in the arts. The award is presented by the U.S. President. On December 3, 2006, Parton received the Kennedy Center Honors from the John F. Kennedy Center for the Performing Arts for her lifetime of contributions to the arts. During the show, some of country music's biggest names came to show their admiration. Carrie Underwood performed \"Islands in the Stream\" with Rogers, Parton's original duet partner. Krauss performed \"Jolene\" and duetted \"Coat of Many Colors\" with Twain. McEntire and Reese Witherspoon also came to pay tribute. On November 16, 2010, Parton accepted the Liseberg Applause Award, the theme park industry's most prestigious honor, on behalf of Dollywood theme park during a ceremony held at IAAPA Attractions Expo 2010 in Orlando, Florida.\nIn 2015, a newly discovered species of lichen found growing in the southern Appalachians was named \"Japewiella dollypartoniana\" in honor of Parton's music and her efforts to bring national and global attention to that region. In 2018, Parton received a second star on the Hollywood Walk of Fame, inducted alongside Linda Ronstadt and Emmylou Harris in recognition of their work as a trio. Parton was also recognized in the Guinness World Records 2018 Edition for holding records for the Most Decades with a Top20 hit on Billboard's Hot Country Songs Chart and Most Hits on Billboard's Hot Country Songs Chart by a Female Artist. In 2020, Parton received a Grammy award for her collaboration with For King &amp; Country on their song, \"God Only Knows\". In 2021, she was included on the \"Time\" 100, \"Time\"s annual list of the 100 most influential people in the world. \"The New York Times\" called her among the three of America's Most Beloved Divas (alongside Patti LaBelle and Barbra Streisand).\nDuring the Trump presidency, Parton turned down the Presidential Medal of Freedom twice due to her husband's illness and the ongoing pandemic. Parton turned down the Presidential Medal of Freedom a third time during the Biden presidency to avoid the appearance of politics. In response to a 2021 proposal by the Tennessee legislature to erect a statue of Parton, she released a statement asking the legislature to remove the bill from consideration, saying \"Given all that is going on in the world, I don't think putting me on a pedestal is appropriate at this time.\"\nIn late 2022, Parton received a $100-million Courage and Civility Award from the founder of Amazon, Jeff Bezos. According to Bezos, the award was given to Parton because of her charity work focused on improving children's literacy around the world.\nIn 2023, Parton was awarded American Library Association Honorary Membership.\nShe was ranked at No. 27 on \"Rolling Stone\"\u2032s 2023 list of the 200 Greatest Singers of All Time.\nAsteroid (10731) Dollyparton, the former 1998 BL3, was named in her honor in 2022.\nHall of Fame honors.\nDuring her career, Parton has gained induction into numerous Halls of Fame. Those honors include:\nDiscography.\nSolo studio albums\nCollaborative studio albums\nFilmography.\nTheatrical releases"}
{"id": "8717", "revid": "24697189", "url": "https://en.wikipedia.org/wiki?curid=8717", "title": "Diprotodon", "text": "Diprotodon (Ancient Greek: \"two protruding front teeth\") is an extinct genus of marsupial from the Pleistocene of Australia containing one species, D. optatum. The earliest finds date to 1.77 million to 780,000 years ago but most specimens are dated to after 110,000 years ago. Its remains were first unearthed in 1830 in Wellington Caves, New South Wales, and contemporaneous paleontologists guessed they belonged to rhinos, elephants, hippos or dugongs. \"Diprotodon\" was formally described by English naturalist Richard Owen in 1838, and was the first named Australian fossil mammal, and led Owen to become the foremost authority of his time on other marsupials and Australian megafauna, which were enigmatic to European science.\n\"Diprotodon\" is the largest-known marsupial to have ever lived; it greatly exceeds the size of its closest living relatives wombats and koalas. It is a member of the extinct family Diprotodontidae, which includes other large quadrupedal herbivores. It grew to at the shoulders, over from head to tail, and likely weighed several tonnes, possibly as much as . Females were much smaller than males. \"Diprotodon\" supported itself on elephant-like legs to travel long distances, and inhabited most of Australia. The digits were weak; most of the weight was probably borne on the wrists and ankles. The hindpaws angled inward at 130\u00b0. Its jaws may have produced a strong bite force of at the long and ever-growing incisor teeth, and over at the last molar. Such powerful jaws would have allowed it to eat vegetation in bulk, crunching and grinding plant materials such as twigs, buds and leaves of woody plants with its bilophodont teeth.\nIt is the only marsupial and metatherian that is known to have made seasonal migrations. Large herds, usually of females, seem to have marched through a wide range of habitats to find food and water, walking at around . \"Diprotodon\" may have formed polygynous societies, possibly using its powerful incisors to fight for mates or fend off predators, such as the largest-known marsupial carnivore \"Thylacoleo carnifex\". Being a marsupial, the mother may have raised her joey in a pouch on her belly, probably with one of these facing backwards, as in wombats.\n\"Diprotodon\" went extinct about 40,000 years ago as part of the Late Pleistocene megafauna extinctions, along with every other Australian animal over ; the extinction was possibly caused by extreme drought conditions and predation pressure from the first Aboriginal Australians, who likely co-existed with \"Diprotodon\" and other megafauna in Australia for several thousand years prior to its extinction. There is little direct evidence of interactions between Aboriginal Australians and \"Diprotodon\"\u2014or most other Australian megafauna. \"Diprotodon\" has been conjectured by some authors to have been the origin of some aboriginal mythological figures\u2014most notably the bunyip\u2014and aboriginal rock artworks, but these ideas are unconfirmable.\nResearch history.\nIn 1830, farmer George Ranken found a diverse fossil assemblage while exploring Wellington Caves, New South Wales, Australia. This was the first major site of extinct Australian megafauna. Remains of \"Diprotodon\" were excavated when Ranken later returned as part of a formal expedition that was headed by explorer Major Thomas Mitchell.\nAt the time these massive fossils were discovered, it was generally thought they were remains of rhinos, elephants, hippos, or dugongs. The fossils were not formally described until Mitchell took them in 1837 to his former colleague English naturalist Richard Owen while in England publishing his journal. In 1838, while studying a piece of a right mandible with an incisor, Owen compared the tooth to those of wombats and hippos; he wrote to Mitchell designating it as a new genus \"Diprotodon\". Mitchell published the correspondence in his journal. Owen formally described \"Diprotodon\" in Volume 2 without mentioning a species; in Volume 1, however, he listed the name \"Diprotodon optatum\", making that the type species. \"Diprotodon\" means \"two protruding front teeth\" in Ancient Greek and \"optatum\" is Latin for \"desire\" or \"wish\". It was the first-ever Australian fossil mammal to be described. In 1844, Owen replaced the name \"D. optatum\" with \"\"D. australis\". Owen only once used the name \"optatum\" and the acceptance of its apparent replacement \"australis\"\" has historically varied widely but \"optatum\" is now standard.\nIn 1843, Mitchell was sent more \"Diprotodon\" fossils from the recently settled Darling Downs and relayed them to Owen. With these, Owen surmised that \"Diprotodon\" was an elephant related to or synonymous with \"Mastodon\" or \"Deinotherium\", pointing to the incisors which he interpreted as tusks, the flattening (anteroposterior compression) of the femur similar to the condition in elephants and rhinos, and the raised ridges of the molar characteristic of elephant teeth. Later that year, he formally synonymised \"Diprotodon\" with \"Deinotherium\" as \"Dinotherium Australe\", which he recanted in 1844 after German naturalist Ludwig Leichhardt pointed out that the incisors clearly belong to a marsupial. Owen still classified the molars from Wellington as \"Mastodon australis\" and continued to describe \"Diprotodon\" as likely elephantine. In 1847, a nearly complete skull and skeleton was recovered from the Darling Downs, the latter confirming this elephantine characterisation. The massive skeleton attracted a large audience while on public display in Sydney. Leichhardt believed the animal was aquatic, and in 1844 he said it might still be alive in an undiscovered tropical area nearer the interior. But, as the European land exploration of Australia progressed, he became certain it was extinct. Owen later become the foremost authority of Australian palaeontology of his time, mostly working with marsupials.\nHuge assemblages of mostly-complete \"Diprotodon\" fossils have been unearthed in dry lakes and riverbeds; the largest assemblage came from Lake Callabonna, South Australia. Fossils were first noticed here by an aboriginal stockman working on a sheep property to the east. The owners, the Ragless brothers, notified the South Australian Museum, which hired Australian geologist Henry Hurst, who reported an enormous wealth of fossil material and was paid \u00a3250 in 1893 to excavate the site. Hurst found up to 360 \"Diprotodon\" individuals over a few acres; excavation was restarted in the 1970s and more were uncovered. American palaeontologist Richard H. Tedford said multiple herds of these animals had at different times become stuck in mud while crossing bodies of water while water levels were low during dry seasons.\nIn addition to \"D. optatum\", several other species were erected in the 19th century, often from single specimens, on the basis of subtle anatomical variations. Among the variations was size difference: adult \"Diprotodon\" specimens have two distinct size ranges. In their 1975 review of Australian fossil mammals, Australian palaeontologists J. A. Mahoney and William David Lindsay Ride did not ascribe this to sexual dimorphism because males and females of modern wombat and koala species\u2014its closest living relatives\u2014are skeletally indistinguishable, so they assumed the same would have been true for extinct relatives, including \"Diprotodon\".\nThese other species are:\nIn 2008, Australian palaeontologist Gilbert Price opted to recognise only one species \"D. optatum\" based most-notably on a lack of dental differences among these supposed species, and said it was likely \"Diprotodon\" was indeed sexually dimorphic, with the male probably being the larger form.\nClassification.\nPhylogeny.\n\"Diprotodon\" is a marsupial in the order Diprotodontia, suborder Vombatiformes (wombats and koalas), and infraorder Vombatomorphia (wombats and allies). It is unclear how different groups of vombatiformes are related to each other because the most-completely known members\u2014living or extinct\u2014are exceptionally derived (highly specialised forms that are quite different from their last common ancestor).\nIn 1872, American mammalogist Theodore Gill erected the superfamily Diprotodontoidea and family Diprotodontidae to house \"Diprotodon\". New species were later added to both groups; by the 1960s, the first diprotodontoids dating to before the Pliocene were discovered, better clarifying their relationship to each other. Because of this, in 1967, American palaeontologist Ruben A. Stirton subdivided Diprotodontoidea into one family, Diprotodontidae, with four subfamilies; Diprotodontinae (containing \"Diprotodon\" among others), Nototheriinae, Zygomaturinae, and Palorchestinae. In 1977, Australian palaeontologist Michael Archer synonymised Nototheriinae with Diprotodontinae and in 1978, Archer and Australian palaeontologist Alan Bartholomai elevated Palorchestinae to family level as Palorchestidae, leaving Diprotodontoidea with families Diprotodontidae and Palorchestidae; and Diprotodontidae with subfamilies Diprotodontinae and Zygomaturinae.\nBelow is the Diprotodontoidea family tree according to Australian palaeontologists Karen H. Black and Brian Mackness, 1999 (top), and Vombatiformes family tree according to Beck \"et al.\" 2020 (bottom):\nEvolution.\nDiprotodontidae is the most diverse family in Vombatomorphia; it was better adapted to the spreading dry, open landscapes over the last tens of millions of years than other groups in the infraorder, living or extinct. \"Diprotodon\" has been found in every Australian state, making it the most-widespread Australian megafauna in the fossil record. The oldest vombatomorph (and vombatiform) is \"Mukupirna\", which was identified in 2020 from Oligocene deposits of the South Australian Namba Formation dating to 26\u201325 million years ago. The group probably evolved much earlier; \"Mukupirna\" was already differentiated as a closer relative to wombats than other vombatiformes, and attained a massive size of roughly , whereas the last common ancestor of vombatiformes was probably a small, creature.\nBoth diprotodontines and zygomaturines were both apparently quite diverse over the Late Oligocene to Early Miocene, roughly 23 million years ago, though the familial and subfamilial classifications of diprotodontoids from this period is debated. Compared to zygomaturines, diprotodontines were rare during the Miocene, the only identified genus being \"Pyramios\". By the Late Miocene, diprotodontians became the commonest marsupial order in fossil sites, a dominance that endures to the present day; at this point, the most-prolific diprotodontians were diprotodontids and kangaroos. Diprotodontidae also began a gigantism trend, along with several other marsupials, probably in response to the lower-quality plant foods available in a drying climate, requiring them to consume much more. Gigantism appears to have evolved independently six times among the vombatiform lineages. Diprotodontine diversity returned in the Pliocene; Diprotodontidae reached peak diversity with seven genera, coinciding with the spread of open forests. In 1977, Archer said \"Diprotodon\" directly evolved from the smaller \"Euryzygoma\", which has been discovered in Pliocene deposits of eastern Australia predating 2.5 million years ago.\nIn general, there is poor resolution on the ages of Australian fossil sites. While the geochronology of \"Diprotodon\" is one of best for Australian megafauna, it is still incomplete and the majority of remains are undated. Price and Australian palaeontologist Katarzyna Piper reported the earliest, indirectly dated \"Diprotodon\" fossils from the Nelson Bay Formation at Nelson Bay, New South Wales, which dates to 1.77 million to 780,000 years ago during the Early Pleistocene. These remains are 8\u201317% smaller than those of Late Pleistocene \"Diprotodon\" but are otherwise indistinguishable. The oldest directly dated \"Diprotodon\" fossils come from the Boney Bite site at Floraville, New South Wales; they were deposited approximately 340,000 years ago during the Middle Pleistocene based on U-series dating and luminescence dating of quartz and orthoclase. Floraville is the only-identified Middle Pleistocene site in tropical northern Australia. Beyond these, almost all dated \"Diprotodon\" material comes from Marine Isotope Stage 5 (MIS5) or younger\u2014after 110,000 years ago during the Late Pleistocene.\nDescription.\nSkull.\n\"Diprotodon\" has a long, narrow skull. Like other marsupials, the top of the skull of \"Diprotodon\" is flat or depressed over the small braincase and the sinuses of the frontal bone. Like many other giant vombatiformes, the frontal sinuses are extensive; in a specimen from Bacchus Marsh, they take up \u2014roughly 25% of skull volume\u2014whereas the brain occupies \u2014only 4% of the skull volume. Marsupials tend to have smaller brain-to-body mass ratios than placental mammals, becoming more disparate the bigger the animal, which could be a response to a need to conserve energy because the brain is a calorically expensive organ, or is proportional to the maternal metabolic rate, which is much less in marsupials due to the shorter gestation period. The expanded sinuses increase the surface area available for the temporalis muscle to attach, which is important for biting and chewing, to compensate for a deflated braincase as a result of a proportionally smaller brain. They may also have helped dissipate stresses produced by biting more efficiently across the skull.\nThe occipital bone, the back of the skull, slopes forward at 45 degrees unlike most modern marsupials, where it is vertical. The base of the occipital is significantly thickened. The occipital condyles, a pair of bones that connect the skull with the vertebral column, are semi-circular and the bottom half is narrower than the top. The inner border, which forms the foramen magnum where the spinal cord feeds through, is thin and well-defined. The top margin of the foramen magnum is somewhat flattened rather than arched. The foramen expands backwards towards the inlet, especially vertically, and is more-reminiscent of a short neural canal\u2014the tube running through a vertebral centrum where the spinal cord passes through\u2014than a foramen magnum.\nA sagittal crest extends across the midline of the skull from the supraoccipital\u2014the top of the occipital bone\u2014to the region between the eyes on the top of the head. The orbit (eye socket) is small and vertically oval-shaped. The nasal bones slightly curve upwards until near their endpoint, where they begin to curve down, giving the bones a somewhat S-shaped profile. Like many marsupials, most of the nasal septum is made of bone rather than cartilage. The nose would have been quite mobile. The height of the skull from the peak of the occipital bone to the end of the nasals is strikingly almost uniform; the end of the nasals is the tallest point. The zygomatic arch (cheek bone) is strong and deep as in kangaroos but unlike those of koalas and wombats, and extends all the way from the supraoccipital.\nJaws.\nAs in kangaroos and wombats, there is a gap between the jointing of the palate (roof of the mouth) and the maxilla (upper jaw) behind the last molar, which is filled by the medial pterygoid plate. This would have been the insertion for the medial pterygoid muscle that was involved in closing the jaw. Like many grazers, the masseter muscle, which is also responsible for closing the jaw, seems to have been the dominant jaw muscle. A probable large temporal muscle compared to the lateral pterygoid muscle may indicate, unlike in wombats, a limited range of side-to-side jaw motion means \"Diprotodon\" would have been better at crushing rather than grinding food. The insertion of the masseter is placed forwards, in front of the orbits, which could have allowed better control over the incisors. \"Diprotodon\" chewing strategy appears to align more with kangaroos than wombats: a powerful vertical crunch was followed by a transverse grinding motion.\nAs in other marsupials, the ramus of the mandible, the portion that goes up to connect with the skull, angles inward. The condyloid process, which connects the jaw to the skull, is similar to that of a koala. The ramus is straight and extends almost vertically, thickening as it approaches the body of the mandible where the teeth are. The depth of the body of the mandible increases from the last molar to the first. The strong mandibular symphysis, which fuses the two halves of the mandible, begins at the front-most end of the third molar; this would prevent either half of the mandible from moving independently of the other, unlike in kangaroos which use this ability to better control their incisors.\nTeeth.\nThe dental formula of \"Diprotodon\" is . In each half of either jaw are three incisors in the upper jaw and one in the lower jaw; there are one premolar and four molars in both jaws but no canines. A long diastema (gap) separates the incisors from the molars.\nThe incisors are scalpriform (chisel-like). Like those of wombats and rodents, the first incisors in both jaws continuously grew throughout the animal's life but the other two upper incisors did not. This combination is not seen in any living marsupial. The cross-section of the upper incisors is circular. In one old male specimen, the first upper incisor measures of which is within the tooth socket; the second is and is in the socket; and the exposed part of the third is . The first incisor is convex and curves outwards but the other two are concave. The lower incisor has a faint upward curve but is otherwise straight and has an oval cross-section. In the same old male specimen, the lower incisor measures , of which is inside the socket.\nThe premolars and molars are bilophodont, each having two distinct lophs (ridges). The premolar is triangular and about half the size of the molars. As in kangaroos, the necks of the lophs are coated in cementum. Unlike in kangaroos, there is no connecting ridge between the lophs. The peaks of these lophs have a thick enamel coating that thins towards the base; this could wear away with use and expose the dentine layer, and beneath that osteodentine. Like the first premolar of other marsupials, the first molar of \"Diprotodon\" and wombats is the only tooth that is replaced. \"D. optatum\" premolars were highly morphologically variable even within the same individual.\nVertebrae.\n\"Diprotodon\" had seven cervical (neck) vertebrae. The atlas, the first cervical (C1), has a pair of deep cavities for insertion of the occipital condyles. The diaphophyses of the atlas, an upward-angled projection on either the side of the vertebra, are relatively short and thick, and resemble those of wombats and koalas. The articular surface of the axis (C2), the part that joints to another vertebra, is slightly concave on the front side and flat on the back side. As in kangaroos, the axis has a low subtriangular hypophysis projecting vertically from the underside of the vertebra and a proportionally long odontoid\u2014a projection from the axis which fits into the atlas\u2014but the neural spine, which projects vertically the topside of the vertebra, is more forwards. The remaining cervicals lack a hypophysis. As in kangaroos, C3 and C4 have a shorter and more-compressed neural spine, which is supported by a low ridge along its midline in the front and the back. The neural spine of C5 is narrower but thicker, and is supported by stronger-but-shorter ridges. C7 had a forked shape on top of the neural spine.\n\"Diprotodon\" probably had 13 dorsal vertebrae and 14 pairs of closely spaced ribs. Like many other mammals, the dorsals initially decrease in breadth and then expand before connecting to the lumbar vertebrae. Unusually, the front dorsals match the short proportions of the cervicals, and the articular surface is flat. At the beginning of the series, the neural spine is broad and angled forward, and is also supported by a low ridge along its midline in the front and the back. In later examples, the neural spine is angled backwards and bifurcates (splits into two). Among mammals, bifurcation of the neural spine is only seen in elephants and humans, and only in a few of the cervicals and not in the dorsals. Compared to those of wombats and kangaroos, the neural arch is proportionally taller. As in elephants, the epiphysial plates (growth plates) and the neural arch, to which the neural spine is attached, are anchylosed\u2014very rigid in regard to the vertebral centrum\u2014which served to support the animal's immense weight.\nLike most marsupials, \"Diprotodon\" likely had six lumbar vertebrae. They retain a proportionally tall neural arch but not the diapophyses, though L1 can retain a small protuberance on one side where a diapophysis would be in a dorsal vertebra; this has been documented in kangaroos and other mammals. The length of each vertebra increases along the series so the lumbar series may have bent downward.\nLike other marsupials, \"Diprotodon\" had two sacral vertebrae. The base of the neural spines of these two were ossified (fused) together.\nLimbs.\nGirdles.\nThe general proportions of the scapula (shoulder blade) align more closely with more-basal vertebrates such as monotremes, birds, reptiles, and fish rather than marsupials and placental mammals. It is triangular and proportionally narrow but unlike most mammals with a triangular scapula, the arm attaches to top of the scapula and the subspinous fossa (the fossa, a depression below the spine of the scapula) becomes bigger towards the arm joint rather than decreasing. The glenoid cavity where the arm connects is oval shaped as in most mammals.\nUnlike other marsupials, the ilia, the large wings of the pelvis, are lamelliform (short and broad, with a flat surface instead of an iliac fossa). Lamelliform ilia have only been recorded in elephants, sloths, and apes, though these groups all have a much-longer sacral vertebra series whereas marsupials are restricted to two sacral vertebrae. The ilia provided strong muscle attachments that were probably oriented and used much the same as those in an elephant. The sacroiliac joint where the pelvis connects to the spine is at 35 degrees in reference to the long axis of the ilium. The ischia, which form part of the hip socket, are thick and rounded tailwards but taper and diverge towards the socket, unlike those in kangaroos, where the ischia proceed almost parallel to each other. They were not connected to the vertebra. The hip socket itself is well-rounded and almost hemispherical.\nLong bones.\nUnlike those of most marsupials, the humerus of \"Diprotodon\" is almost straight rather than S-shaped, and the trochlea of the humerus at the elbow joint is not perforated. The ridges for muscle attachments are poorly developed, which seems to have been compensated for by the powerful forearms. Similarly, the condyles where the radius and ulna (the forearm bones) connect maintain their rounded shape and are quite-similarly sized, and unusually reminiscent of the condyles between the femur and the tibia and fibula in the leg of a kangaroo.\nLike elephants, the femur of \"Diprotodon\" is straight and compressed anteroposteriorly (from headside to tailside). The walls of the femur are prodigiously thickened, strongly constricting the medullary cavity where the bone marrow is located. The proximal end (part closest to the hip joint) is notably long, broad, and deep. The femoral head projects up far from the greater trochanter. As in kangaroos, the greater trochanter is split into two lobes. The femoral neck is roughly the same diameter as the femoral head. Also as in kangaroos, the condyle for the fibula is excavated out but the condyle for the tibia is well-rounded and hemispherical. Like those of many other marsupials, the tibia is twisted and the tibial malleolus (on the ankle) is reduced.\nPaws.\n\"Diprotodon\" has five digits on either paw. Like other plantigrade walkers, where the paws were flat on the ground, the wrist and ankle would have been largely rigid and inflexible. The digits are proportionally weak so the paws probably had a lot of padding. Similarly, the digits do not seem to have been much engaged in weight bearing.\nThe forepaw was strong and the shape of the wrist bones is quite similar to those of kangaroos. Like other vombatiformes, the metacarpals, which connect the fingers to the wrist, are broadly similar to those of kangaroos and allies. The enlarged pisiform bone takes up half the jointing surface of the ulna. The fifth digit on the forepaw is the largest.\nThe digits of the hindpaws turn inwards from the ankle at 130 degrees. The second and third metatarsals (the metatarsals connect the toes to the ankle) are significantly reduced, which may mean these digits were syndactylous (fused) like those of all modern diprotodontians. The first, fourth, and fifth digits are enlarged. The toes are each about the same length, except the fifth which is much stouter.\nSize.\n\"Diprotodon\" is the largest-known marsupial to ever have lived. In life, adult \"Diprotodon\" could have reached at the shoulders and from head to tail. Accounting for cartilaginous intervertebral discs, \"Diprotodon\" may have been 20% longer than reconstructed skeletons, exceeding .\nAs researchers were formulating predictive body-mass equations for fossil species, efforts were largely constrained to eutherian mammals rather than marsupials. The first person to attempt to estimate the living weight of \"Diprotodon\" was Peter Murray in his 1991 review of the megafauna of Pleistocene Australia; Murray made an estimate of using cranial and dental measurements, which he said was probably not a very precise figure. This made \"Diprotodon\" the largest herbivore in Australia. In 2001, Canadian biologist Gary Burness and colleagues did a linear regression between the largest herbivores and carnivores\u2014living or extinct\u2014from every continent (for Australia: \"Diprotodon\", \"Varanus priscus\", and \"Thylacoleo carnifex\") against the landmass area of their continent, and another regression between the daily food intake of living creatures against the landmass of their continents. He calculated the food requirement of \"Diprotodon\" was 50\u201360% smaller than expected for Australia's landmass, which he believed was a result of a generally lower metabolism in marsupials compared to placentals\u2014up to 20% lower\u2014and sparser nutritious vegetation than other continents. The maximum-attainable body size is capped much lower than those for other continents.\nIn 2003, Australian palaeontologist Stephen Wroe and colleagues took a more-sophisticated approach to body mass than Murray's estimate. They made a regression between the minimum circumference of the femora and humeri of 18 quadrupedal marsupials and 32 placentals against body mass, and then inputted 17 \"Diprotodon\" long bones into their predictive model. The results ranged from , for a mean of , though Wroe said reconstructing the weight of extinct creatures that far outweighed living counterparts is problematic. For comparison, an American bison they used in their study weighed and a hippo weighed .\nPaleobiology.\nDiet.\nLike modern megaherbivores, most evidently the African elephant, Pleistocene Australian megafauna likely had a profound effect on the vegetation, limiting the spread of forest cover and woody plants. Carbon isotope analysis suggests \"Diprotodon\" fed on a broad range of foods and, like kangaroos, was consuming both C3\u2014well-watered trees, shrubs, and grasses\u2014and C4 plants\u2014arid grasses, a finding replicated by calcium isotope analysis showing \"Diprotodon\" to have been a mixed feeder. Carbon isotope analyses on \"Diprotodon\" excavated from the Cuddie Springs site in units SU6 (possibly 45,000 years old) and SU9 (350,000 to 570,000 years old) indicate \"Diprotodon\" adopted a somewhat-more-varied seasonal diet as Australia's climate dried but any change was subtle. In contrast, contemporary kangaroos and wombats underwent major dietary shifts or specialisations towards, respectively, C3 and C4 plants. The fossilised, incompletely digested gut contents of one 53,000-year-old individual from Lake Callabonna show its last meal consisted of young leaves, stalks, and twigs.\nThe molars of \"Diprotodon\" are a simple bilophodont shape. Kangaroos use their bilophodont teeth to grind tender, low-fibre plants as a browser as well as grass as a grazer. Kangaroos that predominantly graze have specialised molars to resist the abrasiveness of grass but such adaptations are not exhibited in \"Diprotodon\", which may have had a mixed diet similar to that of a browsing wallaby. It may also have chewed like wallabies, beginning with a vertical crunch before grinding transversely, as opposed to wombats, which only grind transversely. Similarly to many large ungulates (hoofed mammals), the jaws of \"Diprotodon\" were better suited for crushing rather than grinding, which would have permitted it to process vegetation in bulk.\nIn 2016, Australian biologists Alana Sharpe and Thomas Rich estimated the maximum-possible bite force of \"Diprotodon\" using finite element analysis. They calculated at the incisors and across the molar series. For reference, the American alligator can produce forces upwards of . Though these are likely overestimates, the jaws of \"Diprotodon\" were exceptionally strong, which would have allowed it to consume a broad range of plants, including tough, fibrous grasses.\nMigration and sociality.\nIn 2017, by measuring the strontium isotope ratio (87Sr/86Sr) at various points along the \"Diprotodon\" incisor QMF3452 from the Darling Downs, and matching those ratios to the ratios of sites across that region, Price and colleagues determined \"Diprotodon\" made seasonal migrations, probably in search of food or watering holes. This individual appears to have been following the Condamine River and, while apparently keeping to the Darling Downs during the three years this tooth had been growing, it would have been annually making a northwest-southeast round trip. This trek parallels the mammalian mass migrations of modern-day East Africa.\n\"Diprotodon\" is the only identified metatherian that seasonally migrated between two places. A few modern marsupials, such as the red kangaroo, have been documented making migrations when necessary but it is not a seasonal occurrence. Because \"Diprotodon\" could do it, it is likely other Pleistocene Australian megafauna also had seasonal migrations.\n\"Diprotodon\" apparently moved in large herds. Possible fossilised herds, which are most-commonly unearthed in south-eastern Australia, seem to be mostly or entirely female, and sometimes travelled with juveniles. Such sexual segregation is normally seen in polygynous species; it is a common social organisation among modern megaherbivores involving an entirely female herd save for their young and the dominant male, with which the herd exclusinvely breeds. Similarly, the skull is adapted to handling much-higher stresses than that which resulted from bite alone so \"Diprotodon\" may have subjected its teeth or jaws to more-strenuous activities than chewing, such as fighting other \"Diprotodon\" for mates or fending off predators, using the incisors. Like modern red and grey kangaroos, which also sexually segregate, bachelor herds of \"Diprotodon\" seem to have been less tolerant to drought conditions than female herds due to their larger size and nutritional requirements.\nGait.\nThe locomotion of an extinct animal can be inferred using fossil trackways, which seldom preserve in Australia over the Cenozoic. Only the trackways of humans, kangaroos, vombatids, \"Diprotodon\", and the diprotodontid \"Euowenia\" have been identified. \"Diprotodon\" trackways have been found at Lake Callabonna and the Victorian Volcanic Plain grasslands. The diprotodontid manus (forepaw) print is semi-circular and the pes (hindpaw) is reniform (kidney-shaped). Owing to proportionally small digits, most of the weight was borne on the carpus and tarsus\u2014the bones connecting to respectively the wrist and the ankle. Diprotodontines seem to have had a much-more-erect gait, an adaptation to long-distance travel that is similar to that of elephants, rather than the more-sprawling posture of wombats and zygomaturines, though there are no fossil trackways of the latter to verify their reconstructed standing posture.\nAt Lake Callabonna, the single \"Diprotodon\" responsible for the impressions had an average stride length of , trackway width of , and track dimensions in length x width. The gleno-acetabular length\u2014the distance between the shoulders and pelvis\u2014could have been about ; assuming a hip height of , the maker of these tracks was probably moving at around .\nThe single \"Diprotodon\" responsible for the impressions at the volcanic plain had an average stride length of , trackway width of , and pes length of . The gleno-acetabular length may have been about and assuming a hip height of , the maker of the tracks was probably moving at around . Its posture was much-more-sprawled than the example from Callabonna, aligning more with what might be expected of \"Zygomaturus\". The animal may have been a female carrying a large joey in her pouch, the added weight on the stomach altering the gait. The first trackway continues for in a south-easterly direction towards a palaeo-lake. The animal seems to have hesitated while stepping down from the first sand bar on its path with the right pes making three overlapping prints here while shuffling around. The trackway vanishes for a stretch and reappears while the animal seemingly is stepping on wet sediment. Another diprotodontid trackway appears away, moving southerly, which may have been left by the same individual.\nLife history.\nThe marsupial metabolic rate is about 30% lower than that of placentals due to a lower body temperature of . Marsupials give birth at an earlier point in foetal development, relying on lactation to facilitate most of the joey's development; because pregnancy is much-more-energetically expensive, investing in lactation rather than longer gestation can be advantageous in a highly seasonal and unpredictable climate to minimise maternal nutritional requirements. Consequently, marsupials cannot support as large a litter size or as short a generation time.\nBased on the relationship between female body size and life history in kangaroos, a \"Diprotodon\" female would have gestated for six-to-eight weeks, and given birth to a single joey. Given its massive size, \"Diprotodon\" may not have sat down to give birth as do smaller marsupials, possibly standing instead. Like koalas and wombats, the pouch may have faced backwards so the joey could crawl down across its mother's abdomen to enter and attach itself to a teat until it could see\u2014perhaps 260 days\u2014and thermoregulate. It would have permanently left the pouch after 860 days and suckled until reaching after four or five years.\nIn large kangaroos, females usually reach sexual maturity and enter oestrus soon after weaning, and males need double the time to reach sexual maturity. A similar pattern could have been exhibited in \"Diprotodon\". Assuming a lifespan of up to 50 years, a female \"Diprotodon\" could have given birth eight times.\nPalaeoecology.\n\"Diprotodon\" was present across the entire Australian continent by the Late Pleistocene, especially following MIS5 approximately 110,000 years ago. The onset of the Quaternary glaciation, with the continuous advance and retreat of glaciers at the poles, created extreme climatic variability elsewhere. In Australia, the warmer, wetter interglacial periods were received by forests and woodlands; colder, dryer glacial periods were more conducive to grasslands and deserts. The continent progressively became dryer as the Asian monsoons became less influential over Australia: the vast interior had become arid and sandy by 500,000 years ago; the mega-lakes that were once prominent, especially during interglacials in north-western Australia, dried up; and the rainforests of eastern Australia gradually retreated. Aridity has hastened over the last 100,000 years, especially after 60,000 years ago with surging El Ni\u00f1o\u2013Southern Oscillations.\nThe continent-wide distribution of \"Diprotodon\" indicates herds trekked across almost any habitat, much like modern African elephants south of the Sahara. \"Diprotodon\" was a member of a diverse assemblage of megafauna that were endemic to Pleistocene Australia; these also included the thylacine, modern kangaroos, sthenurines (giant short-faced kangaroos), a diversity of modern and giant koala and wombat species, the tapir-like \"Palorchestes\", the giant turtle \"Meiolania\", and the giant bird \"Genyornis\". \"Diprotodon\" coexisted with the diprotodontid \"Zygomaturus trilobus\", which appears to have remained in the forests, whereas \"Diprotodon\" foraged the expanding grasslands and woodlands. Other contemporaneous dipotodontids (\"Hulitherium\", \"Z. nimborensia\", and \"Maokopia\") were insular forms that were restricted to the forests of New Guinea.\nPredation.\nDue to its massive size, \"Diprotodon\" would have been a tough adversary for native carnivores. It contended with the largest-known marsupial predator \"Thylacoleo carnifex\"; while \"Diprotodon\" remains that were gnawed or bitten by \"T. carnifex\" have been identified, it is unclear if the marsupial predator was powerful enough to kill an animal surpassing . The modern jaguar, at half the size of \"T. carnifex\", can kill a bull so it is possible \"T. carnifex\" could have killed small \"Diprotodon\". Similar to recent kangaroos with thylacines or quolls, juvenile \"Diprotodon\" may have been at high risk of predation by \"T. carnifex\"; it and fossils of juvenile \"Diprotodon\" have been recovered from the same caves.\nThe largest predators of Australia were reptiles, most notably the saltwater crocodile, the now-extinct crocodiles \"Paludirex\" and \"Quinkana\", and the giant lizard megalania (\"Varanus priscus\"). At in length, megalania was the largest carnivore of Pleistocene Australia.\nExtinction.\nAs part of the Quaternary extinction event, \"Diprotodon\" and every other Australian land animal heavier than became extinct. The timing and the exact cause are unclear because there is poor resolution on the ages of Australian fossil sites. Since their discovery, the extinction of the Australian megafauna has usually been blamed on the changing climate or overhunting by the first Aboriginal Australians. In 2001, Australian palaeontologist Richard Roberts and colleagues dated 28 major fossil sites across the continent, and were able to provide a precise date for megafaunal extinction. They found most disappear from the fossil record by 80,000 years ago, but \"Diprotodon\"; the giant wombat \"Phascolonus\"; \"Thylacoleo\"; and the short-faced kangaroos \"Procoptodon\", \"Protemnodon\", and \"Simosthenurus\" were identified at Ned's Gully, Queensland, and Kudjal Yolgah Cave, Western Australia, which they dated to respectively 47,000 and 46,000 years ago. Thus, all of the Australian Pleistocene megafauna died out probably between about 50,000 and 41,000 years ago. There also seems to have been a diverse assemblage of megafauna just before their extinction, and all populations across at least western and eastern Australia died out at about the same time. As of 2021, there is still no solid evidence of megafauna surviving past approximately 40,000 years ago; their latest occurrence, including \"Diprotodon\", is recorded at South Walker Creek mine in the north-east at about 40,100 \u00b1 1,700 years ago.\nAt the time Roberts \"et al.\" published their paper, the earliest evidence of human activity in Australia was 56\u00b14 thousand years old, which is close to their calculated date for the megafauna extinction; they hypothesised human hunting had eradicated the last megafauna within about 10,000 years of coexistence. Human hunting had earlier been blamed for the extinction of North American and New Zealand megafauna. Human activity was then generally regarded as the main driver of Australian megafaunal extinction, especially because the megafauna had survived multiple extreme drought periods during glacial periods. At the time, there did not seem to be any evidence of unusually extreme climate during this period. Due to the slowness of marsupial reproduction, even limited megafaunal hunting may have severely weakened the population.\nIn 2005, American geologist Gifford Miller noticed fire abruptly becomes more common about 45,000 years ago; he ascribed this increase to aboriginal fire-stick farmers, who would have regularly started controlled burns to clear highly productive forests and grasslands. Miller said this radically altered the vegetational landscape and promulgated the expanse of the modern-day fire-resilient scrub at the expense of the megafauna. Subsequent studies had difficulty firmly linking controlled burns with major ecological collapse. The frequency of fire could have also increased as a consequence of megafaunal extinction because total plant consumption rapidly fell, leading to faster fuel buildup.\nIn 2017, the human-occupied Madjedbebe rock shelter on the northern Australian coast was dated to about 65,000 years ago, which if correct would mean humans and megafauna had coexisted for over 20,000 years. Other authors have considered this dating questionable. In the 2010s, several ecological studies were published in support of major drought conditions coinciding with the final megafaunal extinctions. Their demise may have been the result of a combination of climatic change, human hunting, and human-driven landscape changes.\nCultural significance.\nFossil evidence.\nDespite the role the first Aboriginal Australians are speculated to have had in the extinction of \"Diprotodon\" and other mammalian megafauna in Australia, there is little evidence humans used them at all in the 20,000 years of coexistence. No fossils of mammalian megafauna suggestive of human butchery or cooking have been found.\nIn 1984, Gail Paton discovered an upper-right \"Diprotodon\" incisor (2I) bearing 28 visible cut marks in Spring Creek, south-western Victoria; Ron Vanderwald and Richard Fullager studied the incisor, which was split in half longitudinally, seemingly while the bone was still fresh but it was glued together before Vanderwald and Fullager could inspect it. Each piece measures in length. The marks are aligned in a straight line, and measure in length, in width, and in depth. They determined it was inconsistent with bite marks from scavenging \"Thylacoleo\" or mice, and concluded it was incised by humans with flint as a counting system or a random doodle. This specimen became one of the most-cited pieces of evidence humans and megafauna directly interacted until a 2020 re-analysis by Australian palaeoanthropologist Michelle Langley identified the engraver as most-likely a tiger quoll.\nIn 2016, Australian archaeologist Giles Hamm and colleagues unearthed a partial right radius belonging to a young \"Diprotodon\" in the Warratyi rock shelter. Because it lacks carnivore damage and the rock shelter is up a sheer face \"Diprotodon\" is unlikely to have climbed, they said humans were responsible for taking the \"Diprotodon\" to the site.\nMythology.\nWhen the first massive fossils in Australia were dug up, it was not clear what animals they might have represented because there were no serious scientists on the continent. Local residents guessed some may have been the remains of rhinos or elephants. European settlers, the most-vocal of whom was Reverend John Dunmore Lang, forwarded these fossils as evidence of the Genesis flood narrative. Aboriginal Australians also attempted to fit the finds into their own religious ideas, quickly associating \"Diprotodon\" with the bunyip, a large, carnivorous, lake monster. Many ethnologists and palaeontologists of the time believed the bunyip to be a tribal memory of the lumbering giant creature that probably frequented marshlands, though at the time it was uncertain whether \"Diprotodon\" and other megafauna were still extant because the Australian continent had not yet been fully explored by Europeans. Scientific investigation into the bunyip was stigmatised after a purported bunyip skull was sensationalised in 1846, and was put on display at the Australian Museum. The following year, however, Owen recognised it as the skull of a foal, and was surprised the burgeoning Australian scientific community could have erred so egregiously.\nIn 1892, Canadian geologist Henry Yorke Lyell Brown reported Aboriginal Australians identified \"Diprotodon\" fossils from Lake Eyre as those of the Rainbow Serpent, which he thought was a giant, bottom-dwelling fish. This notion became somewhat popularised after English geologist John Walter Gregory, who believed the god was a horned, scaly creature, conjectured it was a chimaera of \"Diprotodon\"\u2014which he believed had a horn\u2014and a crocodile. Later workers continued to report some link between the Rainbow Serpent and either \"Diprotodon\" or crocodiles.\nThese kinds of suppositions are not testable and require stories to survive in oral tradition for tens of thousands of years. If Pleistocene megafauna are the basis of some aboriginal mythology, it is unclear if the stories were based on the creatures when they were alive or their fossils being discovered long after their extinction.\nRock art representations.\nAboriginal Australians decorated caves with paintings and drawings of several creatures but the identities of the subjects are often unclear. In 1907, Australian anthropologist Herbert Basedow found footprint petroglyphs in Yunta Springs and Wilkindinna, South Australia, which he believed were those of \"Diprotodon\". In 1988, Australian historian Percy Trezise presented what he thought was a Quinkan depiction of \"Diprotodon\" to the First Congress of the Australian Rock Art Research Association. Both of these claims have their faults because the depictions bear several features that are inconsistent with what is known about \"Diprotodon\". Unlike the more-naturalistic artwork of Early European modern humans, which are more easily identifiable as various animals, aboriginal artwork is much more stylistic and is mostly uninterpretable by an outsider. The subjects of aboriginal paintings can be mythological beings from the Dreaming rather than a corporeal subject."}
{"id": "8718", "revid": "8356162", "url": "https://en.wikipedia.org/wiki?curid=8718", "title": "Dirk Benedict", "text": "Dirk Benedict (born Dirk Niewoehner; March 1, 1945) is an American film, television and stage actor, and author. He is best known for playing the characters Lieutenant Starbuck in the original \"Battlestar Galactica\" film and television series and Templeton \"Face\" Peck in \"The A-Team\" television series. He is the author of \"Confessions of a Kamikaze Cowboy\" and \"And Then We Went Fishing\".\nEarly life.\nBenedict was born Dirk Niewoehner in Helena, Montana, the son of George Edward Niewoehner, a lawyer, and his wife Priscilla Mella (n\u00e9e Metzger), an accountant. He grew up in White Sulphur Springs, Montana. He graduated from Whitman College in 1967.\nBenedict allegedly chose his stage name from a serving of Eggs Benedict he had prior to his acting career. He is of German extraction.\nCareer.\nBenedict's film debut was in the 1972 film \"Georgia, Georgia\". When the New York run for \"Butterflies Are Free\" ended, he received an offer to repeat his performance in Hawaii, opposite Barbara Rush. While there, he appeared as a guest lead on \"Hawaii Five-O\". The producers of a horror film called \"Sssssss\" (1973) saw Benedict's performance in \"Hawaii Five-O\" and promptly cast him as the lead in that movie. He next played the psychotic wife-beating husband of Twiggy in her American film debut, \"W\" (1974). Benedict starred in the television series \"Chopper One\", which aired for one season in 1974. He made two appearances in \"Charlie's Angels\". He also appeared on the \"Donny &amp; Marie\" variety show.\nBenedict's career break came in 1978 when he appeared as Lieutenant Starbuck in the movie and television series \"Battlestar Galactica\". The same year, Benedict starred in the TV film, \"Cruise into Terror\", and appeared in the ensemble movie, \"Scavenger Hunt\" the following year.\n1980s and 1990s.\nIn 1980, Benedict starred alongside Linda Blair in an action-comedy movie called \"Ruckus\". In 1983, Dirk gained further popularity as con man Templeton \"Faceman\" Peck in 1980s action television series \"The A-Team\". He played \"Face\" from to , although the series didn't air until January 1983, and the final episode wasn't shown until 1987 rebroadcasts. The episode \"Steel\" includes a scene at Universal Studios where Face is seen looking bemused as a Cylon walks by him as an in-joke to his previous role in \"Battlestar Galactica\". The clip is incorporated into the series' opening credit sequence from season 3 onward.\nIn 1986, Benedict starred as low-life band manager Harry Smilac in the movie \"Body Slam\" along with Lou Albano, Roddy Piper, and cameo appearances by Freddie Blassie, Ric Flair, and Bruno Sammartino. His character Smilac ends up managing the pro-wrestler \"Quick Rick\" Roberts (Piper) and faces opposition by Captain Lou Murano (Albano) and his wrestling tag-team \"the Cannibals\".\nIn 1987, Benedict took the title role of Shakespeare's \"Hamlet\" at the Abbey Theatre in Manhattan. Both his performance and the entire production were lambasted by critics. Benedict starred in the 1989 TV film \"Trenchcoat in Paradise\".\nIn 1991, Benedict starred in \"Blue Tornado,\" playing Alex, call sign Fireball, an Italian Air Force fighter pilot. Benedict published an autobiography, \"Confessions of a Kamikaze Cowboy: A True Story of Discovery, Acting, Health, Illness, Recovery, and Life\" (Avery Publishing ). In 1993, Benedict starred in \"Shadow Force\".\nBenedict also appeared as Jake Barnes in the 1996 action-adventure film \"Alaska\".\n2000s and 2010s.\nIn 2000, Benedict wrote and directed his first screenplay, \"Cahoots\". Benedict appeared in the 2006 German film \"Goldene Zeiten\" (\"Golden Times\") in a dual role, playing an American former TV star as well as a German lookalike who impersonates him.\nIn 2006, he wrote an online essay criticizing the then-airing \"Battlestar Galactica\" re-imagined series and, especially, its casting of a woman as his character, Starbuck, writing that \"the war against masculinity has been won\" and that \"a television show based on hope, spiritual faith, and family is unimagined and regurgitated as a show of despair, sexual violence and family dysfunction\".\nHe appeared as a contestant on the 2007 UK series of \"Celebrity Big Brother 5\", in which he placed third. He arrived on launch night in a replica of the \"A-Team\" van, smoking a cigar and accompanied by the \"A-Team\" theme tune.\nIn 2010, Benedict starred in a stage production of \"Prescription: Murder\" playing Lieutenant Columbo for the Middle Ground Theatre Company in the UK. Benedict also made a cameo appearance in the 2010 film adaptation of \"The A-Team\" as Pensacola Prisoner Milt.\nIn 2019, Benedict took on the role of Jack Strange in the B movie \"Space Ninjas\", written and directed by Scott McQuaid. Dirk plays an eccentric TV host of a show called \"Stranger Than Fiction\", which is like a hybrid of \"The Twilight Zone\" and \"The X-Files\". The movie is a sci-fi comedy horror that follows a bunch of high school students trying to survive the night of a Space Ninja invasion.\nPersonal life.\nCancer.\nIn the 1970s, Benedict survived a prostate tumor, which he refused to have tested for malignancy. Having rejected conventional medical treatment, he credited his survival to the adoption of a macrobiotic diet recommended to him by actress Gloria Swanson.\nMarriage and family.\nIn 1986, he married Toni Hudson, an actress with whom he has two sons, George and Roland. Hudson had previously appeared as Dana in the fourth season \"A-Team\" episode titled \"Blood, Sweat and Cheers\". They divorced in 1995.\nIn 1998, Benedict learned that he also has another son from an earlier relationship, who was placed for adoption."}
{"id": "8724", "revid": "43618068", "url": "https://en.wikipedia.org/wiki?curid=8724", "title": "Doppler effect", "text": " \nThe Doppler effect (also Doppler shift) is the change in the frequency of a wave in relation to an observer who is moving relative to the source of the wave. The \"Doppler effect\" is named after the physicist Christian Doppler, who described the phenomenon in 1842. A common example of Doppler shift is the change of pitch heard when a vehicle sounding a horn approaches and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.\nWhen the source of the sound wave is moving towards the observer, each successive cycle of the wave is emitted from a position closer to the observer than the previous cycle. Hence, from the observer's perspective, the time between cycles is reduced, meaning the frequency is increased. Conversely, if the source of the sound wave is moving away from the observer, each cycle of the wave is emitted from a position farther from the observer than the previous cycle, so the arrival time between successive cycles is increased, thus reducing the frequency.\nFor waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect in such cases may therefore result from motion of the source, motion of the observer, motion of the medium, or any combination thereof. For waves propagating in vacuum, as is possible for electromagnetic waves or gravitational waves, only the difference in velocity between the observer and the source needs to be considered.\nHistory.\nDoppler first proposed this effect in 1842 in his treatise \"\u00dcber das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels\" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called \"effet Doppler-Fizeau\" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).\nGeneral.\nIn classical physics, where the speeds of source and the receiver relative to the medium are lower than the speed of waves in the medium, the relationship between observed frequency formula_1 and emitted frequency formula_2 is given by:\nformula_3\nwhere\nNote this relationship predicts that the frequency will decrease if either source or receiver is moving away from the other.\nEquivalently, under the assumption that the source is either directly approaching or receding from the observer:\nformula_11\nwhere \nIf the source approaches the observer at an angle (but still with a constant speed), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach; but when the wave is received, the source and observer will no longer be at their closest), and a continued monotonic decrease as it recedes from the observer. When the observer is very close to the path of the object, the transition from high to low frequency is very abrupt. When the observer is far from the path of the object, the transition from high to low frequency is gradual.\nIf the speeds formula_8 and formula_16 are small compared to the speed of the wave, the relationship between observed frequency formula_1 and emitted frequency formula_2 is approximately\nwhere\nwe divide for formula_7\nformula_22\nSince formula_23 we can substitute using the Taylor's series expansion of formula_24 truncating all formula_25 and higher terms:\nformula_26\nWhen substituted in the last line, one gets:\nformula_27\nFor small formula_8 and formula_29, the last term formula_30 becomes insignificant, hence:\nformula_31\nConsequences.\nAssuming a stationary observer and a wave source moving towards the observer at (or exceeding) the speed of the wave, the Doppler equation predicts an infinite (or negative) frequency as from the observer's perspective. Thus, the Doppler equation is inapplicable for such cases. If the wave is a sound wave and the sound source is moving faster than the speed of sound, the resulting shock wave creates a sonic boom.\nLord Rayleigh predicted the following effect in his classic book on sound: if the observer were moving from the (stationary) source at twice the speed of sound, a musical piece \"previously\" emitted by that source would be heard in correct tempo and pitch, but as if played \"backwards\".\nApplications.\nSirens.\nA siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus:\nIn other words, if the siren approached the observer directly, the pitch would remain constant, at a higher than stationary pitch, until the vehicle hit him, and then immediately jump to a new lower pitch. Because the vehicle passes by the observer, the radial speed does not remain constant, but instead varies as a function of the angle between his line of sight and the siren's velocity:\nformula_32\nwhere formula_33 is the angle between the object's forward velocity and the line of sight from the object to the observer.\nAstronomy.\nThe Doppler effect for electromagnetic waves such as light is of widespread use in astronomy to measure the speed at which stars and galaxies are approaching or receding from us, resulting in so called blueshift or redshift, respectively. This may be used to detect if an apparently single star is, in reality, a close binary, to measure the rotational speed of stars and galaxies, or to detect exoplanets. This effect typically happens on a very small scale; there would not be a noticeable difference in visible light to the unaided eye.\nThe use of the Doppler effect in astronomy depends on knowledge of precise frequencies of discrete lines in the spectra of stars.\nAmong the nearby stars, the largest radial velocities with respect to the Sun are +308\u00a0km/s (BD-15\u00b04041, also known as LHS 52, 81.7 light-years away) and \u2212260\u00a0km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial speed means the star is receding from the Sun, negative that it is approaching.\nThe relationship between the expansion of the universe and the Doppler effect is not simple matter of the source moving away from the observer. In cosmology, the redshift of expansion is considered separate from redshifts due to gravity or Doppler motion.\nDistant galaxies also exhibit peculiar motion distinct from their cosmological recession speeds. If redshifts are used to determine distances in accordance with Hubble's law, then these peculiar motions give rise to redshift-space distortions.\nRadar.\nThe Doppler effect is used in some types of radar, to measure the velocity of detected objects. A radar beam is fired at a moving target \u2013 e.g. a motor car, as police use radar to detect speeding motorists \u2013 as it approaches or recedes from the radar source. Each successive radar wave has to travel farther to reach the car, before being reflected and re-detected near the source. As each wave has to move farther, the gap between each wave increases, increasing the wavelength. In some situations, the radar beam is fired at the moving car as it approaches, in which case each successive wave travels a lesser distance, decreasing the wavelength. In either situation, calculations from the Doppler effect accurately determine the car's speed. Moreover, the proximity fuze, developed during World War II, relies upon Doppler radar to detonate explosives at the correct time, height, distance, etc.\nBecause the Doppler shift affects the wave incident upon the target as well as the wave reflected back to the radar, the change in frequency observed by a radar due to a target moving at relative speed formula_34 is twice that from the same target emitting a wave:\nformula_35\nMedical.\nAn echocardiogram can, within certain limits, produce an accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, abnormal communications between the left and right side of the heart, leaking of blood through the valves (valvular regurgitation), and calculation of the cardiac output. Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.\nAlthough \"Doppler\" has become synonymous with \"velocity measurement\" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift (\"when\" the received signal arrives).\nVelocity measurements of blood flow are also used in other fields of medical ultrasonography, such as obstetric ultrasonography and neurology. Velocity measurement of blood flow in arteries and veins based on Doppler effect is an effective tool for diagnosis of vascular problems like stenosis.\nFlow measurement.\nInstruments such as the laser Doppler velocimeter (LDV), and acoustic Doppler velocimeter (ADV) have been developed to measure velocities in a fluid flow. The LDV emits a light beam and the ADV emits an ultrasonic acoustic burst, and measure the Doppler shift in wavelengths of reflections from particles moving with the flow. The actual flow is computed as a function of the water velocity and phase. This technique allows non-intrusive flow measurements, at high precision and high frequency.\nVelocity profile measurement.\nDeveloped originally for velocity measurements in medical applications (blood flow), Ultrasonic Doppler Velocimetry (UDV) can measure in real time complete velocity profile in almost any liquids containing particles in suspension such as dust, gas bubbles, emulsions. Flows can be pulsating, oscillating, laminar or turbulent, stationary or transient. This technique is fully non-invasive.\nSatellites.\nSatellite navigation.\nThe Doppler shift can be exploited for satellite navigation such as in Transit and DORIS.\nSatellite communication.\nDoppler also needs to be compensated in satellite communication. \nFast moving satellites can have a Doppler shift of dozens of kilohertz relative to a ground station. The speed, thus magnitude of Doppler effect, changes due to earth curvature. Dynamic Doppler compensation, where the frequency of a signal is changed progressively during transmission, is used so the satellite receives a constant frequency signal. After realizing that the Doppler shift had not been considered before launch of the Huygens probe of the 2005 Cassini\u2013Huygens mission, the probe trajectory was altered to approach Titan in such a way that its transmissions traveled perpendicular to its direction of motion relative to Cassini, greatly reducing the Doppler shift.\nDoppler shift of the direct path can be estimated by the following formula:\nformula_36\nwhere formula_37 is the speed of the mobile station, formula_38 is the wavelength of the carrier, formula_39 is the elevation angle of the satellite and formula_33 is the driving direction with respect to the satellite.\nThe additional Doppler shift due to the satellite moving can be described as:\nformula_41\nwhere formula_42 is the relative speed of the satellite.\nAudio.\nThe Leslie speaker, most commonly associated with and predominantly used with the famous Hammond organ, takes advantage of the Doppler effect by using an electric motor to rotate an acoustic horn around a loudspeaker, sending its sound in a circle. This results at the listener's ear in rapidly fluctuating frequencies of a keyboard note.\nVibration measurement.\nA laser Doppler vibrometer (LDV) is a non-contact instrument for measuring vibration. The laser beam from the LDV is directed at the surface of interest, and the vibration amplitude and frequency are extracted from the Doppler shift of the laser beam frequency due to the motion of the surface.\nRobotics.\nDynamic real-time path planning in robotics to aid the movement of robots in a sophisticated environment with moving obstacles often take help of Doppler effect. Such applications are specially used for competitive robotics where the environment is constantly changing, such as robosoccer.\nInverse Doppler effect.\nSince 1968 scientists such as Victor Veselago have speculated about the possibility of an inverse Doppler effect. The size of the Doppler shift depends on the refractive index of the medium a wave is traveling through. Some materials are capable of negative refraction, which should lead to a Doppler shift that works in a direction opposite that of a conventional Doppler shift. The first experiment that detected this effect was conducted by Nigel Seddon and Trevor Bearpark in Bristol, United Kingdom in 2003. Later, the inverse Doppler effect was observed in some inhomogeneous materials, and predicted inside a Vavilov\u2013Cherenkov cone."}
{"id": "8725", "revid": "12978", "url": "https://en.wikipedia.org/wiki?curid=8725", "title": "Desmodromic", "text": ""}
{"id": "8727", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=8727", "title": "\u0394T (timekeeping)", "text": "In precise timekeeping, \u0394\"T (Delta \"T, delta-\"T, delta\"T, or D\"T\") is a measure of the cumulative effect of the departure of the Earth's rotation period from the fixed-length day of International Atomic Time (86,400 seconds). Formally, \u0394\"T\" is the time difference between Universal Time (UT, defined by Earth's rotation) and Terrestrial Time (TT, independent of Earth's rotation). The value of \u0394T for the start of 1902 was approximately zero; for 2002 it was about 64 seconds. So Earth's rotations over that century took about 64 seconds longer than would be required for days of atomic time. As well as this long-term drift in the length of the day there are short-term fluctuations in the length of day () which are dealt with separately.\nSince early 2017, the length of the day has happened to be very close to the conventional value, and \u0394T has remained within half a second of 69 seconds.\nCalculation.\nEarth's rotational speed is , and a day corresponds to one period . A rotational acceleration gives a rate of change of the period of , which is usually expressed as . This has dimension of reciprocal time and is commonly reported in units of milliseconds-per-day per century, symbolized as ms/day/cy (understood as (ms/day)/cy). Integrating gives an expression for \u0394\"T\" against time.\nUniversal time.\nUniversal Time is a time scale based on the Earth's rotation, which is somewhat irregular over short periods (days up to a century), thus any time based on it cannot have an accuracy better than 1 in 108. However, a larger, more consistent effect has been observed over many centuries: Earth's rate of rotation is inexorably slowing down. This observed change in the rate of rotation is attributable to two primary forces, one decreasing and one increasing the Earth's rate of rotation. Over the long term, the dominating force is tidal friction, which is slowing the rate of rotation, contributing about ms/day/cy or ms/cy, which is equal to the very small fractional change day/day. The most important force acting in the opposite direction, to speed up the rate, is believed to be a result of the melting of continental ice sheets at the end of the last glacial period. This removed their tremendous weight, allowing the land under them to begin to rebound upward in the polar regions, an effect that is still occurring today and will continue until isostatic equilibrium is reached. This \"post-glacial rebound\" brings mass closer to the rotational axis of the Earth, which makes the Earth spin faster, according to the law of conservation of angular momentum, similar to an ice skater pulling their arms in to spin faster. Models estimate this effect to contribute about \u22120.6\u00a0ms/day/cy. Combining these two effects, the net acceleration (actually a deceleration) of the rotation of the Earth, or the change in the length of the mean solar day (LOD), is +1.7\u00a0ms/day/cy or +62 s/cy2 or +46.5 ns/day2. This matches the average rate derived from astronomical records over the past 27 centuries.\nTerrestrial time.\nTerrestrial Time is a theoretical uniform time scale, defined to provide continuity with the former Ephemeris Time (ET). ET was an independent time-variable, proposed (and its adoption agreed) in the period 1948\u20131952 with the intent of forming a gravitationally uniform time scale as far as was feasible at that time, and depending for its definition on Simon Newcomb's \"Tables of the Sun\" (1895), interpreted in a new way to accommodate certain observed discrepancies. Newcomb's tables formed the basis of all astronomical ephemerides of the Sun from 1900 through 1983: they were originally expressed (and published) in terms of Greenwich Mean Time and the mean solar day, but later, in respect of the period 1960\u20131983, they were treated as expressed in terms of ET, in accordance with the adopted ET proposal of 1948\u201352. ET, in turn, can now be seen (in light of modern results) as close to the average mean solar time between 1750 and 1890 (centered on 1820), because that was the period during which the observations on which Newcomb's tables were based were performed. While TT is strictly uniform (being based on the SI second, every second is the same as every other second), it is in practice realised by International Atomic Time (TAI) with an accuracy of about 1 part in 1014.\nEarth's rate of rotation.\nEarth's rate of rotation must be integrated to obtain time, which is Earth's angular position (specifically, the orientation of the meridian of Greenwich relative to the fictitious mean sun). Integrating +1.7\u00a0ms/d/cy and centering the resulting parabola on the year 1820 yields (to a first approximation) seconds for \u0394\"T\". Smoothed historical measurements of \u0394\"T\" using total solar eclipses are about +17190\u00a0s in the year \u2212500 (501\u00a0BC), +10580\u00a0s in 0 (1\u00a0BC), +5710\u00a0s in 500, +1570\u00a0s in 1000, and +200\u00a0s in 1500. After the invention of the telescope, measurements were made by observing occultations of stars by the Moon, which allowed the derivation of more closely spaced and more accurate values for \u0394\"T\". \u0394\"T\" continued to decrease until it reached a plateau of +11\u00a0\u00b1\u00a06\u00a0s between 1680 and 1866. For about three decades immediately before 1902 it was negative, reaching \u22126.64\u00a0s. Then it increased to +63.83\u00a0s in January 2000 and +68.97\u00a0s in January 2018 and +69.361 s in January 2020, after even a slight decrease from 69.358 s in July 2019 to 69.338 s in September and October 2019 and a new increase in November and December 2019. This will require the addition of an ever-greater number of leap seconds to UTC as long as UTC tracks UT1 with one-second adjustments. (The SI second as now used for UTC, when adopted, was already a little shorter than the current value of the second of mean solar time.) Physically, the meridian of Greenwich in Universal Time is almost always to the east of the meridian in Terrestrial Time, both in the past and in the future. +17190\u00a0s or about \u00a0h corresponds to 71.625\u00b0E. This means that in the year \u2212500 (501\u00a0BC), Earth's faster rotation would cause a total solar eclipse to occur 71.625\u00b0 to the east of the location calculated using the uniform TT.\nValues prior to 1955.\nAll values of \u0394\"T\" before 1955 depend on observations of the Moon, either via eclipses or occultations. The angular momentum lost by the Earth due to friction induced by the Moon's tidal effect is transferred to the Moon, increasing its angular momentum, which means that its moment arm (approximately its distance from the Earth, i.e. precisely the semi-major axis of the Moon's orbit) is increased (for the time being about +3.8\u00a0cm/year), which via Kepler's laws of planetary motion causes the Moon to revolve around the Earth at a slower rate. The cited values of \u0394\"T\" assume that the lunar acceleration (actually a deceleration, that is a negative acceleration) due to this effect is = \u221226\u2033/cy2, where is the mean sidereal angular motion of the Moon. This is close to the best estimate for as of 2002 of \u221225.858\u00a0\u00b1\u00a00.003\u2033/cy2, so \u0394\"T\" need not be recalculated given the uncertainties and smoothing applied to its current values. Nowadays, UT is the observed orientation of the Earth relative to an inertial reference frame formed by extra-galactic radio sources, modified by an adopted ratio between sidereal time and solar time. Its measurement by several observatories is coordinated by the International Earth Rotation and Reference Systems Service (IERS).\nCurrent values.\nRecall by definition. While TT is only theoretical, it is commonly realized as TAI + 32.184 seconds where TAI is UTC plus the current leap seconds, so .\nThis can be rewritten as , where DUT1 is UT1 \u2212 UTC. The value of DUT1 is sent out in the weekly IERS Bulletin A, as well as several time signal services, and by extension serve as a source of the current .\nGeological evidence.\nTidal deceleration rates have varied over the history of the Earth-Moon system. Analysis of layering in fossil mollusc shells from 70 million years ago, in the Late Cretaceous period, shows that there were 372 days a year, and thus that the day was about 23.5 hours long then. Based on geological studies of tidal rhythmites, the day was 21.9\u00b10.4 hours long 620 million years ago and there were 13.1\u00b10.1 synodic months/year and 400\u00b17 solar days/year. The average recession rate of the Moon between then and now has been 2.17\u00b10.31\u00a0cm/year, which is about half the present rate. The present high rate may be due to near resonance between natural ocean frequencies and tidal frequencies."}
{"id": "8728", "revid": "754658", "url": "https://en.wikipedia.org/wiki?curid=8728", "title": "December 22", "text": ""}
{"id": "8729", "revid": "39208069", "url": "https://en.wikipedia.org/wiki?curid=8729", "title": "David Deutsch", "text": "David Elieser Deutsch ( ; born 18 May 1953) is a British physicist at the University of Oxford. He is a visiting professor in the Department of Atomic and Laser Physics at the Centre for Quantum Computation (CQC) in the Clarendon Laboratory of the University of Oxford. He pioneered the field of quantum computation by formulating a description for a quantum Turing machine, as well as specifying an algorithm designed to run on a quantum computer. He is a proponent of the many-worlds interpretation of quantum mechanics.\nEarly life and education.\nDeutsch was born to a Jewish family in Haifa, Israel on 18 May 1953, the son of Oskar and Tikva Deutsch. In London, David attended Geneva House school in Cricklewood (his parents owned and ran the Alma restaurant on Cricklewood Broadway), followed by William Ellis School in Highgate before reading Natural Sciences at Clare College, Cambridge and taking Part III of the Mathematical Tripos. He went on to Wolfson College, Oxford for his doctorate in theoretical physics, about quantum field theory in curved space-time, supervised by Dennis Sciama and Philip Candelas.\nCareer and research.\nHis work on quantum algorithms began with a 1985 paper, later expanded in 1992 along with Richard Jozsa, to produce the Deutsch\u2013Jozsa algorithm, one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm. In his nomination for election as a Fellow of the Royal Society (FRS) in 2008, his contributions were described as:\nSince 2012, he has been working on constructor theory, an attempt at generalizing the quantum theory of computation to cover not just computation but all physical processes. Together with Chiara Marletto, he published a paper in December 2014 entitled \"Constructor theory of information\", that conjectures that information can be expressed solely in terms of which transformations of physical systems are possible and which are impossible.\n\"The Fabric of Reality\".\nIn his 1997 book \"The Fabric of Reality\", Deutsch details his \"Theory of Everything\". It aims not at the reduction of everything to particle physics, but rather mutual support among multiversal, computational, epistemological, and evolutionary principles. His theory of everything is somewhat emergentist rather than reductive. There are four strands to his theory:\nInvariants.\nIn a 2009 TED talk, Deutsch expounded a criterion for scientific explanation, which is to formulate invariants: \"State an explanation [publicly, so that it can be dated and verified by others later] that remains invariant [in the face of apparent change, new information, or unexpected conditions]\".\nInvariance as a fundamental aspect of a scientific account of reality has long been part of philosophy of science: for example, Friedel Weinert's book \"The Scientist as Philosopher\" (2004) noted the presence of the theme in many writings from around 1900 onward, such as works by Henri Poincar\u00e9 (1902), Ernst Cassirer (1920), Max Born (1949 and 1953), Paul Dirac (1958), Olivier Costa de Beauregard (1966), Eugene Wigner (1967), Lawrence Sklar (1974), Michael Friedman (1983), John D. Norton (1992), Nicholas Maxwell (1993), Alan Cook (1994), Alistair Cameron Crombie (1994), Margaret Morrison (1995), Richard Feynman (1997), Robert Nozick (2001), and Tim Maudlin (2002).\n\"The Beginning of Infinity\".\nDeutsch's second book, \"The Beginning of Infinity: Explanations that Transform the World\", was published on 31 March 2011. In this book, he views the European Enlightenment of the 17th and 18th centuries as near the beginning of a potentially unending sequence of purposeful knowledge creation. He examines the nature of knowledge, memes, and how and why creativity evolved in humans.\nAwards and honours.\n\"The Fabric of Reality\" was shortlisted for the Rhone-Poulenc science book award in 1998. Deutsch was awarded the Dirac Prize of the Institute of Physics in 1998, and the Edge of Computation Science Prize in 2005. In 2017, he received the Dirac Medal of the International Centre for Theoretical Physics (ICTP). Deutsch is linked to Paul Dirac through his doctoral advisor Dennis Sciama, whose doctoral advisor was Dirac. Deutsch was elected a Fellow of the Royal Society (FRS) in 2008. In 2018, he received the Micius Quantum Prize. In 2021, he was awarded the Isaac Newton Medal and Prize. On September 22, 2022, he was awarded the Breakthrough Prize in Fundamental Physics, sharing it with 3 others.\nPersonal life.\nDeutsch is a founding member of the parenting and educational method Taking Children Seriously.\nViews on Brexit.\nDeutsch supported Brexit, with his advocacy quoted by then-government adviser, Dominic Cummings, and reported by The New Yorker magazine in January 2020. \nMichael Gove mentioned Deutsch's viewpoint during a BBC Brexit debate. Regarding the debate, Deutsch later commented:\n\"In Britain there is a clear path if you have a grievance, you can join a pressure-group, the pressure-group will pressure the government, or you can see your MP, and the MP will see the grievance building up, and so-on. Whereas, Europe is structured in such a way that it's very difficult to know whom to address your grievance to, or what they could do about it.\" \nDeutsch was not involved in any campaign advocacy for Brexit. His public remarks on the subject were quoted by Cummings and Gove on their own initiative, as Deutsch later made clear."}
{"id": "8730", "revid": "48810580", "url": "https://en.wikipedia.org/wiki?curid=8730", "title": "Volkssturm", "text": "The (; \"people's storm\") was a \"lev\u00e9e en masse\" national militia established by Nazi Germany during the last months of World War II. It was set up by the Nazi Party on the orders of Adolf Hitler and established on 25 September 1944. It was staffed by conscripting males between the ages of 16 and 60 years, who were not already serving in some military unit. \nThe \"Volkssturm\" comprised one of the final components of the total war promulgated by Propaganda Minister Joseph Goebbels, part of a Nazi endeavor to overcome their enemies' military strength through force of will. \"Volkssturm\" units fought unsuccessful battles against Allied forces at the end of the war. On several occasions, its members participated in atrocities, accompanied by German civilians and the Hitler Youth, which were overseen by members of the SS or \"Gaue\" leaders.\nOrigins and organisation.\nThe \"Volkssturm\" drew inspiration from the Prussian \"Landsturm\" of 1813\u20131815, that fought in the liberation wars against Napoleon, mainly as guerrilla forces. Plans to form a \"Landsturm\" national militia in eastern Germany as a last resort to boost fighting strength were first proposed in 1944 by General Heinz Guderian, chief of the German General Staff. The army did not have enough men to resist the Soviet onslaught. So, additional categories of men were called into service, including those in non-essential jobs, those previously deemed unfit, over-age, or under-age, and those recovering from wounds. The \"Volkssturm\" had existed, on paper, since around 1925, but it was only after Hitler ordered Martin Bormann to recruit six million men for this militia that the group became a physical reality. While the regime formally established the \"Volkssturm\" on 25 September, it was not announced to the public until 16 October 1944. The official launch date was two days later, 18 October 1944 and was chosen by Heinrich Himmler to evoke parallels with the popular uprising which, according to popular legend, ended French rule over Germany and culminated in the Battle of Leipzig on the same date in 1813. Despite the appeal for this last-ditch effort, the intended strength of \"six million\" members was never attained.\nJoseph Goebbels and other propagandists depicted the \"Volkssturm\" as an outburst of enthusiasm and the will to resist. Historian Daniel Blatman writes that the \"Volkssturm\" was portrayed as the \"incarnation\" of the greater \"Volksgemeinschaft\", whereby \"all differences in social status, origin, or age vanish and unite all people on the basis of race. It was the service framework for members of the local community, who had been raised together and lived side by side, and now bore arms together in order to defend the community.\" In some regards, the \"Volkssturm\" was the culmination of Goebbels' \"total war\" speech of February 1943 and its formation was \"given a big build-up\" in the November 1944 newsreel episode of \"Die Deutsche Wochenschau\". Consistent messages of final victory from various Nazi media outlets accompanying the \"Volkssturm's\" creation provided a psychological rallying point for the civilian population. While it had some marginal effect on morale, it was undermined by the recruits' visible lack of uniforms and weaponry. Nazi themes of death, transcendence, and commemoration were given full play to encourage the fight. Many German civilians realised that this was a desperate attempt to turn the course of the war. Sardonic old men would remark, \"We old monkeys are the \"F\u00fchrer\"\u2019s newest weapon\" (in German this rhymes: \"Wir alten Affen sind des F\u00fchrers neue Waffen\"). A popular joke about the \"Volkssturm\" went \"Why is the \"Volkssturm\" Germany's most precious resource? Because its members have silver in their hair, gold in their mouths, and lead in their bones.\"\nFor these militia units to be effective, they needed not only strength in numbers, but also fanaticism. During the early stages of \"Volkssturm\" planning, it became apparent that units lacking morale would lack combat effectiveness. To generate fanaticism, \"Volkssturm\" units were placed under the direct command of local Nazi Party officials, the \"Gauleiter\" and \"Kreisleiter\". The new \"Volkssturm\" was also to become a nationwide organisation, with Heinrich Himmler as Replacement Army commander, responsible for armaments and training. Though nominally under party control, \"Volkssturm\" units were placed under \"Heer\" command when engaged in action. At the Reich level, the SS and the Party Chancellery agreed to share responsibility between them. Himmler retained responsibility for military equipment and training while Bormann, head of the Party Chancellery, was charged with oversight of administration and political indoctrination. Aware that a \"people's army\" would not be able to withstand the onslaught of the modern army wielded by the Allies, Hitler issued the following order towards the end of 1944:\n Experience in the East has shown that \"Volkssturm\", emergency and reserve units have little fighting value when left to themselves, and can be quickly destroyed. The fighting value of these units, which are for the most part strong in numbers, but weak in the armaments required for modern battle, is immeasurably higher when they go into action with troops of the regular army in the field. I, therefore, order: where \"Volkssturm\", emergency, and reserve units are available, together with regular units, in any battle sector, mixed battle-groups (brigades) will be formed under unified command, so as to give the \"Volkssturm\", emergency, and reserve units stiffening and support.\nWith the Nazi Party in charge of organising the \"Volkssturm\", each \"Gauleiter\", or Nazi Party District Leader, was charged with the leadership, enrollment, and organisation of the \"Volkssturm\" in their district. The largest \"Volkssturm\" unit seems to have corresponded to the next smaller territorial subdivision of the Nazi Party organisation\u2014the \"Kreis\". The basic unit was a battalion of 642 men. Units were mostly composed of members of the Hitler Youth, invalids, the elderly, or men who had previously been considered unfit for military service. On 12 February 1945, the Nazis conscripted German women and girls into the auxiliaries of the \"Volkssturm\". Correspondingly, girls as young as 14 years were trained in the use of small arms, \"Panzerfausts\", machine guns, and hand grenades from December 1944 through May 1945.\nMunicipal organisation:\nEach \"Gauleiter\" and \"Kreisleiter\" had a \"Volkssturm\" Chief of Staff.\nFrom the militia's inception until the spring of 1945, Himmler and Bormann engaged in a power-struggle over the jurisdictional control over the \"Volkssturm\" regarding security and police powers in Germany and the occupied territories; a contest which Himmler and the SS more or less won on one level (police and security), but lost to Bormann on another (mobilising reserve forces). Historian David Yelton described the situation as two ranking officers at the helm of a sinking ship fighting over command.\nBenito Mussolini suggested, through his son Vittorio, then general secretary of the Republican Fascist Party's German branch, that 30,000 Italians should be added to the \"Volkssturm\" in the defence of Germany. However, no evidence exists that this offer was implemented.\nUniforms and insignia.\nThe \"Volkssturm\" \"uniform\" was only a black armband with the German words \"Deutscher Volkssturm Wehrmacht\" (\"German People's Storm, (of the) Armed Forces\"). The German government tried to issue as many of its members as possible with military uniforms of all sorts, ranging from \"Feldgrau\" to camouflage types. An example of the \"Volkssturm\"'s piecemeal outfitting occurred in the Rhineland, where one unit was provided with \"pre-war black SS uniforms, brown Organization Todt coats, blue Luftwaffe auxiliary caps, and French Adrian helmets.\" Most members of the \"Volkssturm\", especially elderly members, had no uniforms and were not supplied, so they generally wore either work uniforms (including railway workers, policemen, and firemen), Hitler Youth uniforms, old uniforms or parts of uniforms from the First World War, or their civilian clothing and usually carried with them their own personal rucksacks, blankets, cooking-equipment, etc.\nRanks.\nThe simple paramilitary insignia of the \"Volkssturm\" were as follows:\nTraining and impact.\nTypically, members of the \"Volkssturm\" received only very basic military training. It included a brief indoctrination and training on the use of basic weapons such as the Karabiner 98k rifle and \"Panzerfaust\". Because of continuous fighting and weapon shortages, weapon training was often minimal. There was also a lack of instructors, meaning that weapons training was sometimes done by World War I veterans drafted into service themselves. Often \"Volkssturm\" members were only able to familiarise themselves with their weapons when in actual combat.\nThere was no standardisation of any kind and units were issued only what equipment was available. This was true of every form of equipment\u2014\"Volkssturm\" members were required to bring their own uniforms and culinary equipment etc. This resulted in the units looking very ragged and, instead of boosting civilian morale, it often reminded people of Germany's desperate state. Armament was equally haphazard: though some Karabiner 98ks were on hand, members were also issued older Gewehr 98s, Steyr-Mannlicher M1895s, 19th-century Gewehr 71s, and Steyr-Mannlicher M1888s, as well as Dreyse M1907 pistols. In addition there was a plethora of Soviet, British, Belgian, French, Italian, and other weapons that had been captured by German forces during the war. The Germans had also developed cheap \"Volkssturm\" weapons, such as MP 3008 machine pistols and \"Volkssturmgewehr\" rifles. These were completely stamped and machine-pressed constructions (in the 1940s, industrial processes were much cruder than today, so a firearm needed great amounts of semi-artisanal work to be actually reliable). The \"Volkssturm\" troops were nominally supplied when and where possible by both the \"Wehrmacht\" and the SS. By the end of January 1945, the \"Volkssturm\" had only accumulated 40,500 rifles and 2,900 machine guns amid this mish-mash of foreign and outdated assemblage of weapons.\nWhen units had completed their training and received armament, members took a customary oath to Hitler and were then dispatched into combat. Teenagers and middle-aged men were sent to separate training camps, some of whom received as little as ten to fourteen days of training before being sent to fight. Unlike most English-speaking countries, Germany had universal military service for all young men for several generations, so many of the older members would have had at least basic military training from when they served in the German Army and many would have been veterans of the First World War. \"Volkssturm\" units were supposed to be used only in their own districts, but many were sent directly to the front lines. Ultimately, it was their charge to confront the overwhelming power of the British, Canadian, Soviet, American, and French armies alongside \"Wehrmacht\" forces to either turn the tide of the war or set a shining example for future generations of Germans and expunge the defeat of 1918 by fighting to the last, dying before surrendering. It was an apocalyptic goal which some of those assigned to the \"Volkssturm\" took to heart. Unremittingly fanatical members of the \"Volkssturm\" refused to abandon the Nazi ethos unto the dying days of Nazi Germany, and in a number of instances took brutal \"police actions\" against German civilians deemed defeatists or cowards.\nLosses were high among the \"Volkssturm\" \u2013 Battalion 25/235 for instance, started out with 400 men but fought on until there were only 10 men remaining. Fighting at K\u00fcstrin between 30 January to 29 March 1945, militia units made up mostly of the \"Volkssturm\" resisted for nearly two months. Losses were upwards of 60 percent for the \"Volkssturm\" at Kolberg, roughly 1,900 of them died at Breslau, and during the Battle of K\u00f6nigsberg, another 2,400 members of the \"Volkssturm\" were killed. At other times along the western front particularly, \"Volkssturm\" troops would cast their arms aside and disappear into the chaos.\nMany units lost their enthusiasm for the fight when it became clear that the Allies had won, prompting them to lay down their weapons and surrender \u2013 they also feared being captured by Allied forces and tortured or executed as partisans. Duty to their communities also played a part in their capitulation, as did self-preservation.\nBattle of Berlin.\nTheir most extensive use was during the Battle of Berlin, where \"Volkssturm\" units fought in many parts of the city. This battle was particularly devastating to its formations; however, many members fought to the death out of fear of being captured by the Soviets. The \"Volkssturm\" had a strength of about 60,000 in the Berlin area, formed into 92 battalions, of which about 30 battalions of \"Volkssturm I\" (those with some weapons) were sent to forward positions, while those of \"Volkssturm II\" (those without weapons) remained in the inner city. One of the few substantive fighting units left to defend Berlin was the LVI Panzer Corps, which occupied the southeastern sector of the town, whereas the remaining parts of the city were being defended by what remained of the SS, the \"Volkssturm\", and the Hitler Youth formations. Nonetheless, a force of over 2.5 million Soviet troops, equipped with 6,250 tanks and over 40,000 artillery pieces, were assigned to capture the city, and the diminished remnants of the \"Wehrmacht\" were no match for them. Meanwhile, Hitler denounced every perceived \"betrayal\" to the inhabitants of the \"F\u00fchrerbunker\". Not eager to die what was thought to be a pointless death, many older members of the \"Volkssturm\" looked for places to hide from the approaching Soviet Army. \nOne notable and unusual \"Volkssturm\" unit in the Battle for Berlin was the 3/115 Siemensstadt Battalion. It comprised 770 men, mostly World War I veterans in their 50s who were reasonably fit factory workers, with experienced officers. Unlike most \"Volkssturm\" units it was quite well equipped and trained. It was formed into three rifle companies, a support company (with two infantry support guns, four infantry mortars, and heavy machine guns), and a heavy weapons company (with four Soviet M-20 howitzers and a French De Bange 220mm mortar). The battalion first engaged Soviet troops at Friedrichsfelde on 21 April and saw the heaviest fighting over the following two days. It held out until 2 May, by which time it was down to just 50 rifles and two light machine guns. The survivors fell back to join other \"Volkssturm\" units. 26 men from the battalion were awarded the Iron Cross. Allied bombing and Soviet artillery had reduced Berlin to rubble; meanwhile the final stand in Berlin dwindled to fighting against highly trained, battle-hardened Soviet troops on the brink of final victory, who viewed resistance fighters like the \"Volkssturm\" as terrorists in much the same way the \"Wehrmacht\" once had viewed potential partisans during Operation Barbarossa. Red Army soldiers called the Hitler Youth formations and members of the \"Volkssturm\" still fighting to the end in Berlin \"totals\" for being part of Germany's total mobilisation effort.\nRole in atrocities.\nOn several occasions, members of the \"Volkssturm\" participated in atrocities. During January 1945, thousands of prisoners were evacuated and force-marched from several smaller concentration camps\u2014which included Jesau, Seerappen, Schippenbeil, Gerdauen, and Helgenbeil\u2014near K\u00f6nigsberg, many dying along the way. Upon reaching Palmnicken, some 2,500 to 3,000 prisoners of the 5,000 that originally began the journey were lodged in a factory. Mayor and local Nazi party chief, Kurt Friedrichs wanted the SS to send these prisoners on their way since the Red Army was not far away. When local \"Volkssturm\" leader Hans Feyerabend was ordered to transport the suffering prisoners out of the town, he refused to carry out the order and was heard exclaiming that he would not permit a massacre like the one at Katy\u0144 forest. Feyerabend even assigned \"Volkssturm\" guards to keep watch on the local Nazi party members, but this proved fruitless when Friedrich armed a group of Hitler Youth and likewise summoned the local SD elements, whose leaders then commanded the \"Volkssturm\" to help evacuate the prisoners. On 30 January 1945, after the \"Volkssturm\" left with Friedrich in charge, Feyerabend committed suicide; then between 30 January and 1 February the prisoners were murdered by the remaining assemblage of SS guards, Hitler Youth, and the local \"Volkssturm\" unit.\nWhen prisoners fell sick with typhus in Reichsgau Steiermark during February\u2013March 1945, SS men, Hitler Youth, and \"Volkssturm\" units systematically murdered them. Under the orders of Loeben-district Kreisleiter, Otto Christandl, \"Volkssturm\" units in nearby Graz and Eisenerz assisted the Gestapo and Ukrainian \"Waffen-SS\" troops in evacuating between 6,000 and 8,000 prisoners\u2014being marched towards Mauthausen\u2014from their region, many of whom were murdered during the journey when they collapsed from exhaustion.\nSometime in early April 1945 as Allied forces approached the Mittelwerk facilities\u2014where V2 rockets were being produced\u2014the slave labourers from the Mittelbau-Dora concentration camp were force-marched from the western Harz by a collection of guards drawn from the military, the Hitler Youth, and the \"Volkssturm\". Approximately north of Magdeburg, in the village of Mieste, this motley assemblage of guards locked a thousand of these prisoners in a barn and burned them alive at the instruction of a local Nazi Party leader; this event came to be known as the Gardelegen massacre. At the town of Celle in Lower Saxony around the same time, members of the SS, SA, local police, Hitler Youth, and \"Volkssturm\" were aided by locals to \"hunt down and shoot\" prisoners who had fled into the local woodland after their transport train was bombed.\nInterrogated members of the \"Volkssturm\"\u2014when questioned as to where the regular forces had gone\u2014revealed that German soldiers surrendered to the Americans and British instead of the Red Army for fear of reprisals related to the atrocities they had committed in the Soviet Union.\nFinal phase.\nWhile Iron Crosses were being handed out in places like Berlin, other cities and towns like Parchim and Mecklenburg witnessed old elites, acting as military commandants over the Hitler Youth and \"Volkssturm\", asserting themselves and demanding that the defensive fighting stop so as to spare lives and property. Despite their efforts, the last four months of the war were an exercise in futility for the \"Volkssturm\", and the Nazi leadership's insistence to continue the fight to the bitter end contributed to an additional 1.23 million (approximated) deaths, half of them German military personnel and the other half from the \"Volkssturm\".\nIn many small towns, when leading members of the \"Volkssturm\" refused to fight on against the superior forces of the Allies\u2014part of an attempt to circumvent the \"total destruction\" of their home regions\u2014they were tried and \"summarily hanged\" by party activists. During the spring of 1945, thousands of \"Volkssturm\" members were killed like this by Nazi Party fanatics in Franken.\nSee also.\nOther nations:"}
{"id": "8731", "revid": "11917807", "url": "https://en.wikipedia.org/wiki?curid=8731", "title": "Director's cut", "text": "In public use, a director's cut is the director's preferred version of a film (or video game, television episode, music video, commercial, \"etc.\"). It is generally considered a marketing term to represent the version of a film the director prefers, and is usually used as contrast to a theatrical release where the director did not have final cut privilege and did not agree with what was released. (\"Cut\" explicitly refers to the editing process.)\nMost of the time, film directors do not have the \"final cut\" (final say on the version released to the public). Those with money invested in the film, such as the production companies, distributors, or studios, may make changes intended to make the film more profitable at the box office. In extreme cases that can sometimes mean a different ending, less ambiguity, or excluding scenes that would earn a more audience-restricting rating, but more often means that the film is simply shortened to provide more screenings per day.\nWith the rise of home video, the phrase became more generically used as a marketing term to communicate to consumers that this is the director's preferred edit of a film, and it implies the director was not happy with the version that was originally released. Sometimes there are big disagreements between the director's vision and the producer's vision, and the director's preferred edit is sought after by fans (for example Terry Gilliam's \"Brazil\").\nNot all films have separate \"director's cuts\", (often the director is happy with the theatrical release, even if they didn't have final cut privilege), and sometimes separate versions of films are released as \"director's cuts\" even if the director doesn't prefer them. Once such example is Ridley Scott's \"Alien\", which had a \"director's cut\" released in 2003, even though the director said it was purely for \"marketing purposes\" and didn't represent his preferred vision for the film.\nSometimes alternate edits are released, which are not necessarily director's preferred cuts, but which showcase different visions for the project for fans to enjoy. Examples include James Cameron's \"Avatar\", which was released as both a \"Special Edition\" and \"Extended\" cuts, and Peter Jackson's \"Lord of the Rings\", which were released on home video as \"Extended Editions\". These versions do not represent the director's preferred visions.\nThe term since expanded to include media such as video games, comic books and music albums (the latter two of which don't actually have directors).\nOriginal use of the phrase.\nWithin the industry itself, a \"director's cut\" refers to a stage in the editing process, and is not usually what a director wants to release to the public, due to the fact it is unfinished. The editing process of a film is broken into stages: First is the assembly/rough cut, where all selected takes are put together in the order in which they should appear in the film. Next, the editor's cut is reduced from the rough cut; the editor may be guided by their own choices or following notes from the director or producers. Eventually is the final cut, which actually gets released or broadcast. In between the editor's cut and the final cut can come any number of fine cuts, including the director's cut. The director's cut may include unsatisfactory takes, a preliminary soundtrack, a lack of desired pick-up shots etc., which the director would not like to be shown but uses as a placeholder until satisfactory replacements can be inserted. This is still how the term is used within the film industry, as well as commercials, television, and music videos.\nInception.\nThe trend of releasing alternate cuts of films for artistic reasons became prominent in the 1970s; in 1974, the \"director's cut\" of \"The Wild Bunch\" was shown theatrically in Los Angeles to sold-out audiences. The theatrical release of the film had cut 10 minutes to get an R rating, but this cut was hailed as superior and has now become the definitive one. Other early examples include George Lucas's first two films being re-released following the success of \"Star Wars\", in cuts which more closely resembled his vision, or Peter Bogdanovich re-cutting \"The Last Picture Show\" several times. Charlie Chaplin also re-released all of his films in the 1970s, several of which were re-cut (Chaplin's re-release of \"The Gold Rush\" in the 1940s is almost certainly the earliest prominent example of a director's re-cut film being released to the public). A theatrical re-release of \"Close Encounters of the Third Kind\" used the phrase \"Special Edition\" to describe a cut which was closer to Spielberg's intent but had a compromised ending demanded by the studio.\nAs the home video industry rose in the early 1980s, video releases of director's cuts were sometimes created for the small but dedicated cult fan market. Los Angeles cable station Z Channel is also cited as significant in the popularization of alternate cuts. Early examples of films released in this manner include Michael Cimino's \"Heaven's Gate\", where a longer cut was recalled from theatres but subsequently shown on cable and eventually released to home video; James Cameron's \"Aliens\", where a video release restored 20 minutes the studio had insisted on cutting; Cameron also voluntarily made cuts to the theatrical version of \"The Abyss\" for pacing but restored them for a video release, and most famously, Ridley Scott's \"Blade Runner\", where an alternate workprint version was released to fan acclaim, ultimately resulting in the 1992 recut. Scott later recut the film once more, releasing a version dubbed \"The Final Cut\" in 2007. This was the final re-cut and the first in which Scott maintained creative control over the final product, leading to The Final Cut being considered the definitive version of the film.\nCriticism.\nOnce distributors discovered that consumers would buy alternate versions of films, it became more common for films to have alternative versions released. And the original public meaning of a director's preferred vision has become ignored, leading to so-called \"director's cuts\" of films where the director prefers the theatrically released version (or when the director had actual final cut privilege in the first place). Such versions are often marketing ploys, assembled by simply restoring deleted scenes, sometimes adding as much as a half-hour to the length of the film without regard to pacing and storytelling.\nAs a result, the \"director's cut\" is often considered a misnomer. Some directors deliberately try to avoid labelling alternate versions as such (e.g. Peter Jackson and James Cameron; each using the phrases \"Special Edition\" or \"Extended Edition\" for alternate versions of their films).\nSometimes the term is used a marketing ploy. For example, Ridley Scott states on the director's commentary track of \"Alien\" that the original theatrical release was his \"director's cut\", and that the new version was released as a marketing ploy. Director Peter Bogdanovich, no stranger to director's cuts himself, cites \"Red River\" as an example where \nAnother way that released director's cuts can be compromised is when directors were never allowed to even shoot their vision, and thus when the film is re-cut, they must make do with the footage that exists. Examples of this include Terry Zwigoff's \"Bad Santa\", Brian Helgeland's \"Payback\", and most notably the Richard Donner re-cut of \"Superman II\". Donner completed about 75 per cent of the shooting of the sequel during the shooting of the first one but was fired from the project. of the film includes, among other things, screen test footage of stars Christopher Reeve and Margot Kidder, footage used in the first film, and entire scenes that were shot by replacement director Richard Lester which Donner dislikes but were required for story purposes.\nOn the other side, some critics (such as Roger Ebert) have approved of the use of the label in unsuccessful films that had been tampered with by studio executives, such as Sergio Leone's original cut of \"Once Upon a Time in America\", and the moderately successful theatrical version of \"Daredevil\", which were altered by studio interference for their theatrical release. Other well-received director's cuts include Ridley Scott's \"Kingdom of Heaven\" (with \"Empire\" magazine stating: \"The added 45 minutes in the Director\u2019s Cut are like pieces missing from a beautiful but incomplete puzzle\"), or Sam Peckinpah's \"Pat Garrett and Billy the Kid\", where the restored 115-minute cut is closer to the director's intent than the theatrical 105-minute cut (the actual director's cut was 122 minutes; it was never completed to Peckinpah's satisfaction, but was used as a guide for the restoration that was done after his death).\nIn some instances, such as Peter Weir's \"Picnic at Hanging Rock\", Robert Wise's \"\", John Cassavetes's \"The Killing of a Chinese Bookie\", Blake Edwards's \"Darling Lili\" and Francis Ford Coppola's , changes made to a director's cut resulted in a very similar runtime or a shorter, more compact cut. This generally happens when a distributor insists that a film be completed to meet a release date, but sometimes it is the result of removing scenes that the distributor insisted on inserting, as opposed to restoring scenes they insisted on cutting.\nExtended cuts and special editions.\nSeparate to director's cuts are alternate cuts released as \"special editions\" or \"extended cuts\". These versions are often put together for home video for fans, and should not be confused with 'director's cuts'. For example, despite releasing extended versions of his \"The Lord of the Rings\" trilogy, Peter Jackson told IGN in 2019 that \u201cthe theatrical versions are the definitive versions, I regard the extended cuts as being a novelty for the fans that really want to see the extra material.\u201d\nJames Cameron has shared similar sentiments regarding the special editions of his films, \"What I put into theaters is the Director's Cut. Nothing was cut that I didn't want cut. All the extra scenes we've added back in are just a bonus for the fans.\" Similar statements were made by Ridley Scott for the 2003 'director's cut' of \"Alien\".\nSuch alternate versions sometimes include changes to the special effects in addition to different editing, such as George Lucas's \"Star Wars\" films, and Steven Spielberg's \"E.T. the Extra-Terrestrial\".\nExtended or special editions can also apply to films that have been extended for television or cut out to fill time slots and long advertisement breaks, against the explicit wishes of the director, such as the TV versions of \"Dune\" (1984), \"The Warriors\" (1979), \"Superman\" (1978) and the \"Harry Potter\" films.\nExamples of alternate cuts.\n\"The Lord of the Rings\" film series directed by Peter Jackson saw an \"Extended Edition\" release for each of the three films ' (2001), ' (2002), and \"\" (2003) featuring an additional 30 minutes, 47 minutes and 51 minutes respectively of new scenes, special effects and music alongside fan-club credits. These versions of the films were not Jackson's preferred edit, however, they were simply extended versions for fans to enjoy at home.\n\"\" directed by Zack Snyder had an \"Ultimate Edition,\" which added back 31 minutes of footage cut for the theatrical release and received an R rating, released digitally on 28 June 2016, and on Blu-ray on 19 July 2016.\nThe film \"Justice League\" which suffered a very troubled production, was begun by Snyder, who completed a pre-postproduction director's cut but had to step down before completing the project due to his daughter's death. Joss Whedon was hired by the films' distributor Warner Bros. Pictures to complete the film, which was however heavily re-shot, re-edited and released in 2017 with Snyder retaining the directorial credit, to negative reception from general audience, fans and critics alike and a box office failure. Following a global fan campaign to which the director and members of the cast and crew showed support, Snyder was allowed to return and complete the project the way he intended it and a 4-hour version of the film dubbed \"Zack Snyder's Justice League\" with some additionally shot scenes at the end was released on March 18, 2021, on HBO Max to more favorable reviews than the original version. Snyder originally teased a 214-minute cut of the film that was supposed to be the theatrical version released in 2017 if he did not step down from the project.\nSnyder has also confirmed that his Netflix distributed sci-fi film \"Rebel Moon \u2013 Part One: A Child of Fire\" (2023) and its sequel \"\" (2024) would receive R-rated director's cuts with its new titles \"Rebel Moon \u2013 Chapter One: Chalice of Blood\", and the sequel \"Rebel Moon \u2013 Chapter Two: Curse of Forgiveness\" (both 2024). The PG-13 initial versions of those films having been critically panned.\nThe film \"Caligula\" exists in at least 10 different officially released versions, ranging from a sub-90-minute television edit version of TV-14 (later TV-MA) for cable television to an unrated full pornographic version exceeding 3.5 hours. This is believed to be the largest amount of distinct versions of a single film. Among major studio films, the record is believed to be held by \"Blade Runner\"; the magazine \"Video Watchdog\" counted no less than seven distinct versions in a 1993 issue, before director Ridley Scott later released a \"Final Cut\" in 2007 to acclaim from critics including Roger Ebert who included it on his great movies list, The release of \"Blade Runner: The Final Cut\" brings the supposed grand total to eight differing versions of \"Blade Runner\".\nUpon its release on DVD and Blu-ray in 2019, \"\" featured an extended cut with seven minutes of additional footage. This is the first time since \"Harry Potter and the Chamber of Secrets\" that a Wizarding World film has had one.\nAn animated example of an extended cut without the approval of the director was 1983's \"Twice Upon a Time\", which was extended to have more profanity (supervised by co-writer and producer Bill Couturi\u00e9) as opposed to co-director John Korty's original.\nThe Coen Brothers' \"Blood Simple\" is one of few examples that demonstrate director's cuts are not necessarily longer.\nMusic videos.\nThe music video for the 2006 Academy Award-nominated song \"Listen\", performed by Beyonc\u00e9, received a director's cut by Diane Martel. This version of the video was later included on Knowles' \"B'Day Anthology Video Album\" (2007). Linkin Park has a director's cut version for their music video \"Faint\" (directed by Mark Romanek) in which one of the band members spray paints the words \"En Proceso\" on a wall, as well as Hoobastank also having one for 2004's \"The Reason\" which omits the woman getting hit by the car. Britney Spears' music video for 2007's \"Gimme More\" was first released as a director's cut on iTunes, with the official video released 3 days later. Many other director's cut music videos contain sexual content that can't be shown on TV thus creating alternative scenes, such as Thirty Seconds to Mars's \"Hurricane\", and in some cases, alternative videos, such as in the case of Spears' 2008 video for \"Womanizer\".\nExpanded usage in pop culture.\nAs the trend became more widely recognized, the term \"director's cut\" became increasingly used as a colloquialism to refer to an expanded version of other things, including video games, music, and comic books. This confusing usage only served to further reduce the artistic value of a director's cut, and it is currently rarely used in those ways.\nVideo games.\nFor video games, these expanded versions, also referred as \"complete editions\", will have additions to the gameplay or additional game modes and features outside the main portion of the game.\nAs is the case with certain high-profile Japanese-produced games, the game designers may take the liberty to revise their product for the overseas market with additional features during the localization process. These features are later added back to the native market in a re-release of a game in what is often referred as the international version of the game. This was the case with the overseas versions of \"Final Fantasy VII\", \"Metal Gear Solid\" and \"Rogue Galaxy\", which contained additional features (such as new difficulty settings for \"Metal Gear Solid\"), resulting in re-released versions of those respective games in Japan (\"Final Fantasy VII International\", ' and \"Rogue Galaxy: Director's Cut\"). In the case of ' and ', the American versions were released first, followed by the Japanese versions and then the European versions, with each regional release offering new content not found in the previous one. All of the added content from the Japanese and European versions of those games were included in the expanded editions titled ' and \".\nThey also, similar to movies, will occasionally include extra, uncensored or alternate versions of cutscenes, as was the case with \". In markets with strict censorship, a later relaxing of those laws occasional will result in the game being rereleased with the \"Special/Uncut Edition\" tag added to differentiate between the originally released censored version and the current uncensored edition.\nSeveral of the \"Pok\u00e9mon\" games have also received director's cuts and have used the term \"extension\", though \"remake\" and \"third version\" are also often used by many fans. These include ' (Japan only), \"Pok\u00e9mon Yellow\" (for \"Pok\u00e9mon Red\" and \"Green\"/\"Blue\"), \"Pok\u00e9mon Crystal\" (for \"Pok\u00e9mon Gold\" and \"Silver\"), \"Pok\u00e9mon Emerald\" (for \"Pok\u00e9mon Ruby\" and \"Sapphire\"), \"Pok\u00e9mon Platinum\" (for 'Pok\u00e9mon Diamond\" and \"Pearl\"\") and \"Pok\u00e9mon Ultra Sun\" and \"Ultra Moon\".\nFor their PlayStation 5 \"Director's Cut\" releases of the PlayStation 4 games \"Ghost of Tsushima\" and \"Death Stranding\" both received expanded features on both games.\nMusic.\n\"Director's cuts\" in music are rarely released. A few exceptions include Guided by Voices' 1994 album \"Bee Thousand\", which was re-released as a three disc vinyl LP director's cut in 2004, and Fall Out Boy's 2003 album \"Take This to Your Grave\", which was re-released as a Director's cut in 2005 with two extra tracks.\nIn 2011 British singer Kate Bush released the album titled \"Director's Cut\". It is made up of songs from her earlier albums \"The Sensual World\" and \"The Red Shoes\" which have been remixed and restructured, three of which were re-recorded completely."}
{"id": "8733", "revid": "1272791909", "url": "https://en.wikipedia.org/wiki?curid=8733", "title": "Digital video", "text": "Digital video is an electronic representation of moving visual images (video) in the form of encoded digital data. This is in contrast to analog video, which represents moving visual images in the form of analog signals. Digital video comprises a series of digital images displayed in rapid succession, usually at 24, 25, 30, or 60 frames per second. Digital video has many advantages such as easy copying, multicasting, sharing and storage.\nDigital video was first introduced commercially in 1986 with the Sony D1 format, which recorded an uncompressed standard-definition component video signal in digital form. In addition to uncompressed formats, popular compressed digital video formats today include MPEG-2, H.264 and AV1. Modern interconnect standards used for playback of digital video include HDMI, DisplayPort, Digital Visual Interface (DVI) and serial digital interface (SDI).\nDigital video can be copied and reproduced with no degradation in quality. In contrast, when analog sources are copied, they experience generation loss. Digital video can be stored on digital media such as Blu-ray Disc, on computer data storage, or streamed over the Internet to end users who watch content on a personal computer or mobile device screen or a digital smart TV. Today, digital video content such as TV shows and movies also includes a digital audio soundtrack.\nHistory.\nCameras.\nThe basis for digital video cameras is metal\u2013oxide\u2013semiconductor (MOS) image sensors. The first practical semiconductor image sensor was the charge-coupled device (CCD), invented in 1969 by Willard S. Boyle, who won a Nobel Prize for his work in physics. Following the commercialization of CCD sensors during the late 1970s to early 1980s, the entertainment industry slowly began transitioning to digital imaging and digital video from analog video over the next two decades. The CCD was followed by the CMOS active-pixel sensor (CMOS sensor), developed in the 1990s.\nMajor films shot on digital video overtook those shot on film in 2013. Since 2016 over 90% of major films were shot on digital video. , 92% of films are shot on digital. Only 24 major films released in 2018 were shot on 35mm. Today, cameras from companies like Sony, Panasonic, JVC and Canon offer a variety of choices for shooting high-definition video. At the high end of the market, there has been an emergence of cameras aimed specifically at the digital cinema market. These cameras from Sony, Vision Research, Arri, Blackmagic Design, Panavision, Grass Valley and Red offer resolution and dynamic range that exceeds that of traditional video cameras, which are designed for the limited needs of broadcast television.\nCoding.\nIn the 1970s, pulse-code modulation (PCM) induced the birth of digital video coding, demanding high bit rates of 45-140\u00a0Mbit/s for standard-definition (SD) content. By the 1980s, the discrete cosine transform (DCT) became the standard for digital video compression.\nThe first digital video coding standard was H.120, created by the (International Telegraph and Telephone Consultative Committee) or CCITT (now ITU-T) in 1984. H.120 was not practical due to weak performance. H.120 was based on differential pulse-code modulation (DPCM), a compression algorithm that was inefficient for video coding. During the late 1980s, a number of companies began experimenting with DCT, a much more efficient form of compression for video coding. The CCITT received 14 proposals for DCT-based video compression formats, in contrast to a single proposal based on vector quantization (VQ) compression. The H.261 standard was developed based on DCT compression, becoming first practical video coding standard. Since H.261, DCT compression has been adopted by all the major video coding standards that followed.\nMPEG-1, developed by the Motion Picture Experts Group (MPEG), followed in 1991, and it was designed to compress VHS-quality video. It was succeeded in 1994 by MPEG-2/H.262, which became the standard video format for DVD and SD digital television. It was followed by MPEG-4 in 1999, and then in 2003 it was followed by H.264/MPEG-4 AVC, which has become the most widely used video coding standard.\nThe current-generation video coding format is HEVC (H.265), introduced in 2013. While AVC uses the integer DCT with 4x4 and 8x8 block sizes, HEVC uses integer DCT and DST transforms with varied block sizes between 4x4 and 32x32. HEVC is heavily patented, with the majority of patents belonging to Samsung Electronics, GE, NTT and JVC Kenwood. It is currently being challenged by the aiming-to-be-freely-licensed AV1 format. , AVC is by far the most commonly used format for the recording, compression and distribution of video content, used by 91% of video developers, followed by HEVC which is used by 43% of developers.\nProduction.\nStarting in the late 1970s to the early 1980s, video production equipment that was digital in its internal workings was introduced. These included time base correctors (TBC) and digital video effects (DVE) units. They operated by taking a standard analog composite video input and digitizing it internally. This made it easier to either correct or enhance the video signal, as in the case of a TBC, or to manipulate and add effects to the video, in the case of a DVE unit. The digitized and processed video information was then converted back to standard analog video for output.\nLater on in the 1970s, manufacturers of professional video broadcast equipment, such as Bosch (through their Fernseh division) and Ampex developed prototype digital videotape recorders (VTR) in their research and development labs. Bosch's machine used a modified 1-inch type B videotape transport and recorded an early form of CCIR 601 digital video. Ampex's prototype digital video recorder used a modified 2-inch quadruplex videotape VTR (an Ampex AVR-3) fitted with custom digital video electronics and a special \"octaplex\" 8-head headwheel (regular analog 2\" quad machines only used 4 heads). Like standard 2\" quad, the audio on the Ampex prototype digital machine, nicknamed \"Annie\" by its developers, still recorded the audio in analog as linear tracks on the tape. None of these machines from these manufacturers were ever marketed commercially.\nDigital video was first introduced commercially in 1986 with the Sony D1 format, which recorded an uncompressed standard definition component video signal in digital form. Component video connections required 3 cables, but most television facilities were wired for composite NTSC or PAL video using one cable. Due to this incompatibility the cost of the recorder, D1 was used primarily by large television networks and other component-video capable video studios.\nIn 1988, Sony and Ampex co-developed and released the D2 digital videocassette format, which recorded video digitally without compression in ITU-601 format, much like D1. In comparison, D2 had the major difference of encoding the video in composite form to the NTSC standard, thereby only requiring single-cable composite video connections to and from a D2 VCR. This made it a perfect fit for the majority of television facilities at the time. D2 was a successful format in the television broadcast industry throughout the late '80s and the '90s. D2 was also widely used in that era as the master tape format for mastering laserdiscs.\nD1 &amp; D2 would eventually be replaced by cheaper systems using video compression, most notably Sony's Digital Betacam, that were introduced into the network's television studios. Other examples of digital video formats utilizing compression were Ampex's DCT (the first to employ such when introduced in 1992), the industry-standard DV and MiniDV and its professional variations, Sony's DVCAM and Panasonic's DVCPRO, and Betacam SX, a lower-cost variant of Digital Betacam using MPEG-2 compression.\nOne of the first digital video products to run on personal computers was \"PACo: The PICS Animation Compiler\" from The Company of Science &amp; Art in Providence, RI. It was developed starting in 1990 and first shipped in May 1991. PACo could stream unlimited-length video with synchronized sound from a single file (with the \".CAV\" file extension) on CD-ROM. Creation required a Mac, and playback was possible on Macs, PCs, and Sun SPARCstations.\nQuickTime, Apple Computer's multimedia framework, was released in June 1991. Audio Video Interleave from Microsoft followed in 1992. Initial consumer-level content creation tools were crude, requiring an analog video source to be digitized to a computer-readable format. While low-quality at first, consumer digital video increased rapidly in quality, first with the introduction of playback standards such as MPEG-1 and MPEG-2 (adopted for use in television transmission and DVD media), and the introduction of the DV tape format allowing recordings in the format to be transferred directly to digital video files using a FireWire port on an editing computer. This simplified the process, allowing non-linear editing systems (NLE) to be deployed cheaply and widely on desktop computers with no external playback or recording equipment needed.\nThe widespread adoption of digital video and accompanying compression formats has reduced the bandwidth needed for a high-definition video signal (with HDV and AVCHD, as well as several professional formats such as XDCAM, all using less bandwidth than a standard definition analog signal). These savings have increased the number of channels available on cable television and direct broadcast satellite systems, created opportunities for spectrum reallocation of terrestrial television broadcast frequencies, and made tapeless camcorders based on flash memory possible, among other innovations and efficiencies.\nCulture.\nCulturally, digital video has allowed video and film to become widely available and popular, beneficial to entertainment, education, and research. Digital video is increasingly common in schools, with students and teachers taking an interest in learning how to use it in relevant ways. Digital video also has healthcare applications, allowing doctors to track infant heart rates and oxygen levels.\nIn addition, the switch from analog to digital video impacted media in various ways, such as in how businesses use cameras for surveillance. Closed circuit television (CCTV) switched to using digital video recorders (DVR), presenting the issue of how to store recordings for evidence collection. Today, digital video is able to be compressed in order to save storage space.\nDigital television.\nDigital television (DTV) is the production and transmission of digital video from networks to consumers. This technique uses digital encoding instead of analog signals used prior to the 1950s. As compared to analog methods, DTV is faster and provides more capabilities and options for data to be transmitted and shared.\nDigital television's roots are tied to the availability of inexpensive, high-performance computers. It was not until the 1990s that digital TV became a real possibility. Digital television was previously not practically feasible due to the impractically high bandwidth requirements of uncompressed video, requiring around 200Mbit/s for a standard-definition television (SDTV) signal, and over 1Gbit/s for high-definition television (HDTV).\nOverview.\nDigital video comprises a series of digital images displayed in rapid succession. In the context of video, these images are called frames. The rate at which frames are displayed is known as the frame rate and is measured in frames per second. Every frame is a digital image and so comprises a formation of pixels. The color of a pixel is represented by a fixed number of bits of that color where the information of the color is stored within the image. For example, 8-bit captures 256 levels per channel, and 10-bit captures 1,024 levels per channel. The more bits, the more subtle variations of colors can be reproduced. This is called the color depth, or bit depth, of the video.\nInterlacing.\nIn interlaced video each \"frame\" is composed of two halves of an image. The first half contains only the odd-numbered lines of a full frame. The second half contains only the even-numbered lines. These halves are referred to individually as \"fields\". Two consecutive fields compose a full frame. If an interlaced video has a frame rate of 30 frames per second the field rate is 60 fields per second, though both part of interlaced video, frames per second and fields per second are separate numbers.\nBit rate and BPP.\nBy definition, bit rate is a measurement of the rate of information content from the digital video stream. In the case of uncompressed video, bit rate corresponds directly to the quality of the video because bit rate is proportional to every property that affects the video quality. Bit rate is an important property when transmitting video because the transmission link must be capable of supporting that bit rate. Bit rate is also important when dealing with the storage of video because, as shown above, the video size is proportional to the bit rate and the duration. Video compression is used to greatly reduce the bit rate while having little effect on quality.\nBits per pixel (BPP) is a measure of the efficiency of compression. A true-color video with no compression at all may have a BPP of 24 bits/pixel. Chroma subsampling can reduce the BPP to 16 or 12 bits/pixel. Applying JPEG compression on every frame can reduce the BPP to 8 or even 1\u00a0bits/pixel. Applying video compression algorithms like MPEG1, MPEG2 or MPEG4 allows for fractional BPP values to exist.\nConstant bit rate versus variable bit rate.\nBPP represents the \"average\" bits per pixel. There are compression algorithms that keep the BPP almost constant throughout the entire duration of the video. In this case, we also get video output with a constant bitrate (CBR). This CBR video is suitable for real-time, non-buffered, fixed bandwidth video streaming (e.g. in videoconferencing). Since not all frames can be compressed at the same level, because quality is more severely impacted for scenes of high complexity, some algorithms try to constantly adjust the BPP. They keep the BPP high while compressing complex scenes and low for less demanding scenes. This way, it provides the best quality at the smallest average bit rate (and the smallest file size, accordingly). This method produces a variable bitrate because it tracks the variations of the BPP.\nTechnical overview.\nStandard film stocks typically record at 24 frames per second. For video, there are two frame rate standards: NTSC, at 30/1.001 (about 29.97) frames per second (about 59.94 fields per second), and PAL, 25 frames per second (50 fields per second). Digital video cameras come in two different image capture formats: interlaced and progressive scan. Interlaced cameras record the image in alternating sets of lines: the odd-numbered lines are scanned, and then the even-numbered lines are scanned, then the odd-numbered lines are scanned again, and so on.\nOne set of odd or even lines is referred to as a \"field\", and a consecutive pairing of two fields of opposite parity is called a \"frame\". Progressive scan cameras record all lines in each frame as a single unit. Thus, interlaced video captures the scene motion twice as often as progressive video does for the same frame rate. Progressive scan generally produces a slightly sharper image, however, motion may not be as smooth as interlaced video.\nDigital video can be copied with no generation loss; which degrades quality in analog systems. However, a change in parameters like frame size, or a change of the digital format can decrease the quality of the video due to image scaling and transcoding losses. Digital video can be manipulated and edited on non-linear editing systems.\nDigital video has a significantly lower cost than 35\u00a0mm film. In comparison to the high cost of film stock, the digital media used for digital video recording, such as flash memory or hard disk drive is very inexpensive. Digital video also allows footage to be viewed on location without the expensive and time-consuming chemical processing required by film. Network transfer of digital video makes physical deliveries of tapes and film reels unnecessary. \nDigital television (including higher quality HDTV) was introduced in most developed countries in early 2000s. Today, digital video is used in modern mobile phones and video conferencing systems. Digital video is used for Internet distribution of media, including streaming video and peer-to-peer movie distribution.\nMany types of video compression exist for serving digital video over the internet and on optical disks. The file sizes of digital video used for professional editing are generally not practical for these purposes, and the video requires further compression with codecs to be used for recreational purposes.\n, the highest image resolution demonstrated for digital video generation is 132.7 megapixels (15360 x 8640 pixels). The highest speed is attained in industrial and scientific high-speed cameras that are capable of filming 1024x1024 video at up to 1 million frames per second for brief periods of recording.\nTechnical properties.\nLive digital video consumes bandwidth. Recorded digital video consumes data storage. The amount of bandwidth or storage required is determined by the frame size, color depth and frame rate. Each pixel consumes a number of bits determined by the color depth. The data required to represent a frame of data is determined by multiplying by the number of pixels in the image. The bandwidth is determined by multiplying the storage requirement for a frame by the frame rate. The overall storage requirements for a program can then be determined by multiplying bandwidth by the duration of the program.\nThese calculations are accurate for uncompressed video, but due to the relatively high bit rate of uncompressed video, video compression is extensively used. In the case of compressed video, each frame requires only a small percentage of the original bits. This reduces the data or bandwidth consumption by a factor of 5 to 12 times when using lossless compression, but more commonly, lossy compression is used due to its reduction of data consumption by factors of 20 to 200. Note that it is not necessary that all frames are equally compressed by the same percentage. Instead, consider the \"average\" factor of compression for \"all\" the frames taken together.\nInterfaces and cables.\nPurpose-built digital video interfaces\nGeneral-purpose interfaces use to carry digital video\nThe following interface has been designed for carrying MPEG-Transport compressed video:\nCompressed video is also carried using UDP-IP over Ethernet. Two approaches exist for this:\nOther methods of carrying video over IP"}
{"id": "8735", "revid": "37330231", "url": "https://en.wikipedia.org/wiki?curid=8735", "title": "BIND", "text": "BIND () is a suite of software for interacting with the Domain Name System (DNS). Its most prominent component, named (pronounced \"name-dee\": , short for \"name daemon\"), performs both of the main DNS server roles, acting as an authoritative name server for DNS zones and as a recursive resolver in the network. As of 2015, it is the most widely used domain name server software, and is the \"de facto\" standard on Unix-like operating systems. Also contained in the suite are various administration tools such as nsupdate and dig, and a DNS resolver interface library.\nThe software was originally designed at the University of California, Berkeley (UC Berkeley) in the early 1980s. The name originates as an acronym of \"Berkeley Internet Name Domain\", reflecting the application's use within UC Berkeley. The current version is BIND 9, first released in 2000 and still actively maintained by the Internet Systems Consortium (ISC) with new releases issued several times a year.\nKey features.\nBIND 9 is intended to be fully compliant with the IETF DNS standards and draft standards. Important features of BIND 9 include: TSIG, nsupdate, IPv6, RNDC (remote name daemon control), views, multiprocessor support, Response Rate Limiting (RRL), DNSSEC, and broad portability. RNDC enables remote configuration updates, using a shared secret to provide encryption for local and remote terminals during each session.\nDatabase support.\nWhile earlier versions of BIND offered no mechanism to store and retrieve zone data in anything other than flat text files, in 2007 BIND 9.4 DLZ provided a compile-time option for zone storage in a variety of database formats including LDAP, Berkeley DB, PostgreSQL, MySQL, and ODBC.\nBIND 10 planned to make the data store modular, so that a variety of databases may be connected.\nIn 2016 ISC added support for the 'dyndb' interface, contributed by RedHat, with BIND version 9.11.0.\nSecurity.\nSecurity issues that are discovered in BIND 9 are patched and publicly disclosed in keeping with common principles of open source software. A complete list of security defects that have been discovered and disclosed in BIND9 is maintained by Internet Systems Consortium, the current authors of the software.\nThe BIND 4 and BIND 8 releases both had serious security vulnerabilities. Use of these ancient versions, or any un-maintained, non-supported version is strongly discouraged. BIND 9 was a complete rewrite, in part to mitigate these ongoing security issues. The downloads page on the ISC web site clearly shows which versions are currently maintained and which are end of life.\nHistory.\nBIND was originally written by four graduate students at the Computer Systems Research Group (CSRG) at the University of California, Berkeley, Douglas Terry, Mark Painter, David Riggle and Songnian Zhou, in the early 1980s as a result of a DARPA grant. The acronym \"BIND\" is for \"Berkeley Internet Name Domain\", from a technical paper published in 1984. It was first released with Berkeley Software Distribution 4.3BSD.\nVersions of BIND through 4.8.3 were maintained by the CSRG.\nPaul Vixie of Digital Equipment Corporation (DEC) took over BIND development in 1988, releasing versions 4.9 and 4.9.1. Vixie continued to work on BIND after leaving DEC. BIND Version 4.9.2 was sponsored by Vixie Enterprises. Vixie eventually founded the Internet Software Consortium (ISC), which became the entity responsible for BIND versions starting with 4.9.3.\nBIND 8 was released by ISC in May 1997.\nVersion 9 was developed by Nominum, Inc. under an ISC outsourcing contract, and the first version was released 9 October 2000. It was written from scratch in part to address the architectural difficulties with auditing the earlier BIND code bases, and also to support DNSSEC (DNS Security Extensions). The development of BIND 9 took place under a combination of commercial and military contracts. Most of the features of BIND 9 were funded by UNIX vendors who wanted to ensure that BIND stayed competitive with Microsoft's DNS offerings; the DNSSEC features were funded by the US military, which regarded DNS security as important. BIND 9 was released in September 2000.\nIn 2009, ISC started an effort to develop a new version of the software suite, initially called BIND10. In addition to DNS service, the BIND10 suite also included IPv4 and IPv6 DHCP server components. In April 2014, with BIND10 release 1.2.0 the ISC concluded its involvement in the project and renamed it to \"Bundy\", moving the source code repository to GitHub for further development by outside public efforts. ISC discontinued its involvement in the project due to cost-cutting measures. The development of DHCP components was split off to become a new Kea project."}
{"id": "8736", "revid": "14301959", "url": "https://en.wikipedia.org/wiki?curid=8736", "title": "Djbdns", "text": "The djbdns software package is a DNS implementation. It was created by Daniel J. Bernstein in response to his frustrations with repeated security holes in the widely used BIND DNS software. As a challenge, Bernstein offered a $1000 prize for the first person to find a security hole in djbdns, which was awarded in March 2009 to Matthew Dempsky.\n, djbdns's tinydns component was the second most popular DNS server in terms of the number of domains for which it was the authoritative server, and third most popular in terms of the number of DNS hosts running it.\ndjbdns has never been vulnerable to the widespread cache poisoning vulnerability reported in July 2008, but it has been discovered that it is vulnerable to a related attack.\nThe source code has not been centrally managed since its release in 2001, and was released into the public domain in 2007. As of March 2009, there are a number of forks, one of which is dbndns (part of the Debian Project), and more than a dozen patches to modify the released version.\nWhile djbdns does not directly support DNSSEC, there are third party patches to add DNSSEC support to djbdns' authoritative-only tinydns component.\nComponents.\nThe djbdns software consists of servers, clients, and miscellaneous configuration tools.\nDesign.\nIn djbdns, different features and services are split off into separate programs. For example, zone transfers, zone file parsing, caching, and recursive resolving are implemented as separate programs. The result of these design decisions is a reduction in code size and complexity of the daemon program that provides the core function of answering lookup requests. Bernstein asserts that this is true to the spirit of the Unix operating system, and makes security verification much simpler.\nCopyright status.\nOn December 28, 2007, Bernstein released djbdns into the public domain. Previously the package was distributed free of charge as license-free software. However this did not permit the distribution of modified versions of djbdns, which was one of the core principles of open-source software. Consequently, it was not included in those Linux distributions which required all components to be open-source."}
{"id": "8741", "revid": "4246661", "url": "https://en.wikipedia.org/wiki?curid=8741", "title": "Dylan (programming language)", "text": "Dylan is a multi-paradigm programming language that includes support for functional and object-oriented programming (OOP), and is dynamic and reflective while providing a programming model designed to support generating efficient machine code, including fine-grained control over dynamic and static behaviors. It was created in the early 1990s by a group led by Apple Computer.\nDylan derives from Scheme and Common Lisp and adds an integrated object system derived from the Common Lisp Object System (CLOS). In Dylan, all values (including numbers, characters, functions, and classes) are first-class objects. Dylan supports multiple inheritance, polymorphism, multiple dispatch, keyword arguments, object introspection, pattern-based syntax extension macros, and many other advanced features. Programs can express fine-grained control over dynamism, admitting programs that occupy a continuum between dynamic and static programming and supporting evolutionary development (allowing for rapid prototyping followed by incremental refinement and optimization).\nDylan's main design goal is to be a dynamic language well-suited for developing commercial software. Dylan attempts to address potential performance issues by introducing \"natural\" limits to the full flexibility of Lisp systems, allowing the compiler to clearly understand compilable units, such as libraries.\nDylan derives much of its semantics from Scheme and other Lisps; some Dylan implementations were initially built within extant Lisp systems. However, Dylan has an ALGOL-like syntax instead of a Lisp-like prefix syntax.\nHistory.\nDylan was created in the early 1990s by a group led by Apple Computer. At one time in its development, it was intended for use with the Apple Newton computer, but the Dylan implementation did not reach sufficient maturity in time, and Newton instead used a mix of C and the NewtonScript developed by Walter Smith. Apple ended their Dylan development effort in 1995, though they made a \"technology release\" version available (Apple Dylan TR1) that included an advanced integrated development environment (IDE).\nTwo other groups contributed to the design of the language and developed implementations: Harlequin released a commercial IDE for Microsoft Windows and Carnegie Mellon University released an open source compiler for Unix systems called Gwydion Dylan. Both of these implementations are now open source. The Harlequin implementation is now named Open Dylan and is maintained by a group of volunteers, the Dylan Hackers.\nThe Dylan language was code-named Ralph. James Joaquin chose the name Dylan for \"DYnamic LANguage.\"\nSyntax.\nMany of Dylan's syntax features come from its Lisp heritage. Originally, Dylan used a Lisp-like prefix syntax, which was based on s-expressions. By the time the language design was completed, the syntax was changed to an ALGOL-like syntax, with the expectation that it would be more familiar to a wider audience of programmers. The syntax was designed by Michael Kahl. It is described in great detail in the Dylan Reference Manual.\nLexical syntax.\nDylan is not case sensitive. Dylan's lexical syntax allows the use of a naming convention where hyphen (minus) signs are used to connect the parts of multiple-word identifiers (sometimes called \"lisp-case\" or \"kebab case\"). This convention is common in Lisp languages.\nBesides alphanumeric characters and hyphen-minus signs, Dylan allows a variety of non-alphanumeric characters as part of identifiers. Identifiers may not consist of these non-alphanumeric characters alone. If there is any ambiguity, whitespace is used.\nExample code.\nA simple class with several slots:\ndefine class &lt;point&gt; (&lt;object&gt;)\n slot point-x :: &lt;integer&gt;,\n required-init-keyword: x:;\n slot point-y :: &lt;integer&gt;,\n required-init-keyword: y:;\nend class &lt;point&gt;;\nBy convention, classes are named with less-than and greater-than signs used as angle brackets, e.g. the class named codice_1 in the code example.\nIn codice_2 both codice_3 and codice_1 are optional. This is true for all codice_5 clauses. For example, you may write codice_6 or just codice_5 to terminate an codice_8 statement.\nTo make an instance of codice_1:\nmake(&lt;point&gt;, x: 100, y: 200)\nThe same class, rewritten in the most minimal way possible:\ndefine class &lt;point&gt; (&lt;object&gt;)\n slot point-x;\n slot point-y;\nend;\nThe slots are now both typed as codice_10. The slots must be initialized manually:\nlet p = make(&lt;point&gt;);\npoint-x(p) := 100; // or p.point-x := 100;\npoint-y(p) := 200; // or p.point-y := 200;\nBy convention, constant names begin with \"$\":\ndefine constant $pi :: &lt;double-float&gt; = 3.1415927d0;\nA factorial function:\ndefine function factorial (n :: &lt;integer&gt;) =&gt; (n! :: &lt;integer&gt;)\n case\n n &lt; 0 =&gt; error(\"Can't take factorial of negative integer: %d\\n\", n);\n n = 0 =&gt; 1;\n otherwise =&gt; n * factorial(n - 1);\n end\nend;\nHere, codice_11 and codice_12 are just normal identifiers.\nThere is no explicit return statement. The result of a method or function is the last expression evaluated. It is a common style to leave off the semicolon after an expression in return position.\nModules vs. namespace.\nIn many object-oriented languages, classes are the main means of encapsulation and modularity; each class defines a namespace and controls which definitions are externally visible. Further, classes in many languages define an indivisible unit that must be used as a whole. For example, using a codice_13 concatenation function requires importing and compiling against all of codice_13.\nSome languages, including Dylan, also include a separate, explicit namespace or module system that performs encapsulation in a more general way.\nIn Dylan, the concepts of compile-unit and import-unit are separated, and classes have nothing specifically to do with either. A \"library\" defines items that should be compiled and handled together, while a \"module\" defines a namespace. Classes can be placed together in modules, or cut across them, as the programmer wishes. Often the complete definition for a class does not exist in a single module, but is spread across several that are optionally collected together. Different programs can have different definitions of the same class, including only what they need.\nFor example, consider an add-on library for regex support on codice_13. In some languages, for the functionality to be included in strings, the functionality must be added to the codice_13 namespace. As soon as this occurs, the codice_13 class becomes larger, and functions that don't need to use regex still must \"pay\" for it in increased library size. For this reason, these sorts of add-ons are typically placed in their own namespaces and objects. The downside to this approach is that the new functions are no longer a \"part of\" codice_13; instead, it is isolated in its own set of functions that must be called separately. Instead of codice_19, which would be the natural organization from an OO viewpoint, something like codice_20 is used, which effectively reverses the ordering.\nUnder Dylan, many interfaces can be defined for the same code, for instance the String concatenation method could be placed in both the String interface, and the \"concat\" interface which collects together all of the different concatenation functions from various classes. This is more commonly used in math libraries, where functions tend to be applicable to widely differing object types.\nA more practical use of the interface construct is to build public and private versions of a module, something that other languages include as a \"bolt on\" feature that invariably causes problems and adds syntax. Under Dylan, every function call can be simply placed in the \"Private\" or \"Development\" interface, and collect up publicly accessible functions in codice_21. Under Java or C++ the visibility of an object is defined in the code, meaning that to support a similar change, a programmer would be forced to rewrite the definitions fully, and could not have two versions at the same time.\nClasses.\nClasses in Dylan describe codice_22 (data members, fields, ivars, etc.) of objects in a fashion similar to most OO languages. All access to slots is via methods, as in Smalltalk. Default getter and setter methods are automatically generated based on the slot names. In contrast with most other OO languages, other methods applicable to the class are often defined outside of the class, and thus class definitions in Dylan typically include the definition of the storage only. For instance:\ndefine class &lt;window&gt; (&lt;view&gt;)\n slot title :: &lt;string&gt; = \"untitled\", init-keyword: title:;\n slot position :: &lt;point&gt;, required-init-keyword: position:;\nend class;\nIn this example, the class \"codice_23\" is defined. The &lt;class name&gt; syntax is convention only, to make the class names stand out\u2014the angle brackets are merely part of the class name. In contrast, in some languages the convention is to capitalize the first letter of the class name or to prefix the name with a \"C\" or \"T\" (for example). codice_23 inherits from a single class, codice_25, and contains two slots, codice_26 holding a string for the window title, and codice_27 holding an X-Y point for a corner of the window. In this example, the title has been given a default value, while the position has not. The optional \"init-keyword\" syntax allows the programmer to specify the initial value of the slot when instantiating an object of the class.\nIn languages such as C++ or Java, the class would also define its interface. In this case the definition above has no explicit instructions, so in both languages access to the slots and methods is considered codice_28, meaning they can be used only by subclasses. To allow unrelated code to use the window instances, they must be declared codice_29.\nIn Dylan, these sorts of visibility rules are not considered part of the code, but of the module/interface system. This adds considerable flexibility. For instance, one interface used during early development could declare everything public, whereas one used in testing and deployment could limit this. With C++ or Java these changes would require changes to the source code, so people won't do it, whereas in Dylan this is a fully unrelated concept.\nAlthough this example does not use it, Dylan also supports multiple inheritance.\nMethods and generic functions.\nIn Dylan, methods are not intrinsically associated with any specific class; methods can be thought of as existing outside of classes. Like CLOS, Dylan is based on multiple dispatch (multimethods), where the specific method to be called is chosen based on the types of all its arguments. The method need not be known at compile time, the understanding being that the required function may be available, or not, based on a user's preferences.\nUnder Java the same methods would be isolated in a specific class. To use that functionality the programmer is forced to \"import\" that class and refer to it explicitly to call the method. If that class is unavailable, or unknown at compile time, the application simply won't compile.\nIn Dylan, code is isolated from storage in \"functions\". Many classes have methods that call their own functions, thereby looking and feeling like most other OO languages. However code may also be located in \"generic functions\", meaning they are not attached to a specific class, and can be called natively by anyone. Linking a specific generic function to a method in a class is accomplished thusly:\ndefine method turn-blue (w :: &lt;window&gt;)\n w.color := $blue;\nend method;\nThis definition is similar to those in other languages, and would likely be encapsulated within the codice_23 class. Note the := setter call, which is syntactic sugar for codice_31.\nThe utility of generic methods comes into its own when you consider more \"generic\" examples. For instance, one common function in most languages is the codice_32, which returns some human-readable form for the object. For instance, a window might return its title and its position in parens, while a string would return itself. In Dylan these methods could all be collected into a single module called \"codice_32\", thereby removing this code from the definition of the class itself. If a specific object did not support a codice_32, it could be easily added in the codice_32 module.\nExtensibility.\nThis whole concept might strike some readers as very odd. The code to handle codice_32 for a window isn't defined in codice_23? This might not make any sense until you consider how Dylan handles the call of the codice_32. In most languages when the program is compiled the codice_32 for codice_23 is looked up and replaced with a pointer (more or less) to the method. In Dylan this occurs when the program is first run; the runtime builds a table of method-name/parameters details and looks up methods dynamically via this table. That means that a function for a specific method can be located anywhere, not just in the compile-time unit. In the end the programmer is given considerable flexibility in terms of where to place their code, collecting it along class lines where appropriate, and functional lines where it's not.\nThe implication here is that a programmer can add functionality to existing classes by defining functions in a separate file. For instance, you might wish to add spell checking to all codice_41s, which in C++ or Java would require access to the source code of the string class\u2014and such basic classes are rarely given out in source form. In Dylan (and other \"extensible languages\") the spell checking method could be added in the codice_42 module, defining all of the classes on which it can be applied via the codice_43 construct. In this case the actual functionality might be defined in a single generic function, which takes a string and returns the errors. When the codice_42 module is compiled into your program, all strings (and other objects) will get the added functionality.\nApple Dylan.\nApple Dylan is the implementation of Dylan produced by Apple Computer. It was originally developed for the Apple Newton product."}
{"id": "8742", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=8742", "title": "Dublin Core", "text": "The Dublin Core vocabulary, also known as the Dublin Core Metadata Terms (DCMT), is a general purpose metadata vocabulary for describing resources of any type. It was first developed for describing web content in the early days of the World Wide Web. The Dublin Core Metadata Initiative (DCMI) is responsible for maintaining the Dublin \nCore vocabulary.\nInitially developed as fifteen terms in 1998 the set of elements has grown over time and in 2008 was redefined as an Resource Description Framework (RDF) vocabulary.\nDesigned with minimal constraints, each Dublin Core element is optional and may be repeated. There is no prescribed order in Dublin Core for presenting or using the elements.\nEvolution of the Dublin Core vocabulary.\nThe Dublin Core Element Set was a response to concern about accurate finding of resources on the Web, with some early assumptions that this would be a library function. In particular it anticipated a future in which scholarly materials would be searchable on the World Wide Web. Whereas HTML was being used to mark-up the structure of documents, metadata was needed to mark-up the contents of documents. Given the great number of documents on, and soon to be on, the World Wide Web, it was proposed that \"self-identifying\" documents would be necessary.\nTo this end, the Dublin Core Metadata Workshop met beginning in 1995 to develop a vocabulary that could be used to insert consistent metadata into Web documents. Originally defined as 15 metadata elements, the Dublin Core Element Set allowed authors of web pages a vocabulary and method for creating simple metadata for their works. It provided a simple, flat element set that could be used\nQualified Dublin Core was developed in the late 1990s to provide an extension mechanism to the vocabulary of 15 elements. This was a response to communities whose metadata needs required additional detail.\nIn 2012, the \"DCMI Metadata Terms\" was created using a RDF data model. This expanded element set incorporates the original 15 elements and many of the qualifiers of the qualified Dublin Core as RDF properties. The full set of elements is found under the namespace http://purl.org/dc/terms/. There is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.\nDublin Core Metadata Element Set, 1995.\nThe Dublin Core vocabulary published in 1999 consisted of 15 terms:\nThe vocabulary was commonly expressed in HTML 'meta' tagging in the \"&lt;head&gt;\" section of an HTML-encoded page. \n &lt;head&gt;\n &lt;meta name=\"DC.title\" content=\"Services to Government\" &gt;\n &lt;meta name=\"DC.date\" content=\"1997-07\" &gt;\n &lt;/head&gt;\nThe vocabulary could be used in any metadata serialization including key/value pairs and XML.\nQualified Dublin Core, 2000.\nSubsequent to the specification of the original 15 elements, Qualified Dublin Core was developed to provide an extension mechanism to be used when the primary 15 terms were not sufficient. A set of common refinements was provided in the documentation. These schemes include controlled vocabularies and formal notations or parsing rules. Qualified Dublin Core was not limited to these specific refinements, allowing communities to create extended metadata terms to meet their needs.\nThe guiding principle for the qualification of Dublin Core elements, colloquially known as the \"Dumb-Down Principle\", states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.\nQualified Dublin Core added qualifiers to these elements:\nAnd added three elements not in the base 15:\nQualified Dublin Core is often used with a \"dot syntax\", with a period separating the element and the qualifier(s). This is shown in this excerpted example provided by Chan and Hodges:\nTitle: D-Lib Magazine\nTitle.alternative: Digital Library Magazine\nIdentifier.ISSN: 1082-9873\nPublisher: Corporation for National Research Initiatives\nPublisher.place: Reston, VA.\nSubject.topical.LCSH: Digital libraries - Periodicals\nDCMI Metadata Terms, 2008.\nThe DCMI Metadata Terms lists the current set of the Dublin Core vocabulary. This set includes the fifteen terms of the DCMES (in \"italic\"), as well as many of the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as RDF properties.\nIt also includes these RDF classes which are used as domains and ranges of some properties:\nMaintenance of the standard.\nChanges that are made to the Dublin Core standard are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy. This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.\nDublin Core as standards.\nThe Dublin Core Metadata Terms vocabulary has been formally standardized internationally as ISO 15836 by the International Organization for Standardization (ISO) and as IETF RFC 5013 by the Internet Engineering Task Force (IETF), \nas well as in the U.S. as ANSI/NISO Z39.85 by the National Information Standards Organization (NISO).\nSyntax.\nSyntax choices for metadata expressed with the Dublin Core elements depend on context. Dublin Core concepts and semantics are designed to be syntax independent and apply to a variety of contexts, as long as the metadata is in a form suitable for interpretation by both machines and people.\nNotable applications.\nOne Document Type Definition based on Dublin Core is the Open Source Metadata Framework (OMF) specification. OMF is in turn used by Rarian (superseding ScrollKeeper), which is used by the GNOME desktop and KDE help browsers and the ScrollServer documentation server.\nPBCore is also based on Dublin Core. The Zope CMF's Metadata products, used by the Plone, ERP5, the Nuxeo CPS Content management systems, SimpleDL, and Fedora Commons also implement Dublin Core. The EPUB e-book format uses Dublin Core metadata in the OPF file. Qualified Dublin Core is used in the DSpace archival management software.\nThe Australian Government Locator Service (AGLS) metadata standard is an application profile of Dublin Core."}
{"id": "8743", "revid": "1266788627", "url": "https://en.wikipedia.org/wiki?curid=8743", "title": "Document Object Model", "text": "The Document Object Model (DOM) is a cross-platform and language-independent interface that treats an HTML or XML document as a tree structure wherein each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. Nodes can have event handlers (also known as event listeners) attached to them. Once an event is triggered, the event handlers get executed.\nThe principal standardization of the DOM was handled by the World Wide Web Consortium (W3C), which last developed a recommendation in 2004. WHATWG took over the development of the standard, publishing it as a living document. The W3C now publishes stable snapshots of the WHATWG standard.\nIn HTML DOM (Document Object Model), every element is a node:\nHistory.\nThe history of the Document Object Model is intertwined with the history of the \"browser wars\" of the late 1990s between Netscape Navigator and Microsoft Internet Explorer, as well as with that of JavaScript and JScript, the first scripting languages to be widely implemented in the JavaScript engines of web browsers.\nJavaScript was released by Netscape Communications in 1995 within Netscape Navigator 2.0. Netscape's competitor, Microsoft, released Internet Explorer 3.0 the following year with a reimplementation of JavaScript called JScript. JavaScript and JScript let web developers create web pages with client-side interactivity. The limited facilities for detecting user-generated events and modifying the HTML document in the first generation of these languages eventually became known as \"DOM Level 0\" or \"Legacy DOM.\" No independent standard was developed for DOM Level 0, but it was partly described in the specifications for HTML 4.\nLegacy DOM was limited in the kinds of elements that could be accessed. Form, link and image elements could be referenced with a hierarchical name that began with the root document object. A hierarchical name could make use of either the names or the sequential index of the traversed elements. For example, a form input element could be accessed as either codice_1 or codice_2.\nThe Legacy DOM enabled client-side form validation and simple interface interactivity like creating tooltips.\nIn 1997, Netscape and Microsoft released version 4.0 of Netscape Navigator and Internet Explorer respectively, adding support for Dynamic HTML (DHTML) functionality enabling changes to a loaded HTML document. DHTML required extensions to the rudimentary document object that was available in the Legacy DOM implementations. Although the Legacy DOM implementations were largely compatible since JScript was based on JavaScript, the DHTML DOM extensions were developed in parallel by each browser maker and remained incompatible. These versions of the DOM became known as the \"Intermediate DOM\".\nAfter the standardization of ECMAScript, the W3C DOM Working Group began drafting a standard DOM specification. The completed specification, known as \"DOM Level 1\", became a W3C Recommendation in late 1998. By 2005, large parts of W3C DOM were well-supported by common ECMAScript-enabled browsers, including Internet Explorer 6 (from 2001), Opera, Safari and Gecko-based browsers (like Mozilla, Firefox, SeaMonkey and Camino).\nStandards.\nThe W3C DOM Working Group published its final recommendation and subsequently disbanded in 2004. Development efforts migrated to the WHATWG, which continues to maintain a living standard. In 2009, the Web Applications group reorganized DOM activities at the W3C. In 2013, due to a lack of progress and the impending release of HTML5, the DOM Level 4 specification was reassigned to the HTML Working Group to expedite its completion. Meanwhile, in 2015, the Web Applications group was disbanded and DOM stewardship passed to the Web Platform group. Beginning with the publication of DOM Level 4 in 2015, the W3C creates new recommendations based on snapshots of the WHATWG standard.\nApplications.\nWeb browsers.\nTo render a document such as a HTML page, most web browsers use an internal model similar to the DOM. The nodes of every document are organized in a tree structure, called the \"DOM tree\", with the topmost node named as \"Document object\". When an HTML page is rendered in browsers, the browser downloads the HTML into local memory and automatically parses it to display the page on screen. However, the DOM does not necessarily need to be represented as a tree, and some browsers have used other internal models.\nJavaScript.\nWhen a web page is loaded, the browser creates a Document Object Model of the page, which is an object oriented representation of an HTML document that acts as an interface between JavaScript and the document itself. This allows the creation of dynamic web pages, because within a page JavaScript can:\nDOM tree structure.\nA Document Object Model (DOM) tree is a hierarchical representation of an HTML or XML document. It consists of a root node, which is the document itself, and a series of child nodes that represent the elements, attributes, and text content of the document. Each node in the tree has a parent node, except for the root node, and can have multiple child nodes.\nElements as nodes.\nElements in an HTML or XML document are represented as nodes in the DOM tree. Each element node has a tag name and attributes, and can contain other element nodes or text nodes as children. For example, an HTML document with the following structure:\n&lt;html&gt;\n &lt;head&gt;\n &lt;title&gt;My Website&lt;/title&gt;\n &lt;/head&gt;\n &lt;body&gt;\n &lt;h1&gt;Welcome to DOM&lt;/h1&gt;\n &lt;p&gt;This is my website.&lt;/p&gt;\n &lt;/body&gt;\n&lt;/html&gt;\nwill be represented in the DOM tree as:\n- Document (root)\n - html\n - head\n - title\n - \"My Website\"\n - body\n - h1\n - \"Welcome\"\n - p\n - \"This is my website.\"\nText nodes.\nText content within an element is represented as a text node in the DOM tree. Text nodes do not have attributes or child nodes, and are always leaf nodes in the tree. For example, the text content \"My Website\" in the title element and \"Welcome\" in the h1 element in the above example are both represented as text nodes.\nAttributes as properties.\nAttributes of an element are represented as properties of the element node in the DOM tree. For example, an element with the following HTML:\n&lt;a href=\"https://example.com\"&gt;Link&lt;/a&gt;\nwill be represented in the DOM tree as:\n- a\n - href: \"https://example.com\"\n - \"Link\"\nManipulating the DOM tree.\nThe DOM tree can be manipulated using JavaScript or other programming languages. Common tasks include navigating the tree, adding, removing, and modifying nodes, and getting and setting the properties of nodes. The DOM API provides a set of methods and properties to perform these operations, such as codice_3, codice_5, codice_6, and codice_7.\n// Create the root element\nvar root = document.createElement(\"root\");\n// Create a child element\nvar child = document.createElement(\"child\");\n// Add the child element to the root element\nroot.appendChild(child);\nAnother way to create a DOM structure is using the innerHTML property to insert HTML code as a string, creating the elements and children in the process. For example:\ndocument.getElementById(\"root\").innerHTML = \"&lt;child&gt;&lt;/child&gt;\";\nAnother method is to use a JavaScript library or framework such as jQuery, AngularJS, React, Vue.js, etc. These libraries provide a more convenient, eloquent and efficient way to create, manipulate and interact with the DOM.\nIt is also possible to create a DOM structure from an XML or JSON data, using JavaScript methods to parse the data and create the nodes accordingly.\nCreating a DOM structure does not necessarily mean that it will be displayed in the web page, it only exists in memory and should be appended to the document body or a specific container to be rendered.\nIn summary, creating a DOM structure involves creating individual nodes and organizing them in a hierarchical structure using JavaScript or other programming languages, and it can be done using several methods depending on the use case and the developer's preference.\nImplementations.\nBecause the DOM supports navigation in any direction (e.g., parent and previous sibling) and allows for arbitrary modifications, implementations typically buffer the document. However, a DOM need not originate in a serialized document at all, but can be created in place with the DOM API. And even before the idea of the DOM originated, there were implementations of equivalent structure with persistent disk representation and rapid access, for example DynaText's model disclosed in and various database approaches.\nLayout engines.\nWeb browsers rely on layout engines to parse HTML into a DOM. Some layout engines, such as Trident/MSHTML, are associated primarily or exclusively with a particular browser, such as Internet Explorer. Others, including Blink, WebKit, and Gecko, are shared by a number of browsers, such as Google Chrome, Opera, Safari, and Firefox. The different layout engines implement the DOM standards to varying degrees of compliance.\nLibraries.\nDOM implementations:\nAPIs that expose DOM implementations:\nInspection tools:"}
{"id": "8745", "revid": "43301693", "url": "https://en.wikipedia.org/wiki?curid=8745", "title": "Design pattern", "text": "A design pattern is the re-usable form of a solution to a design problem. The idea was introduced by the architect Christopher Alexander and has been adapted for various other disciplines, particularly software engineering.\nDetails.\nAn organized collection of design patterns that relate to a particular field is called a pattern language. This language gives a common terminology for discussing the situations designers are faced with.\nDocumenting a pattern requires explaining why a particular situation causes problems, and how the components of the pattern relate to each other to give the solution. Christopher Alexander describes common design problems as arising from \"conflicting forces\"\u2014such as the conflict between wanting a room to be sunny and wanting it not to overheat on summer afternoons. A pattern would not tell the designer how many windows to put in the room; instead, it would propose a set of values to guide the designer toward a decision that is best for their particular application. Alexander, for example, suggests that enough windows should be included to direct light all around the room. He considers this a good solution because he believes it increases the enjoyment of the room by its occupants. Other authors might come to different conclusions, if they place higher value on heating costs, or material costs. These values, used by the pattern's author to determine which solution is \"best\", must also be documented within the pattern.\nPattern documentation should also explain when it is applicable. Since two houses may be very different from one another, a design pattern for houses must be broad enough to apply to both of them, but not so vague that it doesn't help the designer make decisions. The range of situations in which a pattern can be used is called its context. Some examples might be \"all houses\", \"all two-story houses\", or \"all places where people spend time\".\nFor instance, in Christopher Alexander's work, bus stops and waiting rooms in a surgery center are both within the context for the pattern \"A PLACE TO WAIT\".\nExamples.\nBusiness models also have design patterns. See ."}
{"id": "8748", "revid": "21678789", "url": "https://en.wikipedia.org/wiki?curid=8748", "title": "N,N-Dimethyltryptamine", "text": "N\",\"N\"-Dimethyltryptamine (DMT or N\",\"N\"-DMT) is a substituted tryptamine that occurs in many plants and animals, including humans, and which is both a derivative and a structural analog of tryptamine. DMT is used as a psychedelic drug and prepared by various cultures for ritual purposes as an entheogen. \nDMT has a rapid onset, intense effects, and a relatively short duration of action. For those reasons, DMT was known as the \"businessman's trip\" during the 1960s in the United States, as a user could access the full depth of a psychedelic experience in considerably less time than with other substances such as LSD or psilocybin mushrooms. DMT can be inhaled, ingested, or injected and its effects depend on the dose, as well as the mode of administration. When inhaled or injected, the effects last about five to fifteen minutes. Effects can last three hours or more when orally ingested along with a monoamine oxidase inhibitor (MAOI), such as the ayahuasca brew of many native Amazonian tribes. DMT can produce vivid \"projections\" of mystical experiences involving euphoria and dynamic pseudohallucinations of geometric forms.\nDMT is a functional analog and structural analog of other psychedelic tryptamines such as \"O\"-acetylpsilocin (4-AcO-DMT), psilocybin (4-PO-DMT), psilocin (4-HO-DMT), NB-DMT, \"O\"-methylbufotenin (5-MeO-DMT), and bufotenin (5-HO-DMT). Parts of the structure of DMT occur within some important biomolecules like serotonin and melatonin, making them structural analogs of DMT.\nHuman consumption.\nDMT is produced in many species of plants often in conjunction with its close chemical relatives 5-methoxy-\"N\",\"N\"-dimethyltryptamine (5-MeO-DMT) and bufotenin (5-OH-DMT). DMT-containing plants are commonly used in indigenous Amazonian shamanic practices. It is usually one of the main active constituents of the drink ayahuasca; however, ayahuasca is sometimes brewed with plants that do not produce DMT. It occurs as the primary psychoactive alkaloid in several plants including \"Mimosa tenuiflora\", \"Diplopterys cabrerana\", and \"Psychotria viridis\". DMT is found as a minor alkaloid in snuff made from \"Virola\" bark resin in which 5-MeO-DMT is the main active alkaloid. DMT is also found as a minor alkaloid in bark, pods, and beans of \"Anadenanthera peregrina\" and \"Anadenanthera colubrina\" used to make Yopo and Vilca snuff, in which bufotenin is the main active alkaloid. Psilocin and psilocybin, the main psychoactive compounds in psilocybin mushrooms, are structurally similar to DMT.\nThe psychotropic effects of DMT were first studied scientifically by the Hungarian chemist and psychologist Stephen Sz\u00e1ra, who performed research with volunteers in the mid-1950s. Sz\u00e1ra, who later worked for the United States National Institutes of Health, had turned his attention to DMT after his order for LSD from the Swiss company Sandoz Laboratories was rejected on the grounds that the powerful psychotropic could be dangerous in the hands of a communist country.\nDMT is generally not active orally unless it is combined with a monoamine oxidase inhibitor such as a reversible inhibitor of monoamine oxidase A (RIMA), for example, harmaline. Without a MAOI, the body quickly metabolizes orally administered DMT, and it therefore has no hallucinogenic effect unless the dose exceeds the body's monoamine oxidase's metabolic capacity. Other means of consumption such as vaporizing, injecting, or insufflating the drug can produce powerful hallucinations for a short time (usually less than half an hour), as the DMT reaches the brain before it can be metabolized by the body's natural monoamine oxidase. Taking an MAOI prior to vaporizing or injecting DMT prolongs and enhances the effects.\nClinical use research.\nExisting research on clinical use of DMT mostly focuses on its effects when exogenously administered as a drug. Although the scientific consensus is that DMT is a naturally occurring molecule in humans, the effects of endogenous DMT in humans (and more broadly in mammals) is still not well understood. \nDimethyltryptamine (DMT), an endogenous ligand of sigma-1 receptors (Sig-1Rs), acts against systemic hypoxia. Research demonstrates DMT reduces the number of apoptotic and ferroptotic cells in mammalian forebrain and supports astrocyte survival in an ischemic environment. According to these data, DMT may be considered as adjuvant pharmacological therapy in the management of acute cerebral ischemia.\nDMT is studied as a potential treatment for Parkinson's disease in a Phase 1/2 clinical trial.\nSPL026 (DMT fumarate) is currently undergoing phase II clinical trials investigating its use alongside supportive psychotherapy as a potential treatment for major depressive disorder. Additionally, a safety study is underway to investigate the effects of combining SSRIs with SPL026.\nNeuropharmacology.\nRecently, researchers discovered that \"N\",\"N\"-dimethyltryptamine is a potent psychoplastogen, a compound capable of promoting rapid and sustained neuroplasticity that may have wide-ranging therapeutic benefit. \nQuantities of dimethyltryptamine and \"O\"-methylbufotenin were found present in the cerebrospinal fluid of humans in a psychiatric study.\nEffects.\nSubjective psychedelic experiences.\nSubjective experiences of DMT includes profound time-dilatory, visual, auditory, tactile, and proprioceptive distortions and hallucinations, and other experiences that, by most firsthand accounts, defy verbal or visual description. Examples include perceiving hyperbolic geometry or seeing Escher-like impossible objects.\nSeveral scientific experimental studies have tried to measure subjective experiences of altered states of consciousness induced by drugs under highly controlled and safe conditions.\nRick Strassman and his colleagues conducted a five-year-long DMT study at the University of New Mexico in the 1990s. The results provided insight about the quality of subjective psychedelic experiences. In this study participants received the DMT dosage via intravenous injection and the findings suggested that different psychedelic experiences can occur, depending on the level of dosage. Lower doses (0.01 and 0.05\u00a0mg/kg) produced some aesthetic and emotional responses, but not hallucinogenic experiences (e.g., 0.05\u00a0mg/kg had mild mood elevating and calming properties). In contrast, responses produced by higher doses (0.2 and 0.4\u00a0mg/kg) researchers labeled as \"hallucinogenic\" that elicited \"intensely colored, rapidly moving display of visual images, formed, abstract or both\". Comparing to other sensory modalities, the most affected was the visual. Participants reported visual hallucinations, fewer auditory hallucinations and specific physical sensations progressing to a sense of bodily dissociation, as well as experiences of euphoria, calm, fear, and anxiety. These dose-dependent effects match well with anonymously posted \"trip reports\" online, where users report \"breakthroughs\" above certain doses.\nStrassman also highlighted the importance of the context where the drug has been taken. He claimed that DMT has no beneficial effects of itself, rather the context when and where people take it plays an important role.\nIt appears that DMT can induce a state or feeling wherein the person believes to \"communicate with other intelligent lifeforms\" (see \"machine elves\"). High doses of DMT produce a state that involves a sense of \"another intelligence\" that people sometimes describe as \"super-intelligent\", but \"emotionally detached\".\nA 1995 study by Adolf Dittrich and Daniel Lamparter found that the DMT-induced altered state of consciousness (ASC) is strongly influenced by habitual rather than situative factors. In the study, researchers used three dimensions of the APZ questionnaire to examine ASC. The first dimension, oceanic boundlessness (OB), refers to dissolution of ego boundaries and is mostly associated with positive emotions. The second dimension, anxious ego-dissolution (AED), represents a disordering of thoughts and decreases in autonomy and self-control. Last, visionary restructuralization (VR) refers to auditory/visual illusions and hallucinations. Results showed strong effects within the first and third dimensions for all conditions, especially with DMT, and suggested strong intrastability of elicited reactions independently of the condition for the OB and VR scales.\nReported encounters with external entities.\nEntities perceived during DMT inebriation have been represented in diverse forms of psychedelic art. The term \"machine elf\" was coined by ethnobotanist Terence McKenna for the entities he encountered in DMT \"hyperspace\", also using terms like \"fractal elves\", or \"self-transforming machine elves\". McKenna first encountered the \"machine elves\" after smoking DMT in Berkeley in 1965. His subsequent speculations regarding the hyperdimensional space in which they were encountered have inspired a great many artists and musicians, and the meaning of DMT entities has been a subject of considerable debate among participants in a networked cultural underground, enthused by McKenna's effusive accounts of DMT hyperspace. Cliff Pickover has also written about the \"machine elf\" experience, in the book \"Sex, Drugs, Einstein, &amp; Elves\". Strassman noted similarities between self-reports of his DMT study participants' encounters with these \"entities\", and mythological descriptions of figures such as \u1e24ayyot haq-Qodesh in ancient religions, including both angels and demons. Strassman also argues for a similarity in his study participants' descriptions of mechanized wheels, gears and machinery in these encounters, with those described in visions of encounters with the Living Creatures and Ophanim of the Hebrew Bible, noting they may stem from a common neuropsychopharmacological experience.\nStrassman argues that the more positive of the \"external entities\" encountered in DMT experiences should be understood as analogous to certain forms of angels: \nStrassman's experimental participants also note that some other entities can subjectively resemble creatures more like insects and aliens. As a result, Strassman writes these experiences among his experimental participants \"also left me feeling confused and concerned about where the spirit molecule was leading us. It was at this point that I began to wonder if I was getting in over my head with this research.\"\nHallucinations of strange creatures had been reported by Stephen Szara in a 1958 study in psychotic patients, in which he described how one of his subjects under the influence of DMT had experienced \"strange creatures, dwarves or something\" at the beginning of a DMT trip.\nOther researchers of the entities seemingly encountered by DMT users describe them as \"entities\" or \"beings\" in humanoid as well as animal form, with descriptions of \"little people\" being common (non-human gnomes, elves, imps, etc.). Strassman and others have speculated that this form of hallucination may be the cause of alien abduction and extraterrestrial encounter experiences, which may occur through endogenously-occurring DMT.\nLikening them to descriptions of rattling and chattering auditory phenomena described in encounters with the Hayyoth in the Book of Ezekiel, Rick Strassman notes that participants in his studies, when reporting encounters with the alleged entities, have also described loud auditory hallucinations, such as one subject reporting typically \"the elves laughing or talking at high volume, chattering, twittering\".\nNear-death experience.\nA 2018 study found significant relationships between a DMT experience and a near-death experience (NDE). A 2019 large-scale study pointed that ketamine, \"Salvia divinorum\", and DMT (and other classical psychedelic substances) may be linked to near-death experiences due to the semantic similarity of reports associated with the use of psychoactive compounds and NDE narratives, but the study concluded that with the current data it is neither possible to corroborate nor refute the hypothesis that the release of an endogenous ketamine-like neuroprotective agent underlies NDE phenomenology.\nPhysiological response.\nAccording to a dose-response study in human subjects, dimethyltryptamine administered intravenously slightly elevated blood pressure, heart rate, pupil diameter, and rectal temperature, in addition to elevating blood concentrations of \"beta\"-endorphin, corticotropin, cortisol, and prolactin; growth hormone blood levels rise equally in response to all doses of DMT, and melatonin levels were unaffected.\"\nConjecture regarding endogenous production and effects.\nIn the 1950s, the endogenous production of psychoactive agents was considered to be a potential explanation for the hallucinatory symptoms of some psychiatric diseases; this is known as the transmethylation hypothesis. Several speculative and yet untested hypotheses suggest that endogenous DMT is produced in the human brain and is involved in certain psychological and neurological states. DMT is naturally occurring in small amounts in rat brains, human cerebrospinal fluid, and other tissues of humans and other mammals. Further, mRNA for the enzyme necessary for the production of DMT, INMT, are expressed in the human cerebral cortex, choroid plexus, and pineal gland, suggesting an endogenous role in the human brain. In 2011, Nicholas Cozzi of the University of Wisconsin School of Medicine and Public Health, and three other researchers, concluded that INMT, an enzyme that is associated with the biosynthesis of DMT and endogenous hallucinogens is present in the non-human primate (rhesus macaque) pineal gland, retinal ganglion neurons, and spinal cord. Neurobiologist Andrew Gallimore (2013) suggested that while DMT might not have a modern neural function, it may have been an ancestral neuromodulator once secreted in psychedelic concentrations during REM sleep, a function now lost.\nAdverse effects.\nAcute adverse psychological reaction.\nDMT may trigger psychological reactions, known colloquially as a \"bad trip\", such as intense fear, paranoia, anxiety, panic attacks, and substance-induced psychosis, particularly in predisposed individuals.\nAddiction and dependence liability.\nDMT, like other serotonergic psychedelics, is considered to be non-addictive with low abuse potential. A study examining substance use disorder for DSM-IV reported that almost no hallucinogens produced dependence, unlike psychoactive drugs of other classes such as stimulants and depressants. At present, there have been no studies that report drug withdrawal syndrome with termination of DMT, and dependence potential of DMT and the risk of sustained psychological disturbance may be minimal when used infrequently; however, the physiological dependence potential of DMT and ayahuasca has not yet been documented convincingly.\nTolerance.\nUnlike other classical psychedelics, tolerance does not seem to develop to the subjective effects of DMT. Studies report that DMT did not exhibit tolerance upon repeated administration of twice a day sessions, separated by 5hours, for 5consecutive days; field reports suggests a refractory period of only 15 to 30minutes, while the plasma levels of DMT was nearly undetectable 30minutes after intravenous administration. Another study of four closely spaced DMT infusion sessions with 30minute intervals also suggests no tolerance buildup to the psychological effects of the compound, while heart rate responses and neuroendocrine effects were diminished with repeated administration. Similarly to DMT by itself, tolerance does not appear to develop to ayahuasca. A fully hallucinogenic dose of DMT did not demonstrate cross-tolerance to human subjects who are highly tolerant to LSD; hence, research suggests that DMT exhibits unique pharmacological properties compared to other classical psychedelics.\nLong-term use.\nThere have been no serious adverse effects reported on long-term use of DMT, apart from acute cardiovascular events. Repeated and one-time administration of DMT produces marked changes in the cardiovascular system, with an increase in systolic and diastolic blood pressure; although the changes were not statistically significant, a robust trend towards significance was observed for systolic blood pressure at high doses.\nDrug-interactions.\nDMT is inactive when ingested orally due to metabolism by MAO, and DMT-containing drinks such as ayahuasca have been found to contain MAOIs, in particular, harmine and harmaline. Life-threatening lethalities such as serotonin syndrome (SS) may occur when MAOIs are combined with certain serotonergic medications such as SSRI antidepressants. Serotonin syndrome has also been reported with tricyclic antidepressants, opiates, analgesic, and antimigraine drugs; it is advised to exercise caution when an individual had used dextromethorphan (DXM), MDMA, ginseng, or \"St. John's wort\" recently.\nChronic use of SSRIs, TCAs, and MAOIs diminish subjective effects of psychedelics due to presumed SSRI-induced 5-HT2A receptors downregulation and MAOI-induced 5-HT2A receptor desensitization. The interaction between psychedelics and antipsychotics and anticonvulsant are not well documented, however reports reveal that co-use of psychedelics with mood stabilizers such as lithium may provoke seizure and dissociative effects in individuals with bipolar disorder.\nRoutes of administration.\nInhalation.\nA standard dose for vaporized DMT is 20\u201360\u00a0milligrams, depending highly on the efficiency of vaporization as well as body weight and personal variation. In general, this is inhaled in a few successive breaths, but lower doses can be used if the user can inhale it in fewer breaths (ideally one). The effects last for a short period of time, usually 5 to 15 minutes, dependent on the dose. The onset after inhalation is very fast (less than 45 seconds) and peak effects are reached within a minute. In the 1960s, DMT was known as a \"businessman's trip\" in the US because of the relatively short duration (and rapid onset) of action when inhaled. DMT can be inhaled using a bong, typically when sandwiched between layers of plant matter, using a specially designed pipe, or by using an e-cigarette once it has been dissolved in propylene glycol and/or vegetable glycerin. Some users have also started using vaporizers meant for cannabis extracts (\"wax pens\") for ease of temperature control when vaporizing crystals. A DMT-infused smoking blend is called Changa, and is typically used in pipes or other utensils meant for smoking dried plant matter.\nIntravenous injection.\nIn a study conducted from 1990 through 1995, University of New Mexico psychiatrist Rick Strassman found that some volunteers injected with high doses of DMT reported experiences with perceived alien entities. Usually, the reported entities were experienced as the inhabitants of a perceived independent reality that the subjects reported visiting while under the influence of DMT.\nIn 2023, a study investigated a novel method of DMT administration involving a bolus injection paired with a constant-rate infusion, with the goal of extending the DMT experience.\nOral.\nDMT is broken down by the enzyme monoamine oxidase through a process called deamination, and is quickly inactivated orally unless combined with a monoamine oxidase inhibitor (MAOI). The traditional South American beverage ayahuasca is derived by boiling \"Banisteriopsis caapi\" with leaves of one or more plants containing DMT, such as \"Psychotria viridis\", \"Psychotria carthagenensis\", or \"Diplopterys cabrerana\". The \"Banisteriopsis caapi\" contains harmala alkaloids, a highly active reversible inhibitor of monoamine oxidase A (RIMAs), rendering the DMT orally active by protecting it from deamination. A variety of different recipes are used to make the brew depending on the purpose of the ayahuasca session, or local availability of ingredients. Two common sources of DMT in the western US are reed canary grass (\"Phalaris arundinacea\") and Harding grass (\"Phalaris aquatica\"). These invasive grasses contain low levels of DMT and other alkaloids but also contain gramine, which is toxic and difficult to separate. In addition, Jurema (\"Mimosa tenuiflora\") shows evidence of DMT content: the pink layer in the inner rootbark of this small tree contains a high concentration of \"N\",\"N\"-DMT.\nTaken orally with an RIMA, DMT produces a long-lasting (over three hours), slow, deep metaphysical experience similar to that of psilocybin mushrooms, but more intense.\nThe intensity of orally administered DMT depends on the type and dose of MAOI administered alongside it. When ingested with 120\u00a0mg of harmine (a RIMA and member of the harmala alkaloids), 20\u00a0mg of DMT was reported to have psychoactive effects by author and ethnobotanist Jonathan Ott. Ott reported that to produce a visionary state, the threshold oral dose was 30\u00a0mg DMT alongside 120\u00a0mg harmine. This is not necessarily indicative of a standard dose, as dose-dependent effects may vary due to individual variations in drug metabolism.\nHistory.\nNaturally occurring substances (of both vegetable and animal origin) containing DMT have been used in South America since pre-Columbian times.\nDMT was first synthesized in 1931 by Canadian chemist Richard Helmuth Fredrick Manske. In general, its discovery as a natural product is credited to Brazilian chemist and microbiologist Oswaldo Gon\u00e7alves de Lima, who isolated an alkaloid he named \"nigerina\" (nigerine) from the root bark of \"Mimosa tenuiflora\" in 1946. However, in a careful review of the case Jonathan Ott shows that the empirical formula for nigerine determined by Gon\u00e7alves de Lima, which notably contains an atom of oxygen, can match only a partial, \"impure\" or \"contaminated\" form of DMT. It was only in 1959, when Gon\u00e7alves de Lima provided American chemists a sample of \"Mimosa tenuiflora\" roots, that DMT was unequivocally identified in this plant material. Less ambiguous is the case of isolation and formal identification of DMT in 1955 in seeds and pods of \"Anadenanthera peregrina\" by a team of American chemists led by Evan Horning (1916\u20131993). Since 1955, DMT has been found in a number of organisms: in at least fifty plant species belonging to ten families, and in at least four animal species, including one gorgonian and three mammalian species (including humans).\nIn terms of a scientific understanding, the hallucinogenic properties of DMT were not uncovered until 1956 by Hungarian chemist and psychiatrist Stephen Szara. In his paper \"Dimethyltryptamin: Its Metabolism in Man; the Relation of its Psychotic Effect to the Serotonin Metabolism\", Szara employed synthetic DMT, synthesized by the method of Speeter and Anthony, which was then administered to 20 volunteers by intramuscular injection. Urine samples were collected from these volunteers for the identification of DMT metabolites. This is considered to be the converging link between the chemical structure DMT to its cultural consumption as a psychoactive and religious sacrament.\nAnother historical milestone is the discovery of DMT in plants frequently used by Amazonian natives as additive to the vine \"Banisteriopsis caapi\" to make ayahuasca decoctions. In 1957, American chemists Francis Hochstein and Anita Paradies identified DMT in an \"aqueous extract\" of leaves of a plant they named \"Prestonia amazonicum\" [\"sic\"] and described as \"commonly mixed\" with \"B. caapi\". The lack of a proper botanical identification of \"Prestonia amazonica\" in this study led American ethnobotanist Richard Evans Schultes (1915\u20132001) and other scientists to raise serious doubts about the claimed plant identity. The mistake likely led the writer William Burroughs to regard the DMT he experimented with in Tangier in 1961 as \"Prestonia\". Better evidence was produced in 1965 by French pharmacologist Jacques Poisson, who isolated DMT as a sole alkaloid from leaves, provided and used by Aguaruna Indians, identified as having come from the vine \"Diplopterys cabrerana\" (then known as \"Banisteriopsis rusbyana\"). Published in 1970, the first identification of DMT in the plant \"Psychotria viridis\", another common additive of ayahuasca, was made by a team of American researchers led by pharmacologist Ara der Marderosian. Not only did they detect DMT in leaves of \"P. viridis\" obtained from Kaxinaw\u00e1 indigenous people, but they also were the first to identify it in a sample of an ayahuasca decoction, prepared by the same indigenous people.\nChemistry.\nAppearance and form.\nDMT is commonly handled and stored as a hemifumarate, as other DMT acid salts are extremely hygroscopic and will not readily crystallize. Its freebase form, although less stable than DMT hemifumarate, is favored by recreational users choosing to vaporize the chemical as it has a lower boiling point.\nDMT is a lipophilic compound, with an experimental log P of 2.57.\nSynthesis.\nBiosynthesis.\nDimethyltryptamine is an indole alkaloid derived from the shikimate pathway. Its biosynthesis is relatively simple and summarized in the adjacent picture. In plants, the parent amino acid -tryptophan is produced endogenously where in animals -tryptophan is an essential amino acid coming from diet. No matter the source of -tryptophan, the biosynthesis begins with its decarboxylation by an aromatic amino acid decarboxylase (AADC) enzyme (step 1). The resulting decarboxylated tryptophan analog is tryptamine. Tryptamine then undergoes a transmethylation (step 2): the enzyme indolethylamine-\"N\"-methyltransferase (INMT) catalyzes the transfer of a methyl group from cofactor \"S\"-adenosylmethionine (SAM), via nucleophilic attack, to tryptamine. This reaction transforms SAM into \"S\"-adenosylhomocysteine (SAH), and gives the intermediate product \"N\"-methyltryptamine (NMT). NMT is in turn transmethylated by the same process (step 3) to form the end product \"N\",\"N\"-dimethyltryptamine. Tryptamine transmethylation is regulated by two products of the reaction: SAH, and DMT were shown \"ex vivo\" to be among the most potent inhibitors of rabbit INMT activity.\nThis transmethylation mechanism has been repeatedly and consistently proven by radiolabeling of SAM methyl group with carbon-14 ((14C-CH3)SAM).\nLaboratory synthesis.\nDMT can be synthesized through several possible pathways from different starting materials. The two most commonly encountered synthetic routes are through the reaction of indole with oxalyl chloride followed by reaction with dimethylamine and reduction of the carbonyl functionalities with lithium aluminium hydride to form DMT. The second commonly encountered route is through the \"N\",\"N\"-dimethylation of tryptamine using formaldehyde followed by reduction with sodium cyanoborohydride or sodium triacetoxyborohydride. Sodium borohydride can be used but requires a larger excess of reagents and lower temperatures due to it having a higher selectivity for carbonyl groups as opposed to imines. Procedures using sodium cyanoborohydride and sodium triacetoxyborohydride (presumably created \"in situ\" from cyanoborohydride though this may not be the case due to the presence of water or methanol) also result in the creation of cyanated tryptamine and \"beta\"-carboline byproducts of unknown toxicity while using sodium borohydride in absence of acid does not. Bufotenine, a plant extract, can also be synthesized into DMT. \nAlternatively, an excess of methyl iodide or methyl \"p\"-toluenesulfonate and sodium carbonate can be used to over-methylate tryptamine, resulting in the creation of a quaternary ammonium salt, which is then dequaternized (demethylated) in ethanolamine to yield DMT. The same two-step procedure is used to synthesize other \"N\",\"N\"-dimethylated compounds, such as 5-MeO-DMT.\nClandestine manufacture.\nIn a clandestine setting, DMT is not typically synthesized due to the lack of availability of the starting materials, namely tryptamine and oxalyl chloride. Instead, it is more often extracted from plant sources using a nonpolar hydrocarbon solvent such as naphtha or heptane, and a base such as sodium hydroxide.\nAlternatively, an acid\u2013base extraction is sometimes used instead.\nA variety of plants contain DMT at sufficient levels for being viable sources, but specific plants such as \"Mimosa tenuiflora, Acacia acuminata\" and \"Acacia confusa\" are most often used.\nThe chemicals involved in the extraction are commonly available. The plant material may be illegal to procure in some countries. The end product (DMT) is illegal in most countries.\nEvidence in mammals.\nPublished in \"Science\" in 1961, Julius Axelrod found an \"N\"-methyltransferase enzyme capable of mediating biotransformation of tryptamine into DMT in a rabbit's lung. This finding initiated a still ongoing scientific interest in endogenous DMT production in humans and other mammals. From then on, two major complementary lines of evidence have been investigated: localization and further characterization of the \"N\"-methyltransferase enzyme, and analytical studies looking for endogenously produced DMT in body fluids and tissues.\nIn 2013, researchers reported DMT in the pineal gland microdialysate of rodents.\nA study published in 2014 reported the biosynthesis of \"N\",\"N\"-dimethyltryptamine (DMT) in the human melanoma cell line SK-Mel-147 including details on its metabolism by peroxidases.\nIt is assumed that more than half of the amount of DMT produced by the acidophilic cells of the pineal gland is secreted before and during death, the amount being 2.5\u20133.4\u00a0mg/kg. However, this claim by Strassman has been criticized by David Nichols who notes that DMT does not appear to be produced in any meaningful amount by the pineal gland. Removal or calcification of the pineal gland does not induce any of the symptoms caused by removal of DMT. The symptoms presented are consistent solely with reduction in melatonin, which is the pineal gland's known function. Nichols instead suggests that dynorphin and other endorphins are responsible for the reported euphoria experienced by patients during a near-death experience.\nIn 2014, researchers demonstrated the immunomodulatory potential of DMT and 5-MeO-DMT through the Sigma-1 receptor of human immune cells. This immunomodulatory activity may contribute to significant anti-inflammatory effects and tissue regeneration.\nEndogenous DMT.\n\"N\",\"N\"-Dimethyltryptamine (DMT), a psychedelic compound identified endogenously in mammals, is biosynthesized by aromatic -amino acid decarboxylase (AADC) and indolethylamine-\"N\"-methyltransferase (INMT). Studies have investigated brain expression of INMT transcript in rats and humans, coexpression of INMT and AADC mRNA in rat brain and periphery, and brain concentrations of DMT in rats. INMT transcripts were identified in the cerebral cortex, pineal gland, and choroid plexus of both rats and humans via \"in situ\" hybridization. Notably, INMT mRNA was colocalized with AADC transcript in rat brain tissues, in contrast to rat peripheral tissues where there existed little overlapping expression of INMT with AADC transcripts. Additionally, extracellular concentrations of DMT in the cerebral cortex of normal behaving rats, with or without the pineal gland, were similar to those of canonical monoamine neurotransmitters including serotonin. A significant increase of DMT levels in the rat visual cortex was observed following induction of experimental cardiac arrest, a finding independent of an intact pineal gland. These results show for the first time that the rat brain is capable of synthesizing and releasing DMT at concentrations comparable to known monoamine neurotransmitters and raise the possibility that this phenomenon may occur similarly in human brains.\nThe first claimed detection of endogenous DMT in mammals was published in June 1965: German researchers F. Franzen and H. Gross report to have evidenced and quantified DMT, along with its structural analog bufotenin (5-HO-DMT), in human blood and urine. In an article published four months later, the method used in their study was strongly criticized, and the credibility of their results challenged.\nFew of the analytical methods used prior to 2001 to measure levels of endogenously formed DMT had enough sensitivity and selectivity to produce reliable results. Gas chromatography, preferably coupled to mass spectrometry (GC-MS), is considered a minimum requirement. A study published in 2005 implements the most sensitive and selective method ever used to measure endogenous DMT: liquid chromatography\u2013tandem mass spectrometry with electrospray ionization (LC-ESI-MS/MS) allows for reaching limits of detection (LODs) 12 to 200 fold lower than those attained by the best methods employed in the 1970s. The data summarized in the table below are from studies conforming to the abovementioned requirements (abbreviations used: CSF = cerebrospinal fluid; LOD = limit of detection; \"n\" = number of samples; ng/L and ng/kg = nanograms (10\u22129 g) per litre, and nanograms per kilogram, respectively):\nA 2013 study found DMT in microdialysate obtained from a rat's pineal gland, providing evidence of endogenous DMT in the mammalian brain. In 2019 experiments showed that the rat brain is capable of synthesizing and releasing DMT. These results raise the possibility that this phenomenon may occur similarly in human brains.\nDetection in human body fluids.\nDMT may be measured in blood, plasma or urine using chromatographic techniques as a diagnostic tool in clinical poisoning situations or to aid in the medicolegal investigation of suspicious deaths. In general, blood or plasma DMT levels in recreational users of the drug are in the 10\u201330 \u03bcg/L range during the first several hours post-ingestion. Less than 0.1% of an oral dose is eliminated unchanged in the 24-hour urine of humans.\nINMT.\nBefore techniques of molecular biology were used to localize indolethylamine \"N\"-methyltransferase (INMT), characterization and localization went on a par: samples of the biological material where INMT is hypothesized to be active are subject to enzyme assay. Those enzyme assays are performed either with a radiolabeled methyl donor like (14C-CH3)SAM to which known amounts of unlabeled substrates like tryptamine are added or with addition of a radiolabeled substrate like (14C)NMT to demonstrate in vivo formation. As qualitative determination of the radioactively tagged product of the enzymatic reaction is sufficient to characterize INMT existence and activity (or lack of), analytical methods used in INMT assays are not required to be as sensitive as those needed to directly detect and quantify the minute amounts of endogenously formed DMT. The essentially qualitative method thin layer chromatography (TLC) was thus used in a vast majority of studies. Also, robust evidence that INMT can catalyze transmethylation of tryptamine into NMT and DMT could be provided with reverse isotope dilution analysis coupled to mass spectrometry for rabbit and human lung during the early 1970s.\nSelectivity rather than sensitivity proved to be a challenge for some TLC methods with the discovery in 1974\u20131975 that incubating rat blood cells or brain tissue with (14C-CH3)SAM and NMT as substrate mostly yields tetrahydro-\u03b2-carboline derivatives, and negligible amounts of DMT in brain tissue. It is indeed simultaneously realized that the TLC methods used thus far in almost all published studies on INMT and DMT biosynthesis are incapable to resolve DMT from those tetrahydro-\u03b2-carbolines. These findings are a blow for all previous claims of evidence of INMT activity and DMT biosynthesis in avian and mammalian brain, including in vivo, as they all relied upon use of the problematic TLC methods: their validity is doubted in replication studies that make use of improved TLC methods, and fail to evidence DMT-producing INMT activity in rat and human brain tissues. Published in 1978, the last study attempting to evidence in vivo INMT activity and DMT production in brain (rat) with TLC methods finds biotransformation of radiolabeled tryptamine into DMT to be real but \"insignificant\". Capability of the method used in this latter study to resolve DMT from tetrahydro-\u03b2-carbolines is questioned later.\nTo localize INMT, a qualitative leap is accomplished with use of modern techniques of molecular biology, and of immunohistochemistry. In humans, a gene encoding INMT is determined to be located on chromosome 7. Northern blot analyses reveal INMT messenger RNA (mRNA) to be highly expressed in rabbit lung, and in human thyroid, adrenal gland, and lung. Intermediate levels of expression are found in human heart, skeletal muscle, trachea, stomach, small intestine, pancreas, testis, prostate, placenta, lymph node, and spinal cord. Low to very low levels of expression are noted in rabbit brain, and human thymus, liver, spleen, kidney, colon, ovary, and bone marrow. INMT mRNA expression is absent in human peripheral blood leukocytes, whole brain, and in tissue from seven specific brain regions (thalamus, subthalamic nucleus, caudate nucleus, hippocampus, amygdala, substantia nigra, and corpus callosum). Immunohistochemistry showed INMT to be present in large amounts in glandular epithelial cells of small and large intestines. In 2011, immunohistochemistry revealed the presence of INMT in primate nervous tissue including retina, spinal cord motor neurons, and pineal gland. A 2020 study using in-situ hybridization, a far more accurate tool than the northern blot analysis, found mRNA coding for INMT expressed in the human cerebral cortex, choroid plexus, and pineal gland.\nPharmacology.\nPharmacodynamics.\nDMT binds non-selectively with affinities below 0.6\u00a0\u03bcmol/L to the following serotonin receptors: 5-HT1A, 5-HT1B, 5-HT1D, 5-HT2A, 5-HT2B, 5-HT2C, 5-HT6, and 5-HT7. An agonist action has been determined at 5-HT1A, 5-HT2A and 5-HT2C. Its efficacies at other serotonin receptors remain to be determined. Of special interest will be the determination of its efficacy at human 5-HT2B receptor as two \"in vitro\" assays evidenced DMT's high affinity for this receptor: 0.108\u00a0\u03bcmol/L and 0.184\u00a0\u03bcmol/L. This may be of importance because chronic or frequent uses of serotonergic drugs showing preferential high affinity and clear agonism at 5-HT2B receptor have been causally linked to valvular heart disease.\nIt has also been shown to possess affinity for the dopamine D1, \u03b11-adrenergic, \u03b12-adrenergic, imidazoline-1, and \u03c31 receptors. Converging lines of evidence established activation of the \u03c31 receptor at concentrations of 50\u2013100\u00a0\u03bcmol/L. Its efficacies at the other receptor binding sites are unclear. It has also been shown \"in vitro\" to be a substrate for the cell-surface serotonin transporter (SERT) expressed in human platelets, and the rat vesicular monoamine transporter 2 (VMAT2), which was transiently expressed in fall armyworm Sf9 cells. DMT inhibited SERT-mediated serotonin uptake into platelets at an average concentration of 4.00 \u00b1 0.70\u00a0\u03bcmol/L and VMAT2-mediated serotonin uptake at an average concentration of 93 \u00b1 6.8\u00a0\u03bcmol/L. In addition, DMT is a potent serotonin releasing agent with an value of 114nM.\nAs with other so-called \"classical hallucinogens\", a large part of DMT psychedelic effects can be attributed to a functionally selective activation of the 5-HT2A receptor. DMT concentrations eliciting 50% of its maximal effect (half maximal effective concentration = EC50) at the human 5-HT2A receptor \"in vitro\" are in the 0.118\u20130.983\u00a0\u03bcmol/L range. This range of values coincides well with the range of concentrations measured in blood and plasma after administration of a fully psychedelic dose (see Pharmacokinetics).\nDMT is one of the only psychedelics that isn't known to produce tolerance to its hallucinogenic effects. The lack of tolerance with DMT may be related to the fact that, unlike other psychedelics such as LSD and DOI, DMT does not desensitize serotonin 5-HT2A receptors \"in vitro\". This may be due to the fact that DMT is a biased agonist of the serotonin 5-HT2A receptor. More specifically, DMT activates the Gq signaling pathway of the serotonin 5-HT2A receptor without significantly recruiting \u03b2-arrestin2. Activation of \u03b2-arrestin2 is linked to receptor downregulation and tachyphylaxis. Similarly to DMT, 5-MeO-DMT is a biased agonist of the serotonin 5-HT2A receptor, with minimal \u03b2-arrestin2 recruitment, and likewise has been associated with little tolerance to its hallucinogenic effects.\nAs DMT has been shown to have slightly better efficacy (EC50) at human serotonin 2C receptor than at the 2A receptor, 5-HT2C is also likely implicated in DMT's overall effects. Other receptors such as 5-HT1A and \u03c31 may also play a role.\nIn 2009, it was hypothesized that DMT may be an endogenous ligand for the \u03c31 receptor. The concentration of DMT needed for \u03c31 activation \"in vitro\" (50\u2013100\u00a0\u03bcmol/L) is similar to the behaviorally active concentration measured in mouse brain of approximately 106\u00a0\u03bcmol/L This is minimally 4 orders of magnitude higher than the average concentrations measured in rat brain tissue or human plasma under basal conditions (see Endogenous DMT), so \u03c31 receptors are likely to be activated only under conditions of high local DMT concentrations. If DMT is stored in synaptic vesicles, such concentrations might occur during vesicular release. To illustrate, while the \"average\" concentration of serotonin in brain tissue is in the 1.5\u20134\u00a0\u03bcmol/L range, the concentration of serotonin in synaptic vesicles was measured at 270 mM. Following vesicular release, the resulting concentration of serotonin in the synaptic cleft, to which serotonin receptors are exposed, is estimated to be about 300\u00a0\u03bcmol/L. Thus, while \"in vitro\" receptor binding affinities, efficacies, and average concentrations in tissue or plasma are useful, they are not likely to predict DMT concentrations in the vesicles or at synaptic or intracellular receptors. Under these conditions, notions of receptor selectivity are moot, and it seems probable that most of the receptors identified as targets for DMT (see above) participate in producing its psychedelic effects.\nIn September 2020, an \"in vitro\" and \"in vivo\" study found that DMT present in the ayahuasca infusion promotes neurogenesis, meaning it helps with generating neurons.\nPharmacokinetics.\nDMT peak level concentrations (\"C\"max) measured in whole blood after intramuscular (IM) injection (0.7\u00a0mg/kg, \"n\" = 11) and in plasma following intravenous (IV) administration (0.4\u00a0mg/kg, \"n\" = 10) of fully psychedelic doses are in the range of around 14 to 154\u00a0\u03bcg/L and 32 to 204\u00a0\u03bcg/L, respectively.\nThe corresponding molar concentrations of DMT are therefore in the range of 0.074\u20130.818\u00a0\u03bcmol/L in whole blood and 0.170\u20131.08\u00a0\u03bcmol in plasma. However, several studies have described active transport and accumulation of DMT into rat and dog brains following peripheral administration.\nSimilar active transport, and accumulation processes likely occur in human brains and may concentrate DMT in brain by several-fold or more (relatively to blood), resulting in local concentrations in the micromolar or higher range. Such concentrations would be commensurate with serotonin brain tissue concentrations, which have been consistently determined to be in the 1.5\u20134\u00a0\u03bcmol/L range.\nClosely coextending with peak psychedelic effects, mean time to reach peak concentrations (\"T\"max) was determined to be 10\u201315 minutes in whole blood after IM injection, and 2 minutes in plasma after IV administration. When taken orally mixed in an ayahuasca decoction, and in freeze-dried ayahuasca gel caps, DMT \"T\"max is considerably delayed: 107.59\u00a0\u00b1\u00a032.5 minutes, and 90\u2013120 minutes, respectively.\nThe pharmacokinetics for vaporizing DMT have not been studied or reported.\nDue to its lipophilicity, DMT easily crosses the blood\u2013brain barrier and enters the central nervous system.\nSociety and culture.\nLegal status.\nInternational law.\nInternationally DMT is illegal to possess without authorisation, exemption or license, but ayahuasca and DMT brews and preparations are lawful. DMT is controlled by the Convention on Psychotropic Substances at the international level. The Convention makes it illegal to possess, buy, purchase, sell, to retail and to dispense without a licence.\nBy continent and country.\nIn some countries, ayahuasca is a forbidden or controlled or regulated substance, while in other countries it is not a controlled substance or its production, consumption, and sale, is allowed to various degrees.\nNorth America.\nIn December 2004, the Supreme Court lifted a stay, thereby allowing the Brazil-based Uni\u00e3o do Vegetal (UDV) church to use a decoction containing DMT in their Christmas services that year. This decoction is a tea made from boiled leaves and vines, known as hoasca within the UDV, and ayahuasca in different cultures. In \"Gonzales v. O Centro Esp\u00edrita Beneficente Uni\u00e3o do Vegetal\", the Supreme Court heard arguments on 1 November 2005, and unanimously ruled in February 2006 that the U.S. federal government must allow the UDV to import and consume the tea for religious ceremonies under the 1993 Religious Freedom Restoration Act.\nIn September 2008, the three Santo Daime churches filed suit in federal court to gain legal status to import DMT-containing ayahuasca tea. The case, \"Church of the Holy Light of the Queen v. Mukasey\", presided over by U.S. District Judge Owen M. Panner, was ruled in favor of the Santo Daime church. As of 21 March 2009, a federal judge says members of the church in Ashland can import, distribute and brew ayahuasca. Panner issued a permanent injunction barring the government from prohibiting or penalizing the sacramental use of \"Daime tea\". Panner's order said activities of The Church of the Holy Light of the Queen are legal and protected under freedom of religion. His order prohibits the federal government from interfering with and prosecuting church members who follow a list of regulations set out in his order.\nBlack market.\nElectronic cigarette cartridges filled with DMT started to be sold on the black market in 2018."}
{"id": "8750", "revid": "44249800", "url": "https://en.wikipedia.org/wiki?curid=8750", "title": "Da capo", "text": "Da capo ( , , ; often abbreviated as D.C.) is an Italian musical term that means \"from the beginning\" (literally, \"from the head\"). The term is a directive to repeat the previous part of music, often used to save space, and thus is an easier way of saying to repeat the music from the beginning.\nIn small pieces, this might be the same thing as a repeat. But in larger works, D.C. might occur after one or more repeats of small sections, indicating a return to the very beginning. The resulting structure of the piece is generally in ternary form. Sometimes, the composer describes the part to be repeated, for example: \"Menuet da capo\". In opera, where an aria of this structure is called a \"da capo aria\", the repeated section is often adorned with grace notes.\nThe word \"Fine\" (Ital. 'end') is generally placed above the stave at the point where the movement ceases after a 'Da capo' repetition. Its place is occasionally taken by a pause (see fermata).\""}
{"id": "8751", "revid": "1272009067", "url": "https://en.wikipedia.org/wiki?curid=8751", "title": "Dominatrix", "text": "A dominatrix ( ; or dominatrices ), or domme, is a woman who takes the dominant role in BDSM activities. The BDSM practice is called female dominance, or femdom. A dominatrix can be of any sexual orientation, but this does not necessarily limit the genders of her submissive partners. Dominatrices are popularly known for inflicting physical pain on their submissive subjects, but this is not done in every case. In some instances erotic humiliation is used, such as verbal humiliation or the assignment of humiliating tasks. Dominatrices also make use of other forms of servitude. Practices of domination common to many BDSM and other various sexual relationships are also prevalent. A dominatrix is typically a paid professional (\"pro-domme\") as the term \"dominatrix\" is little-used within the non-professional BDSM scene.\nTerminology and etymology.\n\"Dominatrix\" is the feminine form of the Latin \"dominator\", a ruler or lord, and was originally used in a non-sexual sense. Its use in English dates back to at least 1561. Its earliest recorded use in the prevalent modern sense, as a female dominant in sadomasochism, dates to 1961. It was initially coined to describe a woman who provides punishment-for-pay as one of the case studies within Bruce Roger's pulp paperback \"The Bizarre Lovemakers\". The term was taken up shortly after by the Myron Kosloff title \"Dominatrix\" (with art by Eric Stanton) in 1968, and entered more popular mainstream knowledge following the 1976 film \"Dominatrix Without Mercy\".\nThe term \"domme\" is likely a coined pseudo-French feminine inflection of the slang \"dom\" (short for \"dominant\"). The use of \"domme\", \"dominatrix\", \"dom\", or \"dominant\" by any woman in a dominant role is chosen mostly by personal preference and the conventions of the local BDSM scene. The term mistress or dominant mistress is sometimes also used. Female dominance (also known as female domination or femdom) is a BDSM activity in which the dominant partner is female. However, while the term \"mistress\" is often used in the media, members of the BDSM community often avoid it, as it can be confused with \"mistress\" in the sense of a woman who has an illicit relationship with a married man, a term which has the negative implication of cheating on a partner. Since there is a large overlap between the BDSM and polyamory communities, where ethical conduct is a prime concern, any such relationship is a source of disapproval.\nAlthough the term \"dominatrix\" was not used, the classic example in literature of the female dominant-male submissive relationship is portrayed in the 1870 novella \"Venus in Furs\" by Austrian writer Leopold von Sacher-Masoch. The term \"masochism\" was later derived from the author's name by Richard von Krafft-Ebing in the latter's 1886 forensic study \"Psychopathia Sexualis\".\nHistory.\nThe history of the dominatrix is argued to date back to rituals of the goddess Inanna (or Ishtar as she was known in Akkadian), in ancient Mesopotamia. Ancient cuneiform texts consisting of \"Hymns to Inanna\" have been cited as examples of the archetype of powerful, sexual female displaying dominating behaviors and forcing gods and men into submission to her. The pseudonymous archaeologist and BDSM historian Anne O. Nomis notes that Inanna's rituals included cross-dressing of cult personnel, and rituals \"imbued with pain and ecstasy, bringing about initiation and journeys of altered consciousness; punishment, moaning, ecstasy, lament and song, participants exhausting themselves with weeping and grief.\"\nThe fictional tale of Phyllis and Aristotle, which became popular and gained numerous versions from the 12th century onwards, tells the story of a dominant woman who seduced and dominated the male intellect of the greatest philosopher. In the story, Phyllis forces Aristotle to kneel on the ground so that she rides on his back while whipping and verbally humiliating him.\nThe profession appears to have originated as a specialization within brothels, before becoming its own unique craft. As far back as the 1590s, flagellation within an erotic setting is recorded. The profession features in erotic prints of the era, such as the British Museum mezzotint \"The Cully Flaug'd\" (c. 1674\u20131702), and in accounts of forbidden books which record the flogging schools and the activities practised.\nWithin the 18th century, female \"Birch Disciplinarians\" advertised their services in a book masked as a collection of lectures or theatrical plays, entitled \"Fashionable Lectures\" (c. 1761). This included the names of 57 women, some actresses and courtesans, who catered to birch discipline fantasies, keeping a room with rods and cat o' nine tails, and charging their clients a Guinea for a \"lecture\".\nThe 19th century is characterised by what Nomis characterises as the \"Golden Age of the Governess\". No fewer than twenty establishments were documented as having existed by the 1840s, supported entirely by flagellation practices and known as \"Houses of Discipline\" distinct from brothels. Amongst the well-known \"dominatrix governesses\" were Mrs Chalmers, Mrs Noyeau, the late Mrs Jones of Hertford Street and London Street, the late Mrs Theresa Berkley, Bessy Burgess of York Square and Mrs Pyree of Burton Crescent. The most famous of these Governess \"female flagellants\" was Theresa Berkley, who operated her establishment on Charlotte Street in the central London district of Marylebone. She is recorded to have used implements such as whips, canes and birches, to chastise and punish her male clients, as well as the Berkley Horse, a specially designed flogging machine, and a pulley suspension system for lifting them off the floor. Such historical use of corporal punishment and suspension, in a setting of domination roleplay, connects very closely to the practices of modern-day professional dominatrices.\nThe \"bizarre style\" (as it came to be called) of leather catsuits, claws, tail whips, and latex rubber only came about in the 20th century, initially within commercial fetish photography, and taken up by dominatrices. Within the mid-20th century, dominatrices operated in a very discreet and underground manner, which has made them difficult to trace within the historical record. A few photographs still exist of the women who ran their domination businesses in London, New York, The Hague and Hamburg's Herbertstra\u00dfe, predominantly in sepia and black-and-white photographs, and scans from magazine articles, copied and re-copied. Amongst these were Miss Doreen of London who was acquainted with John Sutcliffe of \"AtomAge\" fame, whose clients reportedly included Britain's top politicians and businessmen. In New York, the dominatrix Anne Laurence was known within the underground circle of acquaintances during the 1950s, with Monique Von Cleef arriving in the early 1960s, and hitting national headlines when her home was raided by police detectives on 22 December 1965. Von Cleef went on to set up her \"House of Pain\" in The Hague in the 1970s, which became one of the world capitals for dominatrices, reportedly with visiting lawyers, ambassadors, diplomats and politicians. Domenica Niehoff worked as a dominatrix in Hamburg and appeared on talk shows on German television from the 1970s onwards, campaigning for sex workers' rights. Mistress Raven, founder and manager of Pandora's Box, one of New York's best known BDSM studios, was featured in Nick Broomfield's 1996 documentary film \"Fetishes\".\nProfessional dominatrices.\nThe term \"dominatrix\" is mostly used to describe a female professional dominant (or \"pro-domme\") who is paid to engage in BDSM play with a submissive. Professional dominatrices are, according to some people, not prostitutes, despite the sensual and erotic interactions they have. An appointment or roleplay is referred to as a \"session\", and is often conducted in a dedicated professional play space which has been set up with specialist equipment, known as a \"dungeon\". Sessions may also be conducted remotely by letter or telephone, or in the contemporary era of technological connectivity by email, online chat or platforms such as OnlyFans. Most, but not all, clients of female professional dominants are men. Male professional dominants also exist, catering predominantly to the gay male market.\nWomen who engage in female domination typically promote and title themselves under the terms \"dominatrix\", \"mistress\", \"lady\", \"madame\", \"\" (German for \"mistress\") or \"goddess\". In a study of German dominatrices, Andrew Wilson said that the trend for dominatrices choosing names aimed at creating and maintaining an atmosphere in which class, femininity and mystery are key elements of their self-constructed identity.\nSome professional dominatrices set minimum age limits for their clients. Popular requests from clients are for dungeon play including bondage, spanking and cock and ball torture, or for medical play using hoods, gas masks and urethral sounding. Verbal erotic humiliation, such as small penis humiliation, is also popular. There are some professional dominatrices that engage in sexual contact activities such as facesitting, handjobs or fellatio but others disapprove of this. Other BDSM activities can include various forms of body worship, such as foot worship, ass worship, breast worship and pussy worship; tease and denial; corporal punishment including breast torture, caning, whipping; orgasm denial; and as well as face slapping, hair pulling, dripping hot wax on the genitals, spitting, golden showers, \"forced\" chastity, cock and ball torture, and pussy torture.\nIt is not unusual for a dominatrix to consider her profession different from that of an escort and not perform tie and tease or \"happy endings\". Typically professional dominatrices do not have sexual intercourse with their clients, do not become naked with their clients and do not allow their clients to touch them. The Canadian dominatrix Terri-Jean Bedford, who was one of three women who initiated an application in the Ontario Superior Court seeking invalidation of Canada's laws regarding brothels, sought to differentiate for clarity her occupation as a dominatrix rather than a prostitute to the media, due to frequent misunderstanding and conflation by the public of the two terms.\nThat being said, it is now generally accepted that a professional dominatrix is a sex worker, and many of the acts conducted during a session may be interpreted as equally sexual to the participants.\nWhile dominatrices come from many different backgrounds, it has been shown that a considerable number are well-educated. Research into US dominatrices published in 2012 indicated that 39% of the sample studied had received some sort of graduate training.\nA 1985 study suggested that about 30 percent of participants in BDSM subculture were female. A 1994 report indicated that around a quarter of the women who took part in BDSM subculture did so professionally. In a 1995 study of Internet discussion group messages, the preference for the dominant-initiator role was expressed by 11% of messages by heterosexual women, compared to 71% of messages by heterosexual men.\nProfessional dominatrices can be seen advertising their services online and in print publications which carry erotic services advertising, such as contact magazines and fetish magazines that specialise in female domination. The precise number of women actively offering professional domination services is unknown. Most professional dominatrices practice in large metropolitan cities such as New York, Los Angeles, and London, with as many as 200 women working as dominatrices in Los Angeles.\nProfessional dominatrices may take pride or differentiation in their psychological insight into their clients' fetishes and desires, as well as their technical ability to perform complex BDSM practices, such as Japanese shibari, head-scissoring, and other forms of bondage, suspension, torture roleplay, and corporal punishment, and other such practices which require a high degree of knowledge and competency to safely oversee. From a sociological point of view, Danielle Lindemann has stated the \"embattled purity regime\" in which many pro-dommes emphasise their specialist knowledge and professional skills, while distancing themselves from economic criteria for success, in a way which is comparable to avant-garde artists.\nSome dominatrices practice financial domination, or findom, a fetish in which a submissive is aroused by sending money or gifts to a dominatrix at her instruction. In some cases the dominatrix is given control of the submissive's finances or a \"blackmail\" scenario is acted out. In the majority of cases the dominatrix and the submissive do not physically meet. The interactions are typically performed using the Internet, which is also where such services are advertised. Findom was originally a niche service that a traditional dominatrix would offer, but it has become popular with less-experienced online practitioners.\nTo differentiate women who identify as a dominatrix but do not offer paid services, non-professional dominants are occasionally referred to as a \"lifestyle\" dominatrix or Mistress. The term \"lifestyle\" to signify BDSM is occasionally a contention topic in the BDSM community and that some dominatrices may dislike the term. Some professional dominatrices are also \"lifestyle\" dominatrices\u2014i.e., in addition to paid sessions with submissive clients they engage in unpaid recreational sessions or may incorporate power exchange within their own private lives and relationships. However, the term has fallen out of general usage with respect to women who are dominant in their private relationships, and has taken on more and more the connotation of \"professional\". Nathalie Lugand in her 2023 book \"A Psychodynamic Approach to Female Domination in BDSM Relationships\" describes this strict separation as artificial.\nNotable dominatrices.\nCatherine Robbe-Grillet is a lifestyle dominatrix. Born in Paris on September 24, 1930, she then became France's most famous lifestyle dominatrix. She is also a writer and actress, the widow of nouveau roman pioneer and sadist Alain Robbe-Grillet. She currently lives with Beverly Charpentier, a 51-year-old South African woman who is her submissive companion. Although being such a famous dominatrix, she has never accepted payment for her \"ceremonies\". She's quoted as saying \"If someone pays, then they are in charge. I need to remain free. It is important that everyone involved knows that I do it solely for my pleasure.\" \"Catherine is my secret garden,\" Charpentier says. \"I have given myself to her, body and soul. She does whatever she wants, whenever she wants, with either or both, according to her pleasure\u2014and her pleasure is also my pleasure.\" Robbe-Grillet has been criticised for writing about S/M stories. She identifies as a \"pro-sex feminist\" and \"the kind of feminist who supports the right of any man or woman to work as a prostitute, if it is their free choice.\"\nImagery.\nThe dominatrix is a symbolic female archetype. In popular culture, the conception of the dominatrix is generally associated with specialized clothing and props used to signify her role as a strong, dominant, sexualised woman. This role is linked to but distinct from images of sexual fetish. During the twentieth century, dominatrix imagery was developed by the work of a number of artists including the costume designer and photographer Charles Guyette, the publisher and film director Irving Klaw, and the illustrators Eric Stanton and Gene Bilbrew who drew for the fetish magazine \"Exotique\".\nOne of the garments associated with the dominatrix is the catsuit. The black leather female catsuit entered dominant fetish culture in the 1950s with the \"AtomAge\" magazine and its connections to fetish fashion designer John Sutcliffe. Its appearance in mainstream culture began when catsuits were worn by strong female protagonists in popular 1960s TV programs like \"The Avengers\" and by comic super-heroines such as Catwoman. The catsuit represented the independence of a woman capable of \"kick-ass\" moves and action, giving complete freedom of movement. At the same time, the one-piece catsuit accentuated and exaggerated the sexualized female form, providing visual access to a woman's body, while simultaneously obstructing physical penetrative access. \"You can look but you can't touch\" is the message, which plays upon the BDSM practice known as \"tease and denial\".\nAnother common image is that of a dominatrix wearing thigh-high boots in leather or shiny PVC, which have long held a fetishistic status and are sometimes called kinky boots, along with very high stiletto heels. Fishnet stockings, seamed hosiery, stockings and garter belts (suspenders) are also used in the representation and attire of dominatrices, to emphasize the form and length of the legs with erotic connotation.\nTight leather corsets are another popular dominatrix garment. Gloves, whether long opera gloves or fingerless gloves, are often a further accessory to emphasize the feminine role. Neck corsets are also sometimes worn.\nDominatrices frequently wear clothing made from fetish fashion materials. Examples include PVC clothing, latex clothing and garments drawn from the leather subculture. In some cases elements of dominatrix attire, such as leather boots and peaked cap, are drawn from Nazi chic, particularly the black SS officer's uniform which has been widely adopted and fetishized by underground gay and BDSM lifestyle groups to satisfy a uniform fetish.\nA dominatrix often uses strong, dominant body language which is comparable to dominant posturing in the animal world. The props she brandishes signify her role as dominatrix, such as a flogger, whip or riding crop as illustrated in the artwork of Bruno Zach in the early 20th century.\nAnother often-depicted characteristic of the dominatrix character is of smoking, either of tobacco cigarettes or cannabis products. While smoking tobacco has been in rapid decline worldwide, depiction of it in BDSM literature and media is increasing, as the negative image of smoking reinforces the \"bad girl\" stereotype associated with a dominatrix.\nPracticing professional dominatrices may draw their attire from the conventional imagery associated with the role, or adapt it to create their own individual style. There is a potential conflict between meeting conventional expectations and a desire for dominant independent self-expression. Some contemporary dominatrices draw upon an eclectic range of strong female archetypes, including the goddess, the female superheroine, the femme fatale, the priestess, the empress, the queen, the governess and the KGB secret agent.\nIn literature.\nThemes associated with the dominatrix character have appeared in literature since the 10th century. Canoness Hroswitha, in her manuscript \"Maria,\" uses the word \"Dominatrix\" for the main character. She is portrayed as an unattainable woman who is too good for any of the men who are in love with her. The theme of \"the unattainable woman\" has been used thoroughly in medieval literature as well, although it differs from a dominatrix. Medieval themes surrounding the unattainable woman concerned issues of social classes and structure, with chivalry being a prime part of a relationship between a man and woman. There are some exceptions to this trend during medieval times. In Cervantes\u2019 \"Don Quixote\" (1605), Celadon is imprisoned by Galatea. Celadon complains that his \"mistress . . . Galatea keeps me on such a short leash\". In Robert Herrick's \"Hesperides\", a book of poems published in 1648, there were three revealing poems \"An Hymne to Love\", \"The Dream\", and \"To Love\" which showcase masculine longing for domination, restraint, discipline. In \"Ulysses\" by James Joyce, the character Leopold Bloom has many fantasies of submission to a lady and of receiving whippings from her.\nIn popular culture.\nThere have been a number of depictions of dominatrices in film and television, almost always featuring a professional dominatrix. Depictions of dominatrices in popular culture include:"}
{"id": "8752", "revid": "47722236", "url": "https://en.wikipedia.org/wiki?curid=8752", "title": "Flag of Denmark", "text": "The flag of Denmark (, ) is red with a white Nordic cross, which means that the cross extends to the edges of the flag and that the vertical part of the cross is shifted to the hoist side.\nA banner with a white-on-red cross is attested as having been used by the kings of Denmark since the 14th century. An origin legend with considerable impact on Danish national historiography connects the introduction of the flag to the Battle of Lindanise of 1219.\nThe elongated Nordic cross, which represents Christianity, reflects its use as a maritime flag in the 18th century. The flag became popular as a national flag in the early 16th century. Its private use was outlawed in 1834 but again permitted by a regulation of 1854. The flag holds the Guinness world record of being the oldest continuously used national flag, that is since 1625.\nDescription.\nA 1748 regulation, which is still in force, defines the flag as constructed of two squares of , with a white cross the height of the flag and the two rectangular fields as . Multiplying the proportions by three to get whole numbers gives the proportions in the construction sheet below (28 divided by 4 being 7 for the white cross).\nColour.\nNo official definition of \"Dannebrog r\u00f8d\" exists. The private company \"Dansk Standard\", regulation number 359 (2005), defines the red colour of the flag as Pantone 186c.\nHistory.\n1219 origin legend.\nA tradition recorded in the 16th century traces the origin of the flag to the campaigns of Valdemar II of Denmark (r. 1202\u20131241). The oldest of them is in Christiern Pedersen's \"Danske Kr\u00f8nike\", which is a sequel to Saxo Grammaticus's , which was written in 1520 to 1523. Here, the flag falls from the sky during one of Valdemar's military campaigns overseas. Pedersen also states that the very same flag was taken into exile by Eric of Pomerania in 1440.\nThe second source is the writing of the Franciscan friar Petrus Olai (Peder Olsen) of Roskilde (died ). This record describes a battle in 1208 near Fellin during the Estonia campaign of King Valdemar II. The Danes were all but defeated when a lamb-skin banner depicting a white cross fell from the sky and miraculously led to a Danish victory. In a third account, also by Petrus Olai, in \"Danmarks Tolv Herligheder\" (\"Twelve Splendours of Denmark\"), in splendour number nine, the same story is retold almost verbatim, with a paragraph inserted correcting the year to 1219. Now, the flag is falling from the sky in the Battle of Lindanise, also known as the Battle of Valdemar (Danish: \"Volmerslaget\"), near Lindanise (Tallinn) in Estonia, of 15 June 1219.\nIt is this third account that has been the most influential, and some historians have treated it as the primary account taken from a (lost) source dating to the first half of the 15th century.\nIn Olai's account, the battle was going badly, and defeat seemed imminent. However the Danish bishop, Anders Sunesen, was on top of a hill overlooking the battle and prayed to God with his arms raised. The Danes moved closer to victory as prayed. When he raised his arms, the Danes surged forward, but when his arms grew tired, and he let them fall, the Estonians turned the Danes back. Attendants rushed forward to raise his arms once again, and the Danes again surged forward, but for a second time he grew so tired that he dropped his arms, and the Danes again lost the advantage and became closer to defeat. He needed two soldiers to keep his hands up (which might be a copycat of Exodus 17:11-12). When the Danes were about to lose, the \"Dannebrog\" miraculously fell from the sky. The King took it and showed it to the troops, their hearts were filled with courage, and the Danes won the battle.\nThe possible historical nucleus behind this origin legend was extensively discussed by Danish historians in the 19th to 20th centuries. One such example is Adolf Ditlev J\u00f8rgensen, who argued that Bishop Theoderich was the original instigator of the 1218 inquiry from Bishop Albert of Buxhoeveden to King Valdemar II which led to the Danish participation in the Baltic crusades. J\u00f8rgensen speculates that Bishop Theoderich might have carried the Knight Hospitaller's banner in the 1219 battle and that \"the enemy thought this was the King's symbol and mistakenly stormed Bishop Theoderich tent. He claims that the origin of the legend of the falling flag comes from this confusion in the battle\".\nThe Danish church-historian L. P. Fabricius (1934) ascribes the origin to the 1208 Battle of Fellin, not the Battle of Lindanise in 1219, based on the earliest source available about the story. Fabricius speculated that it might have been Archbishop Andreas Sunes\u00f8n's personal ecclesiastical banner or perhaps even the flag of Archbishop Absalon under whose initiative and supervision several smaller crusades had already been conducted in Estonia. The banner would then already be known in Estonia. Fabricius repeats J\u00f8rgensen's idea about the flag being planted in front of Bishop Theodorik's tent, which the enemy mistakenly attacked believing it to be the tent of the King.\nA different theory is briefly discussed by Fabricius and elaborated more by Helge Bruhn (1949). Bruhn interprets the story in the context of the widespread tradition of the miraculous appearance of crosses in the sky in Christian legend, specifically comparing such an event attributed to a battle of 10 September 1217 near Alcazar in which it is said that a golden cross on white appeared in the sky and brought victory to the Christians.\nIn Swedish national historiography of the 18th century, there is a tale paralleling the Danish legend, in which \na golden cross appears in the blue sky during a Swedish battle in Finland in 1157.\nMiddle Ages.\nThe white-on-red cross emblem originates in the age of the Crusades. In the 12th century, it was also used as war flag by the Holy Roman Empire.\nIn the \"Gelre Armorial\", dated 1340\u20131370, such a banner is shown alongside the coat of arms of the king of Denmark. This is the earliest known undisputed colour rendering of the Dannebrog. About the same time, Valdemar IV of Denmark displays a cross in his coat of arms on his \"Dan\u00e6log\" seal (\"Rettertingsseglet\", dated 1356). The image from the Armorial Gelre is nearly identical to an image found in a 15th-century coat of arms book now located in the National Archives of Sweden (\"Riksarkivet\"). The seal of Eric of Pomerania (1398) as king of the Kalmar Union displays the arms of Denmark's chief dexter, three lions. In this version, the lions hold a Dannebrog banner.\nThe reason that the kings of Denmark in the 14th century began displaying the cross banner in their coats of arms is unknown. Caspar Paludan-M\u00fcller (1873) suggested that it may reflect a banner sent by the pope to support the king during the Livonian Crusade. Adolf Ditlev J\u00f8rgensen (1875) identifies the banner as that of the Knights Hospitaller, an order that had a presence in Denmark from the later 12th century.\nSeveral coins, seals and images exist, both foreign and domestic, from the 13th to the 15th centuries and even earlier and show simlar heraldic designs similar, alongside the royal coat of arms (three blue lions on a golden shield.)\nThere is a record suggesting that the Danish Army had a \"chief banner\" (\"hoffuitbanner\") in the early 16th century. Such a banner is mentioned in 1570 by Niels Hemmings\u00f8n in the context of a 1520 battle between Danes and Swedes near Uppsala as nearly captured by the Swedes but saved by the heroic actions of the banner-carrier Mogens Gyldenstierne and Peder Skram. The legend attributing the miraculous origin of the flag to the campaigns of Valdemar II of Denmark (r. 1202-1241) was recorded by Christiern Pedersen and Petrus Olai in the 1520s.\nHans Svaning's \"History of King Hans\" from 1558 to 1559 and Johan Rantzau's \"History about the Last Dithmarschen War\", from 1569, record the further fate of the Danish \"hoffuitbanner\": According to the tradition, the original flag from the Battle of Lindanise was used in the small campaign of 1500, when King Hans tried to conquer Dithmarschen (in western Holstein in northern Germany). The flag was lost in a devastating defeat at the Battle of Hemmingstedt, on 17 February 1500. In 1559, King Frederik II recaptured it during his own Dithmarschen campaign.\nIn 1576, the son of Johan Rantzau, Henrik Rantzau, also writes about the war and the fate of the flag, noting that the flag was in a poor condition when returned. He records that the flag after its return to Denmark was placed in the cathedral in Slesvig. Slesvig historian Ulrik Petersen (1656\u20131735) confirms the presence of such a banner in the cathedral in the early 17th century and records that it had crumbled away by about 1660.\nContemporary records describing the battle of Hemmingstedt make no reference to the loss of the original Dannebrog, although the capitulation state that all Danish banners lost in 1500 was to be returned. In a letter dated 22 February 1500 to Oluf Stigs\u00f8n, King John describes the battle but does not mention the loss of an important flag. In fact, the entire letter gives the impression that the lost battle was of limited importance. In 1598, Neocorus wrote that the banner captured in 1500 was brought to the church in W\u00f6hrden and hung there for the next 59 years until it was returned to the Danes as part of the peace settlement in 1559.\nModern period.\nUsed as a maritime flag since the 16th century, the Dannebrog was introduced as a regimental flag in the Danish army in 1785, and for the militia (landev\u00e6rn) in 1801. From 1842, it was used as the flag of the entire army.\nDuring the first half of the 19th century, in parallel to the development of Romantic nationalism in other European countries, the military flag increasingly came to be seen as representing the nation itself. Poems of the period invoking the \"Dannebrog\" were written by B.S. Ingemann, N.F.S. Grundtvig, Oehlenschl\u00e4ger, Chr. Winther and H.C. Andersen. By the 1830s, the military flag had become popular as an unofficial national flag, and its use by private citizens was outlawed in a circular enacted on 7 January 1834.\nIn the national enthusiasm sparked by the First Schleswig War from 1848 to 1850, the flag was still very widely displayed, and the prohibition of private use was repealed in a regulation of 7 July 1854 that for the first time allowed Danish citizens to display the Dannebrog (but not the swallow-tailed \"Splitflag\" variant. Special permission to use the \"Splitflag\" was given to individual institutions and private companies, especially after 1870. In 1886, the war ministry introduced a regulation indicating that the flag should be flown from military buildings on thirteen specified days, including royal birthdays, the date of the signing of the Constitution of 5 June 1849 and days of remembrance for military battles. In 1913, the naval ministry issued its own list of flag days. On 10 April 1915, the hoisting of any other flag on Danish soil was prohibited. The prohibition was lifted on 24 June 2023, after a Supreme Court ruling. From 1939 to 2012, the yearbook \"Hvem-Hvad-Hvor\" included a list of flag days. As of 2019, flag days can be viewed at the \"Ministry of Justice (Justitsministeriet)\" as well as \"The Denmark Society (Danmarks-Samfundet)\".\nVariants.\nMaritime flag and corresponding Kingdom flag.\nThe size and shape of the civil ensign (\"Koffardiflaget\") for merchant ships is given in the regulation of 11 June 1748, which says: \"A red flag with a white cross with no split end. The white cross must be of the flag's height. The two first fields must be square in form and the two outer fields must be lengths of those\". The proportions are thus: 3:1:3 vertically and 3:1:4.5 horizontally. This definition are the absolute proportions for the Danish national flag to this day, for both the civil version of the flag (\"Stutflaget\"), as well as the merchant flag (\"Handelsflaget\"). The civil flag and the merchant flag are identical in colour and design.\nA regulation passed in 1758 required Danish ships sailing in the Mediterranean to carry the royal cypher in the center of the flag to distinguish them from Maltese ships because of its similarity of the flag of the Sovereign Military Order of Malta.\nAccording to the regulation of 11 June 1748, the colour was simply red, which is common known today as \"Dannebrog r\u00f8d\" (\"Dannebrog red\"). The only red fabric dye then available was made of madder root, which can be processed to produce a brilliant red dye and was used historically for British and Danish and soldiers' jackets. A regulation of 4 May 1927 once again stated that Danish merchant ships had to fly flags according to the regulation of 1748.\nThe first regulation regarding the \"Splitflag\" dates from 27 March 1630, in which King Christian IV ordered that Norwegian \"Defensionskibe\" (armed merchants ships) were allowed to use only the \"Splitflag\" if they were in Danish war service. In 1685, an order was distributed to a number of cities in Slesvig and stateed that all ships had to carry the Danish flag, and in 1690, all merchant ships were forbidden to use the \"Splitflag\" except for ships sailing in the East Indies, the West Indies or along the coast of Africa. In 1741, it was confirmed that the regulation of 1690 was still very much in effect that merchant ships were not allowed to use the \"Splitflag\". At the same time, the Danish East India Company was allowed to fly the \"Splitflag\" past the equator.\nSome confusion must have existed regarding the \"Splitflag\". In 1696 the Admiralty, presented the King with a proposal for a standard regulating both size and shape of the \"Splitflag\". In the same year, a royal resolution defined the proportions of the \"Splitflag\", which was called \"Kongeflaget\" (the King's flag), as follows: \"The cross must be of the flags height. The two first fields ,ist be square in form with the sides three times the cross width. The two outer fields are rectangular and the length of the square fields. The tails are the length of the flag\".\nThose numbers are still the base for the \"Splitflag\" and the \"Orlogsflag\" though the numbers have been slightly altered. The term \"Orlogsflag\" dates from 1806 and denotes its use in the Danish Navy.\nFrom about 1750 to the early 19th century, a number of ships and companies in which the government has interests received approval to use the \"Splitflag\".\nIn the royal resolution of 25 October 1939 for the Danish Navy, it was stated that the \"Orlogsflag\" is a \"Splitflag\" with a deep red (\"dybr\u00f8d\") or madder red (\"krapr\u00f8d\") colour. As for the national flag, no shade was given, but it is now stated as 195U. Furthermore, the size and the shape were corrected in the new resolution: \"The cross must be of the flag's height. The two first fields must be square in form with the height of of the flag's height. The two outer fields are rectangular and the length of the square fields. The tails are the length of the rectangular fields\". Thus, if compared to the standard of 1696, both the rectangular fields and the tails have decreased in size.\nThe \"Splitflag\" and \"Orlogsflag\" have similar shapes but different sizes and shades of red. Legally, they are two different flags. The \"Splitflag\" is a Danish flag ending in a swallow-tail, it is \"Dannebrog red\" and is used on land. The \"Orlogsflag\" is an elongated \"Splitflag\" with a deeper red colour and is used only at sea.\nThe \"Orlogsflag\" with no markings may be used only by the Royal Danish Navy, but there are a few exceptions. A few institutions have been allowed to fly the clean \"Orlogsflag\". The same flag with markings has been approved for a few dozen companies and institutions over the years.\nFurthermore, the \"Orlogsflag\" is described as such only if it has no additional markings. Any swallow-tail flag, no matter the colour, is called a \"Splitflag\" provided it bears additional markings.\nMonarch.\nThe current version of the royal standard was introduced on 1 January 2025. after King Frederik X adopted a new version of his personal coat of arms on 20 December 2024. The royal standard is the flag of Denmark with a swallow-tail and charged with the monarch's coat of arms set in a white square. The centre square is 32 parts in a flag with the ratio 56:107.\nOther flags in the Kingdom of Denmark.\nGreenland and the Faroe Islands are autonomous territories within the Kingdom of Denmark. They have their own official flags.\nSome areas in Denmark have unofficial flags. While they have no legal recognition or regulation, they can be used freely.\nThe regional flags of Bornholm and \u00c6r\u00f8 are occasionally used by locals of those islands and in tourist-related businesses.\nThe proposal for a flag of Jutland has hardly found any actual use, maybe in part because of its peculiar design.\nThe flag of Vendsyssel (Vendelbrog) is seen infrequently, but many locals recognise it. According to an article in the newspaper \"Nordjyske\", the flag had been used in the former insignia of Flight Eskadrille 723 of Aalborg Air Base, in the 1980s."}
{"id": "8753", "revid": "38637735", "url": "https://en.wikipedia.org/wiki?curid=8753", "title": "Dharma", "text": "Dharma (; , ) is a key concept in the Indian religions of Hinduism, Buddhism, Jainism and Sikhism. The term \"dharma\" is considered untranslatable into English (or other European languages); it is understood to refer to behaviours which are in harmony with the \"order and custom\" that sustains life; \"virtue\", righteousness or \"religious and moral duties\". The antonym of \"dharma\" is adharma.\nThe concept of \"dharma\" was in use in the historical Vedic religion (1500\u2013500 BCE), and its meaning and conceptual scope has evolved over several millennia. In Hinduism, \"dharma\" denotes behaviours that are considered to be in accord with \"\u1e5ata\"\u2014the \"order and custom\" that makes life and universe possible. This includes duties, rights, laws, conduct, virtues and \"right way of living\". \"Dharma\" is believed to have a transtemporal validity, and is one of the Puru\u1e63\u0101rtha.\nIn Buddhism, \"dharma\" () refers to the teachings of the Buddha. In Buddhist philosophy, \"dhamma/dharma\" is also the term for \"phenomena\". \"Dharma\" in Jainism refers to the teachings of Tirthankara (Jina) and the body of doctrine pertaining to the purification and moral transformation of humans. In Sikhism, \"dharma\" indicates the path of righteousness, proper religious practices, and performing one's own moral duties.\nAs with the other components of the Puru\u1e63\u0101rtha, the concept of \"dharma\" is pan-Indian. The ancient Tamil text \"Tirukku\u1e5fa\u1e37\", despite being a collection of aphoristic teachings on dharma (\"aram\"), artha (\"porul\"), and kama (\"inpam\"), is completely and exclusively based on \"a\u1e5fam\"\u2014the Tamil term for \"dharma\".\nEtymology.\nThe word \"dharma\" () has roots in the Sanskrit \"dhr-\", which means \"to hold\" or \"to support\", and is related to Latin \"firmus\" (firm, stable). From this, it takes the meaning of \"what is established or firm\", and hence \"law\". It is derived from an older Vedic Sanskrit \"n\"-stem \"dharman-\", with a literal meaning of \"bearer, supporter\", in a religious sense conceived as an aspect of Rta.\nIn the Rigveda, the word appears as an \"n\"-stem, \"\", with a range of meanings encompassing \"something established or firm\" (in the literal sense of prods or poles). Figuratively, it means \"sustainer\" and \"supporter\" (of deities). It is semantically similar to the Greek \"themis\" (\"fixed decree, statute, law\").\nIn Classical Sanskrit, and in the Vedic Sanskrit of the Atharvaveda, the stem is thematic: \"\" (Devanagari: \u0927\u0930\u094d\u092e). In Prakrit and Pali, it is rendered \"dhamma\". In some contemporary Indian languages and dialects it alternatively occurs as \"dharm\".\nIn the 3rd century BCE the Mauryan Emperor Ashoka translated \"dharma\" into Greek and Aramaic and he used the Greek word \"eusebeia\" (\u03b5\u1f50\u03c3\u03ad\u03b2\u03b5\u03b9\u03b1, piety, spiritual maturity, or godliness) in the Kandahar Bilingual Rock Inscription and the Kandahar Greek Edicts. In the former, he used the Aramaic word (\"\"; truth, rectitude).\nDefinition.\nDharma is a concept of central importance in Indian philosophy and Indian religions. It has multiple meanings in Hinduism, Buddhism, Sikhism and Jainism. It is difficult to provide a single concise definition for \"dharma\", as the word has a long and varied history and straddles a complex set of meanings and interpretations. There is no equivalent single-word synonym for \"dharma\" in western languages.\nThere have been numerous, conflicting attempts to translate ancient Sanskrit literature with the word \"dharma\" into German, English and French. The concept, claims Paul Horsch, has caused exceptional difficulties for modern commentators and translators. For example, while Grassmann's translation of Rig-Veda identifies seven different meanings of dharma, Karl Friedrich Geldner in his translation of the Rig-Veda employs 20 different translations for dharma, including meanings such as \"law\", \"order\", \"duty\", \"custom\", \"quality\", and \"model\", among others. However, the word \"dharma\" has become a widely accepted loanword in English, and is included in all modern unabridged English dictionaries.\nThe root of the word \"dharma\" is \"dhr\u0325\", which means \"to support, hold, or bear\". It is the thing that regulates the course of change by not participating in change, but that principle which remains constant. \"Monier-Williams Sanskrit-English Dictionary\", the widely cited resource for definitions and explanation of Sanskrit words and concepts of Hinduism, offers numerous definitions of the word \"dharma\", such as that which is established or firm, steadfast decree, statute, law, practice, custom, duty, right, justice, virtue, morality, ethics, religion, religious merit, good works, nature, character, quality, property. Yet, each of these definitions is incomplete, while the combination of these translations does not convey the total sense of the word. In common parlance, \"dharma\" means \"right way of living\" and \"path of rightness\". Dharma also has connotations of order, and when combined with the word \"sanatana\", it can also be described as eternal truth.\nThe meaning of the word \"dharma\" depends on the context, and its meaning has evolved as ideas of Hinduism have developed through history. In the earliest texts and ancient myths of Hinduism, \"dharma\" meant cosmic law, the rules that created the universe from chaos, as well as rituals; in later Vedas, Upanishads, Puranas and the Epics, the meaning became refined, richer, and more complex, and the word was applied to diverse contexts.\nIn certain contexts, \"dharma\" designates human behaviours considered necessary for order of things in the universe, principles that prevent chaos, behaviours and action necessary to all life in nature, society, family as well as at the individual level. \"Dharma\" encompasses ideas such as duty, rights, character, vocation, religion, customs and all behaviour considered appropriate, correct or morally upright. For further context, the word \"varnasramdharma\" is often used in its place, defined as dharma specifically related to the stage of life one is in. The concept of \"Dharma\" is believed to have a transtemporal validity.\nThe antonym of \"dharma\" is \"adharma\" (Sanskrit: \u0905\u0927\u0930\u094d\u092e), meaning that which is \"not dharma\". As with \"dharma\", the word \"adharma\" includes and implies many ideas; in common parlance, adharma means that which is against nature, immoral, unethical, wrong or unlawful.\nIn Buddhism, \"dharma\" incorporates the teachings and doctrines of the founder of Buddhism, the Buddha.\nHistory.\nAccording to Pandurang Vaman Kane, author of the book \"History of Dharma\u015b\u0101stra\", the word \"dharma\" appears at least fifty-six times in the hymns of the Rigveda, as an adjective or noun. According to Paul Horsch, the word \"dharma\" has its origin in Vedic Hinduism. The hymns of the Rigveda claim Brahman created the universe from chaos, they hold (dhar-) the earth and sun and stars apart, they support (dhar-) the sky away and distinct from earth, and they stabilise (dhar-) the quaking mountains and plains. \nThe Deities, mainly Indra, then deliver and hold order from disorder, harmony from chaos, stability from instability \u2013 actions recited in the Veda with the root of word dharma. In hymns composed after the mythological verses, the word dharma takes expanded meaning as a cosmic principle and appears in verses independent of deities. It evolves into a concept, claims Paul Horsch, that has a dynamic functional sense in Atharvaveda for example, where it becomes the cosmic law that links cause and effect through a subject. Dharma, in these ancient texts, also takes a ritual meaning. The ritual is connected to the cosmic, and \"dharmani\" is equated to ceremonial devotion to the principles that deities used to create order from disorder, the world from chaos.\nPast the ritual and cosmic sense of dharma that link the current world to mythical universe, the concept extends to an ethical-social sense that links human beings to each other and to other life forms. It is here that dharma as a concept of law emerges in Hinduism.\nDharma and related words are found in the oldest Vedic literature of Hinduism, in later Vedas, Upanishads, Puranas, and the Epics; the word dharma also plays a central role in the literature of other Indian religions founded later, such as Buddhism and Jainism. According to Brereton, \"Dharman\" occurs 63 times in Rig-veda; in addition, words related to Dharman also appear in Rig-veda, for example once as dharmakrt, 6 times as \"satyadharman\", and once as \"dharmavant\", 4 times as \"dharman\" and twice as \"dhariman\".\nIndo-European parallels for \"dharma\" are known, but the only Iranian equivalent is Old Persian \"darm\u0101n\", meaning \"remedy\". This meaning is different from the Indo-Aryan \"dh\u00e1rman\", suggesting that the word \"dharma\" did not play a major role in the Indo-Iranian period. Instead, it was primarily developed more recently under the Vedic tradition. \nIt is thought that the \"Daena\" of Zoroastrianism, also meaning the \"eternal Law\" or \"religion\", is related to Sanskrit \"dharma\". Ideas in parts overlapping to \"Dharma\" are found in other ancient cultures: such as Chinese Tao, Egyptian Maat, Sumerian Me.\nEusebeia and dharma.\nIn the mid-20th century, an inscription of the Indian Emperor Asoka from the year 258 BCE was discovered in Afghanistan, the Kandahar Bilingual Rock Inscription. This rock inscription contains Greek and Aramaic text. According to Paul Hacker, on the rock appears a Greek rendering for the Sanskrit word dharma: the word eusebeia.\nScholars of Hellenistic Greece explain eusebeia as a complex concept. Eusebia means not only to venerate deities, but also spiritual maturity, a reverential attitude toward life, and includes the right conduct toward one's parents, siblings and children, the right conduct between husband and wife, and the conduct between biologically unrelated people. This rock inscription, concludes Paul Hacker, suggests dharma in India, about 2300 years ago, was a central concept and meant not only religious ideas, but ideas of right, of good, of one's duty toward the human community.\nRta, maya and dharma.\nThe evolving literature of Hinduism linked \"dharma\" to two other important concepts: \"\u1e5ata\" and \"M\u0101y\u0101\". \u1e5ata in Vedas is the truth and cosmic principle which regulates and coordinates the operation of the universe and everything within it. M\u0101y\u0101 in Rig-veda and later literature means illusion, fraud, deception, magic that misleads and creates disorder, thus is contrary to reality, laws and rules that establish order, predictability and harmony. Paul Horsch suggests \u1e5ata and \"dharma\" are parallel concepts, the former being a cosmic principle, the latter being of moral social sphere; while M\u0101y\u0101 and \"dharma\" are also correlative concepts, the former being that which corrupts law and moral life, the later being that which strengthens law and moral life.\nDay proposes \"dharma\" is a manifestation of \u1e5ata, but suggests \u1e5ata may have been subsumed into a more complex concept of \"dharma\", as the idea developed in ancient India over time in a nonlinear manner. The following verse from the Rigveda is an example where \"rta\" and \"dharma\" are linked:\nHinduism.\n\"Dharma\" is an organising principle in Hinduism that applies to human beings in solitude, in their interaction with human beings and nature, as well as between inanimate objects, to all of cosmos and its parts. It refers to the order and customs which make life and universe possible, and includes behaviours, rituals, rules that govern society, and ethics. Hindu \"dharma\" includes the religious duties, moral rights and duties of each individual, as well as behaviours that enable social order, right conduct, and those that are virtuous. \"Dharma\", according to Van Buitenen, is that which all existing beings must accept and respect to sustain harmony and order in the world. It is neither the act nor the result, but the natural laws that guide the act and create the result to prevent chaos in the world. It is innate characteristic, that makes the being what it is. It is, claims Van Buitenen, the pursuit and execution of one's nature and true calling, thus playing one's role in cosmic concert. In Hinduism, it is the \"dharma\" of the bee to make honey, of cow to give milk, of sun to radiate sunshine, of river to flow. In terms of humanity, \"dharma\" is the need for, the effect of and essence of service and interconnectedness of all life. This includes duties, rights, laws, conduct, virtues and \"right way of living\".\nIn its true essence, \"dharma\" means for a Hindu to \"expand the mind\". Furthermore, it represents the direct connection between the individual and the societal phenomena that bind the society together. In the way societal phenomena affect the conscience of the individual, similarly may the actions of an individual alter the course of the society, for better or for worse. This has been subtly echoed by the credo \u0927\u0930\u094d\u092e\u094b \u0927\u093e\u0930\u092f\u0924\u093f \u092a\u094d\u0930\u091c\u093e: meaning \"dharma\" is that which holds and provides support to the social construct.\nIn Hinduism, \"dharma\" generally includes various aspects:\nIn Vedas and Upanishads.\nThe history section of this article discusses the development of \"dharma\" concept in Vedas. This development continued in the Upanishads and later ancient scripts of Hinduism. In Upanishads, the concept of \"dharma\" continues as universal principle of law, order, harmony, and truth. It acts as the regulatory moral principle of the Universe. It is explained as law of righteousness and equated to \"satya\" (, truth), in hymn 1.4.14 of Brhadaranyaka Upanishad, as follows:\nDharma and Mimamsa.\n\"Mimamsa\", developed through commentaries on its foundational texts, particularly the \"Mimamsa Sutras\" attributed to Jaimini, emphasizes \"the desire to know dharma\" as the central concern, defining dharma as what connects a person with the highest good, always yet to be realized. While some schools associate dharma with post-mortem existence, \"Mimamsakas\" focus on the continual renewal and realization of a ritual world through adherence to Vedic injunctions. They assert that the ultimate good is essentially inaccessible to perception and can only be understood through language, reflecting confidence in Vedic injunctions and the reality of language as a means of knowing.\n\"Mimamsa\" addresses the delayed results of actions (like wealth or heaven) through the concept of apurva or adrsta, an unseen force that preserves the connection between actions and their outcomes. This ensures that Vedic sacrifices, though their results are delayed, are effective and reliable in guiding toward dharma.\nIn the Epics.\nThe Hindu religion and philosophy, claims Daniel Ingalls, places major emphasis on individual practical morality. In the Sanskrit epics, this concern is omnipresent. In Hindu Epics, the good, morally upright, law-abiding king is referred to as \"dharmaraja\".\n\"Dharma\" is at the centre of all major events in the life of Dasharatha, Rama, Sita, and Lakshman in Ramayana. In the Ramayana, Dasharatha upholds his dharma by honoring a promise to Kaikeyi, resulting in his beloved son Rama's exile, even though it brings him immense personal suffering. \nIn the Mahabharata\", dharma\" is central, and it is presented through symbolism and metaphors. Near the end of the epic, Yama referred to as \"dharma\" in the text, is portrayed as taking the form of a dog to test the compassion of Yudhishthira, who is told he may not enter paradise with such an animal. Yudhishthira refuses to abandon his companion, for which he is then praised by \"dharma\". The value and appeal of the Mahabharata, according to Ingalls, is not as much in its complex and rushed presentation of metaphysics in the 12th book. Indian metaphysics, he argues, is more eloquently presented in other Sanskrit scriptures. Instead, the appeal of Mahabharata, like Ramayana, lies in its presentation of a series of moral problems and life situations, where there are usually three answers: one answer is of Bhima, which represents brute force, an individual angle representing materialism, egoism, and self; the second answer is of Yudhishthira, which appeals to piety, deities, social virtue, and tradition; the third answer is of introspective Arjuna, which falls between the two extremes, and who, claims Ingalls, symbolically reveals the finest moral qualities of man. The Epics of Hinduism are a symbolic treatise about life, virtues, customs, morals, ethics, law, and other aspects of \"dharma\". There is extensive discussion of \"dharma\" at the individual level in the Epics of Hinduism; for example, on free will versus destiny, when and why human beings believe in either, the strong and prosperous naturally uphold free will, while those facing grief or frustration naturally lean towards destiny. The Epics of Hinduism illustrate various aspects of \"dharma\" with metaphors.\nAccording to 4th-century Vatsyayana.\nAccording to Klaus Klostermaier, 4th-century CE Hindu scholar V\u0101tsy\u0101yana explained \"dharma\" by contrasting it with adharma. V\u0101tsy\u0101yana suggested that \"dharma\" is not merely in one's actions, but also in words one speaks or writes, and in thought. According to V\u0101tsy\u0101yana:\nAccording to Patanjali Yoga.\nIn the \"Yoga Sutras\" of Patanjali the \"dharma\" is real; in the Vedanta it is unreal.\n\"Dharma\" is part of yoga, suggests Patanjali; the elements of Hindu dharma are the attributes, qualities and aspects of yoga. Patanjali explained \"dharma\" in two categories: \"yamas\" (restraints) and \"niyamas\" (observances).\nThe five yamas, according to Patanjali, are: abstain from injury to all living creatures, abstain from falsehood (satya), abstain from unauthorised appropriation of things-of-value from another (acastrapurvaka), abstain from coveting or sexually cheating on your partner, and abstain from expecting or accepting gifts from others. The five yama apply in action, speech and mind. In explaining yama, Patanjali clarifies that certain professions and situations may require qualification in conduct. For example, a fisherman must injure a fish, but he must attempt to do this with least trauma to fish and the fisherman must try to injure no other creature as he fishes.\nThe five niyamas (observances) are cleanliness by eating pure food and removing impure thoughts (such as arrogance or jealousy or pride), contentment in one's means, meditation and silent reflection regardless of circumstances one faces, study and pursuit of historic knowledge, and devotion of all actions to the Supreme Teacher to achieve perfection of concentration.\nSources.\n\"Dharma\" is an empirical and experiential inquiry for every man and woman, according to some texts of Hinduism. For example, Apastamba Dharmasutra states:\nIn other texts, three sources and means to discover \"dharma\" in Hinduism are described. These, according to , are: First, learning historical knowledge such as Vedas, Upanishads, the Epics and other Sanskrit literature with the help of one's teacher. Second, observing the behaviour and example of good people. The third source applies when neither one's education nor example exemplary conduct is known. In this case, \"atmatusti\" is the source of \"dharma\" in Hinduism, that is the good person reflects and follows what satisfies his heart, his own inner feeling, what he feels driven to.\nDharma, life stages and social stratification.\nSome texts of Hinduism outline \"dharma\" for society and at the individual level. Of these, the most cited one is \"Manusmriti\", which describes the four \"Varnas\", their rights and duties. Most texts of Hinduism, however, discuss \"dharma\" with no mention of \"Varna\" (caste). Other \"dharma\" texts and Smritis differ from Manusmriti on the nature and structure of Varnas. Yet, other texts question the very existence of varna. Bhrigu, in the Epics, for example, presents the theory that \"dharma\" does not require any varnas. In practice, medieval India is widely believed to be a socially stratified society, with each social strata inheriting a profession and being endogamous. Varna was not absolute in Hindu dharma; individuals had the right to renounce and leave their Varna, as well as their asramas of life, in search of moksa. While neither Manusmriti nor succeeding Smritis of Hinduism ever use the word varnadharma (that is, the \"dharma\" of varnas), or varnasramadharma (that is, the \"dharma\" of varnas and asramas), the scholarly commentary on Manusmriti use these words, and thus associate \"dharma\" with varna system of India. In 6th century India, even Buddhist kings called themselves \"protectors of varnasramadharma\" \u2013 that is, \"dharma\" of varna and asramas of life.\nAt the individual level, some texts of Hinduism outline four \u0101\u015bramas, or stages of life as individual's \"dharma\". These are: (1) brahmac\u0101rya, the life of preparation as a student, (2) g\u1e5bhastha, the life of the householder with family and other social roles, (3) v\u0101nprastha or aranyaka, the life of the forest-dweller, transitioning from worldly occupations to reflection and renunciation, and (4) sanny\u0101sa, the life of giving away all property, becoming a recluse and devotion to moksa, spiritual matters. Patrick Olivelle suggests that \"ashramas represented life choices rather than sequential steps in the life of a single individual\" and the vanaprastha stage was added before renunciation over time, thus forming life stages.\nThe four stages of life complete the four human strivings in life, according to Hinduism. \"Dharma\" enables the individual to satisfy the striving for stability and order, a life that is lawful and harmonious, the striving to do the right thing, be good, be virtuous, earn religious merit, be helpful to others, interact successfully with society. The other three strivings are Artha \u2013 the striving for means of life such as food, shelter, power, security, material wealth, and so forth; Kama \u2013 the striving for sex, desire, pleasure, love, emotional fulfilment, and so forth; and Moksa \u2013 the striving for spiritual meaning, liberation from life-rebirth cycle, self-realisation in this life, and so forth. The four stages are neither independent nor exclusionary in Hindu \"dharma\".\nDharma and poverty.\n\"Dharma\" being necessary for individual and society, is dependent on poverty and prosperity in a society, according to Hindu dharma scriptures. For example, according to Adam Bowles, Shatapatha Brahmana 11.1.6.24 links social prosperity and \"dharma\" through water. Waters come from rains, it claims; when rains are abundant there is prosperity on the earth, and this prosperity enables people to follow \"Dharma\" \u2013 moral and lawful life. In times of distress, of drought, of poverty, everything suffers including relations between human beings and the human ability to live according to \"dharma\".\nIn Rajadharmaparvan 91.34-8, the relationship between poverty and \"dharma\" reaches a full circle. A land with less moral and lawful life suffers distress, and as distress rises it causes more immoral and unlawful life, which further increases distress. Those in power must follow the raja dharma (that is, dharma of rulers), because this enables the society and the individual to follow dharma and achieve prosperity.\nDharma and law.\nThe notion of \"dharma\" as duty or propriety is found in India's ancient legal and religious texts. Common examples of such use are pitri dharma (meaning a person's duty as a father), putra dharma (a person's duty as a son), raj dharma (a person's duty as a king) and so forth. In Hindu philosophy, justice, social harmony, and happiness requires that people live per \"dharma\". The Dharmashastra is a record of these guidelines and rules. The available evidence suggest India once had a large collection of \"dharma\" related literature (sutras, shastras); four of the sutras survive and these are now referred to as Dharmasutras. Along with laws of Manu in Dharmasutras, exist parallel and different compendium of laws, such as the laws of Narada and other ancient scholars. These different and conflicting law books are neither exclusive, nor do they supersede other sources of \"dharma\" in Hinduism. These Dharmasutras include instructions on education of the young, their rites of passage, customs, religious rites and rituals, marital rights and obligations, death and ancestral rites, laws and administration of justice, crimes, punishments, rules and types of evidence, duties of a king, as well as morality.\nBuddhism.\nBuddhism held the Hindu view of \"Dharma\" as \"cosmic law\", as in the working of Karma. The term Dharma () later came to refer to the teachings of the Buddha (\"pariyatti\"); the practice (\"pa\u1e6dipatti\") of the Buddha's teachings is then comprehended as Dharma. In Buddhist philosophy, \"dhamma/dharma\" is also the term for \"phenomena\".\nBuddha's teachings.\nFor practising Buddhists, references to \"dharma\" (\"dhamma\" in Pali) particularly as \"the dharma\", generally means the teachings of the Buddha, commonly known throughout the East as Buddhadharma. It includes especially the discourses on the fundamental principles (such as the Four Noble Truths and the Noble Eightfold Path), as opposed to the parables and to the poems. The Buddha's teachings explain that in order to end suffering, \"dharma\", or the right thoughts, understanding, actions and livelihood, should be cultivated.\nThe status of \"dharma\" is regarded variably by different Buddhist traditions. Some regard it as an ultimate truth, or as the fount of all things which lie beyond the \"three realms\" (Sanskrit: \"tridhatu\") and the \"wheel of becoming\" (Sanskrit: \"bhavachakra\"). Others, who regard the Buddha as simply an enlightened human being, see the \"dharma\" as the essence of the \"84,000 different aspects of the teaching\" (Tibetan: \"chos-sgo brgyad-khri bzhi strong\") that the Buddha gave to various types of people, based upon their individual propensities and capabilities.\nDharma refers not only to the sayings of the Buddha, but also to the later traditions of interpretation and addition that the various schools of Buddhism have developed to help explain and to expand upon the Buddha's teachings. For others still, they see the \"dharma\" as referring to the \"truth\", or the ultimate reality of \"the way that things really are\" (Tibetan: \"Ch\u00f6\").\nThe \"dharma\" is one of the Three Jewels of Buddhism in which practitioners of Buddhism seek refuge, or that upon which one relies for his or her lasting happiness. The Three Jewels of Buddhism are the Buddha, meaning the mind's perfection of enlightenment, the \"dharma\", meaning the teachings and the methods of the Buddha, and the Sangha, meaning the community of practitioners who provide one another guidance and support.\nChan Buddhism.\nDharma is employed in Chan Buddhism in a specific context in relation to transmission of authentic doctrine, understanding and bodhi; recognised in dharma transmission.\nTheravada Buddhism.\nIn Theravada Buddhism obtaining ultimate realisation of the dhamma is achieved in three phases; learning, practising and realising.\nIn Pali:\nJainism.\nThe word \"dharma\" in Jainism is found in all its key texts. It has a contextual meaning and refers to a number of ideas. In the broadest sense, it means the teachings of the Jinas, or teachings of any competing spiritual school, a supreme path, socio-religious duty, and that which is the highest (holy).\nThe \"Tattvartha Sutra\", a major Jain text, mentions () with referring to ten righteous virtues: forbearance, modesty, straightforwardness, purity, truthfulness, self-restraint, austerity, renunciation, non-attachment, and celibacy. , author of the Jain text, writes:\n\"Dharm\u0101stik\u0101ya\".\nThe term () also has a specific ontological and soteriological meaning in Jainism, as a part of its theory of six (substance or a reality). In the Jain tradition, existence consists of (soul, ) and (non-soul, ), the latter consisting of five categories: inert non-sentient atomic matter (), space (), time (), principle of motion (), and principle of rest (). The use of the term to mean motion and to refer to an ontological sub-category is peculiar to Jainism, and not found in the metaphysics of Buddhism and various schools of Hinduism.\nSikhism.\nFor Sikhs, the word \"dharam\" () means the path of righteousness and proper religious practice. Guru Granth Sahib connotes \"dharma\" as duty and moral values. The 3HO movement in Western culture, which has incorporated certain Sikh beliefs, defines Sikh Dharma broadly as all that constitutes religion, moral duty and way of life.\nIn Sangam literature.\nSeveral works of the Sangam and post-Sangam period, many of which are of Hindu or Jain origin, emphasizes on \"dharma\". Most of these texts are based on \"a\u1e5fam\", the Tamil term for \"dharma\". The ancient Tamil moral text of the \"Tirukku\u1e5fa\u1e37\" or \"Kural\", a text probably of Jain or Hindu origin, despite being a collection of aphoristic teachings on \"dharma\" (\"aram\"), artha (\"porul\"), and kama (\"inpam\"), is completely and exclusively based on \"a\u1e5fam\". The Naladiyar, a Jain text of the post-Sangam period, follows a similar pattern as that of the Kural in emphasizing \"a\u1e5fam\" or \"dharma\".\nDharma in symbols.\nThe importance of \"dharma\" to Indian civilization is illustrated by India's decision in 1947 to include the Ashoka Chakra, a depiction of the \"dharmachakra\" (the \"wheel of \"dharma\"\"), as the central motif on its flag."}
{"id": "8754", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=8754", "title": "Dhamma", "text": ""}
{"id": "8755", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=8755", "title": "Distance function", "text": ""}
